{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex10/Alexander_Valle/Alexander_Valle_IA025_Aula10_Exerc%C3%ADcio_44M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "4fcbc5f9-2293-4df9-8be9-f75eaf6c0023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Rolan Alexander Valle Rey Sánchez ra230254\n"
          ]
        }
      ],
      "source": [
        "nome = 'Rolan Alexander Valle Rey Sánchez ra230254'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "5ee16a65-0e80-4212-94f8-e3910633df6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9f3PfifAwpU",
        "outputId": "58ecae9c-4af7-4630-e346-2eee7954c7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  8 20:18:33 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTCe2i7AtoV",
        "outputId": "b39d51f8-2fc4-4164-d297-27783fbd0997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1-EVuUiGMM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        self.tokens_ids =[]\n",
        "        starid=tokenize(text='[CLS]', tokenizer=tokenizer)        #print('starid',starid)\n",
        "        padid=tokenizer.pad_token_id        #print('padid',padid)\n",
        "        for text in texts:\n",
        "          #print('text: ',text)\n",
        "          tokens = tokenize(text, tokenizer)          #print(len(tokens),max_seq_length)\n",
        "          if len(tokens) <= max_seq_length:\n",
        "            self.tokens_ids.append(starid+tokens[0:len(tokens)]+[ padid for k in range(len(tokens),max_seq_length)])\n",
        "          else:\n",
        "            for i in range(len(tokens)+1 - max_seq_length):\n",
        "              self.tokens_ids.append(starid+tokens[i:i +max_seq_length])\n",
        "        self.tokens_ids=torch.tensor(self.tokens_ids, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.tokens_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokens_ids[idx,:-1], self.tokens_ids[idx,1:]"
      ],
      "metadata": {
        "id": "G6AHiMxN-HSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wew-gFbWeBTq"
      },
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r7jBFFUeApe"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtk1pjbpMHB6"
      },
      "outputs": [],
      "source": [
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEzKSgOzVTPk",
        "outputId": "cbc8358a-885e-4dbf-cc68-c20acafb4461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n"
          ]
        }
      ],
      "source": [
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQEqTojeY1h4",
        "outputId": "42b4d19c-f155-42e4-a3ae-d6cd4867aa58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
              "         [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]]),\n",
              " tensor([[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
              "         [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "first_batch_input, first_batch_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzYCxs_MVLTR",
        "outputId": "f73aa011-f452-4a73-aa4e-625241f52748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de dataset.\n"
          ]
        }
      ],
      "source": [
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxjW8EGO5Zl0",
        "outputId": "21c100cc-6c96-43a6-f79b-92b03acaf5bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "len(dummy_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ0NNNlc2m91",
        "outputId": "7e42c3d0-5465-449b-a731-50926e45d740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ],
      "source": [
        "#test with log text and short text\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr e de comer muita pizza de carne', 'Ela gosta muito de comer pizza']\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,  125, 13239,  122,    125,  1847,  5747],\n",
        "     [  101,  10303, 125, 13239,   122,  125,   1847,  5747, 13779],\n",
        "     [  101,   125, 13239,   122,   125,  1847,  5747, 13779, 15616],\n",
        "    [  101, 13239,   122,   125,  1847,  5747, 13779, 15616,   125],\n",
        "     [  101,  1660,  5971,  785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,  125, 13239,  122,    125,  1847,  5747, 13779],\n",
        "     [ 10303, 125, 13239,   122,  125,   1847,  5747, 13779, 15616],\n",
        "     [  125, 13239,   122,   125,  1847,  5747, 13779, 15616,   125],\n",
        "    [13239,   122,   125,  1847,  5747, 13779, 15616,   125,  7714],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de dataset.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "a9138fde-9df4-4037-bf05-f9f848c46daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample-1gb.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1OAf3uS8UaT",
        "outputId": "f7de8dad-e48a-4358-c619-88a69c5b9038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 250000 lines.\n",
            "Truncating to 46000 lines.\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "max_seq_length = 9\n",
        "\n",
        "train_examples = 40000\n",
        "valid_examples = 2000\n",
        "test_examples = 4000\n",
        "\n",
        "texts = open('sample-1gb.txt').readlines()\n",
        "\n",
        "print(f'Read {len(texts)} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "texts = texts[:max_lines]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSS42vYAPNp",
        "outputId": "295e42cc-847a-4665-a1be-05ea3413c41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 44099473\n",
            "valid examples: 2113552\n",
            "test examples: 4220703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "gx9loSfiAQ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RULFFecFzbJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  #https://github.com/happy-jihye/Natural-Language-Processing/tree/main/code\n",
        "    def __init__(self, hid_dim, n_heads):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        self.Wq = nn.Linear(hid_dim, hid_dim)\n",
        "        self.Wk = nn.Linear(hid_dim, hid_dim)\n",
        "        self.Wv = nn.Linear(hid_dim, hid_dim)\n",
        "        self.Wo = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        B = query.shape[0]#batch_size\n",
        "        Q = self.Wq(query)#query: Q = [batch size, L, hid_dim]\n",
        "        K = self.Wk(key)#key :K = [batch size, key len, hid_dim]\n",
        "        V = self.Wv(value)#value : V = [batch size, value len, hid_dim]\n",
        "        Q = Q.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)#Q = [B, n_heads, L, head dim]\n",
        "        K = K.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)#K = [B, n_heads, key len, head dim]\n",
        "        V = V.view(B, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)#V = [B, n_heads, value len, head dim]\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale#energy = [B, n_heads, L, key len]\n",
        "        #if mask is not None:\n",
        "        #    energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim = -1)#attention = [B, n_heads, L, key len]\n",
        "        x = torch.matmul(self.dropout(attention), V)#x = [B, n_heads, L, head dim]\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()#x = [B, L, n heads, head dim]\n",
        "        x = x.view(B, -1, self.hid_dim)#x = [B, L, hid_dim]\n",
        "        x = self.Wo(x)#x = [B, L, hid_dim]\n",
        "        return x#, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_nF9fZSX-PF"
      },
      "outputs": [],
      "source": [
        "class TransfomerBlock(nn.Module):\n",
        "    def __init__(self, input_dim,num_heads):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Dimensionality of the input\n",
        "            num_heads: Number of heads to use in the attention block\n",
        "            embed_dim: embedings \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttentionLayer( input_dim, num_heads)# Attention layer\n",
        "        hsf=2#hidden size factor\n",
        "        feedforward_dim=hsf*input_dim\n",
        "        self.L1 = nn.Linear(input_dim, feedforward_dim)\n",
        "        self.Drop1 =nn.Dropout(0.1)\n",
        "        self.Drop2 =nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU(inplace=True)#inplace=Truevocab_size\n",
        "        self.L2 = nn.Linear(feedforward_dim,input_dim)#, bias=False\n",
        "        # Two-layer MLP# Maps the final output sequence to class logits\n",
        "        self.ff = nn.Sequential(self.L1 ,self.Drop1,self.relu,self.L2,self.Drop2)#feedforward\n",
        "        self.debug=False\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.self_attn(x,x,x, mask=mask) #residual+Attention part\n",
        "        E = self.norm1(x)\n",
        "        fedforward = self.ff(E) # MLP part: B, V  \n",
        "        out=self.norm2(fedforward + E)\n",
        "        if self.debug:\n",
        "          print('x.shape:', x.shape)\n",
        "          #print('A.shape:', A.shape)\n",
        "          print('E.shape',E.shape)\n",
        "          print('FF.shape',fedforward.shape)\n",
        "          print('out.shape',out.shape)\n",
        "        return out\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04-wklqejgd2"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int):\n",
        "        \"\"\"Implements the Self-attention, decoder-only.\n",
        "        Args: vocab_size (int): Size of the input vocabulary.\n",
        "              max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "              dim (int): Dimension of the embedding layer for each word in the context.\n",
        "              n_layers (int): number of self-attention layers.\n",
        "              pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.D=dim#embedding_dim  \n",
        "        self.n_layes=n_layers\n",
        "        self.nh=4# four heads\n",
        "        self.L=max_seq_length\n",
        "        self.pad_token_id=pad_token_id\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.C= nn.Embedding(vocab_size, dim, padding_idx=pad_token_id)#word embeddings\n",
        "        self.P= nn.Embedding(max_seq_length, dim)# positional_embeddings of the words\n",
        "\n",
        "        # Multi-Head Attention layes\n",
        "        self.attention_layers = nn.ModuleList([TransfomerBlock( self.D, self.nh) for _ in range(n_layers)])\n",
        "        self.l1=torch.nn.Linear(dim, 2*dim)\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.l2=torch.nn.Linear(dim*2, vocab_size, bias=False)\n",
        "        self.tologits = torch.nn.Sequential(self.l1,self.relu,self.l2)\n",
        "\n",
        "    def get_mask(self, inputs):\n",
        "        #https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "        #= [batch size, L]\n",
        "        B = inputs.shape[1]#batch size\n",
        "        pad_mask = (inputs != self.pad_token_id).unsqueeze(1).unsqueeze(2)  #pad_mask = [B, 1, 1, L]\n",
        "        sub_mask = torch.tril(torch.ones((self.L, self.L))).bool().to(device)   #sub_mask = [B, L]\n",
        "        mask = pad_mask & sub_mask      #mask = [B, 1, L, L]\n",
        "        return mask \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "        Returns:\n",
        "        logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        B = inputs.shape[0]# B: batch_size\n",
        "        X=self.C(inputs) +self.P.weight# token embeddings +  position embeddings , L: max_seq_length   # shape = B, L, D\n",
        "        mask=self.get_mask(inputs)\n",
        "        for layer in self.attention_layers:\n",
        "          # X shape: (B, L, D)\n",
        "          X = layer(X, mask = mask)\n",
        "        logits = self.tologits(X) # B, V # shape = B, L, D\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm6_PTH2i98e"
      },
      "source": [
        "## Teste o modelo com um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwnxfZlrZoT_"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji8esjaVgBg1",
        "outputId": "5f5b797d-5864-412e-c0d1-cdfd1b29ef45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        }
      ],
      "source": [
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-nF5DwaRta6",
        "outputId": "481fd39d-3b26-4b7e-a114-d2a043f55a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5796288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nhbUVsYnVAp"
      },
      "source": [
        "## Assert da Perplexidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbMP8VAUncfX",
        "outputId": "10d9fb63-ca28-4161-a531-f9749dbc2358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              30710\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "    Args:        logits: a FloatTensor of shape (batch_size, seq_len, vocab_size)\n",
        "                  target: a LongTensor of shape (batch_size, seq_len)\n",
        "    Returns:      A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiJtrsqPnE_l"
      },
      "source": [
        "## Laço de Treinamento e Validação"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max_examples =    150_000_000\n",
        "#training examples: 18 268 128\n",
        "max_examples = 10_000_000\n",
        "\n",
        "eval_every_steps = 1000\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=1024)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6l2SOzUGgTa",
        "outputId": "ff39304a-ea14-48f2-8e27-753240864456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 30911.82, valid ppl: 30532.46\n",
            "1000 steps; 1024000 examples so far; train ppl: 86.85, valid ppl: 3.92\n",
            "2000 steps; 2048000 examples so far; train ppl: 2.71, valid ppl: 2.34\n",
            "3000 steps; 3072000 examples so far; train ppl: 2.29, valid ppl: 2.24\n",
            "4000 steps; 4096000 examples so far; train ppl: 2.23, valid ppl: 2.20\n",
            "5000 steps; 5120000 examples so far; train ppl: 2.19, valid ppl: 2.16\n",
            "6000 steps; 6144000 examples so far; train ppl: 2.15, valid ppl: 2.13\n",
            "7000 steps; 7168000 examples so far; train ppl: 2.13, valid ppl: 2.11\n",
            "8000 steps; 8192000 examples so far; train ppl: 2.10, valid ppl: 2.09\n",
            "9000 steps; 9216000 examples so far; train ppl: 2.08, valid ppl: 2.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgdNymJdNPXP"
      },
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7210b66-96a9-4937-ab11-dec2e15c53bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 2.0485470222335063\n"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHvEs8mPszy_"
      },
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSdrDlJm3tyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a586c84b-e622-483a-8a56-b6eb55906179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontem fui ao restaurante comer um prato delicioso, pediilar\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar?\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,.\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,..\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...?\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo,\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo,.\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo,..\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo,...\n",
            "Ontem fui ao restaurante comer um prato delicioso, pediilar? se mas enquanto perto,...? se mas enquanto segundo,...?\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Ontem fui ao restaurante comer um prato delicioso, pedi'#  tem medo de água fria\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfUKaZrC4S59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bdf21c-c2ed-4d3b-d8ef-ad74e6b60bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A grama da vizinho é sempre é sempre mais verdelas\n",
            "A grama da vizinho é sempre é sempre mais verdelas da\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las -\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide.\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme.\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é sempre\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é sempre las\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é sempre las -\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é sempre las - etc\n",
            "A grama da vizinho é sempre é sempre mais verdelas da conforme é sempre é sempre las - ide. conforme. 30 é sempre las - etc.\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A grama da vizinho é sempre é sempre mais verde'# \n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7hqPDOhsWJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba3af18-a53f-4e65-c401-07b7257b9759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de comer pizza pois me faz se\n",
            "Eu gosto de comer pizza pois me faz se com\n",
            "Eu gosto de comer pizza pois me faz se com de\n",
            "Eu gosto de comer pizza pois me faz se com de?\n",
            "Eu gosto de comer pizza pois me faz se com de? las\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de?\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue pois\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue pois me\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue pois me tem\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue pois me tem se\n",
            "Eu gosto de comer pizza pois me faz se com de? las geralmente pois me tem se com de? las consegue pois me tem se com\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      },
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Alexander_Valle_IA025_Aula10_Exercício_44M",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}