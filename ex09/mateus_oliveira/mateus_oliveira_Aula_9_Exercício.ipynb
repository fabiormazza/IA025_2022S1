{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_mateus_oliveira_Aula_9_Exercício.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4679345347e14082bcbbf845405636c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ce9ce99454e45958e10c79f315f6d10",
              "IPY_MODEL_17fe576d3b094c6ba115fd904580074a",
              "IPY_MODEL_521629784816452b90a68d4eea017568"
            ],
            "layout": "IPY_MODEL_a389c17f799f41f5865446bd696a6c18"
          }
        },
        "8ce9ce99454e45958e10c79f315f6d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c954576b1e42f4a3b25b0d6bd541d6",
            "placeholder": "​",
            "style": "IPY_MODEL_79e8e6716eb04041a8d52933d63410fa",
            "value": "Downloading: 100%"
          }
        },
        "17fe576d3b094c6ba115fd904580074a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddcd4836edcd4216a58a76b7ab9773d6",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4aa525784b74c2b997af55a5c4f0e96",
            "value": 209528
          }
        },
        "521629784816452b90a68d4eea017568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec79761606f44562991c79d20f625c88",
            "placeholder": "​",
            "style": "IPY_MODEL_3659307c94fc44e8b5f442acfebaea83",
            "value": " 205k/205k [00:00&lt;00:00, 2.18MB/s]"
          }
        },
        "a389c17f799f41f5865446bd696a6c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c954576b1e42f4a3b25b0d6bd541d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e8e6716eb04041a8d52933d63410fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddcd4836edcd4216a58a76b7ab9773d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4aa525784b74c2b997af55a5c4f0e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec79761606f44562991c79d20f625c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3659307c94fc44e8b5f442acfebaea83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3112209dc365452cacd8c28ec1beb26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c1a59fc8a2d4f43a595bb466ff0b32c",
              "IPY_MODEL_20469f3cf11b4806ba9e6121f9bb4aae",
              "IPY_MODEL_6a7da671353d4146a37df5763077d233"
            ],
            "layout": "IPY_MODEL_979bd6b6fa9640329661d27eef580675"
          }
        },
        "0c1a59fc8a2d4f43a595bb466ff0b32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e4b565501274204bddaf86c72ffc28e",
            "placeholder": "​",
            "style": "IPY_MODEL_7aa331bc5ad34d39b612f3b5380b6464",
            "value": "Downloading: 100%"
          }
        },
        "20469f3cf11b4806ba9e6121f9bb4aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6c244ad9a7644c8883378f2f66eb481",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_903c62051ae0492390b2160c1adb59b1",
            "value": 2
          }
        },
        "6a7da671353d4146a37df5763077d233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81bfee5f666a48249f753ed6ea181772",
            "placeholder": "​",
            "style": "IPY_MODEL_8168d0f3b36b478bb43f2397156f5aa3",
            "value": " 2.00/2.00 [00:00&lt;00:00, 54.8B/s]"
          }
        },
        "979bd6b6fa9640329661d27eef580675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e4b565501274204bddaf86c72ffc28e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa331bc5ad34d39b612f3b5380b6464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6c244ad9a7644c8883378f2f66eb481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "903c62051ae0492390b2160c1adb59b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81bfee5f666a48249f753ed6ea181772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8168d0f3b36b478bb43f2397156f5aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "014ff0f6787347a8b9fe216027192024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_150634419a8043398aa83f25f5c8a3cc",
              "IPY_MODEL_c104df965f6b42ffa7b3eba0e4f5ffa7",
              "IPY_MODEL_4742ab827adc4950b4f8d37d10567538"
            ],
            "layout": "IPY_MODEL_9ef5415f528144f48129dd58185f0875"
          }
        },
        "150634419a8043398aa83f25f5c8a3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1583768ea814cc1a9ae0468a3dd14bc",
            "placeholder": "​",
            "style": "IPY_MODEL_d0921567ded94d8b83c9dc67bf03d2d1",
            "value": "Downloading: 100%"
          }
        },
        "c104df965f6b42ffa7b3eba0e4f5ffa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2d6601b8244e608500360211580b49",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_325f6ee6a985459ba4b5137946bbc524",
            "value": 112
          }
        },
        "4742ab827adc4950b4f8d37d10567538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b23abb8690e649f0a57c8fe54b451383",
            "placeholder": "​",
            "style": "IPY_MODEL_0d1a74af5a074dbf9b6af1dffc5b66b5",
            "value": " 112/112 [00:00&lt;00:00, 3.23kB/s]"
          }
        },
        "9ef5415f528144f48129dd58185f0875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1583768ea814cc1a9ae0468a3dd14bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0921567ded94d8b83c9dc67bf03d2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2d6601b8244e608500360211580b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325f6ee6a985459ba4b5137946bbc524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b23abb8690e649f0a57c8fe54b451383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d1a74af5a074dbf9b6af1dffc5b66b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0bfc25ae184e51ae7f3414884d9982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c86a5ce6a85432d991cdd062c798fa3",
              "IPY_MODEL_e9d9f826219443d196b743a0f3a8ecab",
              "IPY_MODEL_10d248fcf1814a6182aa870df88f5ae4"
            ],
            "layout": "IPY_MODEL_666a8af9160444c29de3c57ccfad8a94"
          }
        },
        "5c86a5ce6a85432d991cdd062c798fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cca1012ce174f29a0c7754bf4eef991",
            "placeholder": "​",
            "style": "IPY_MODEL_c0e9f6bf36754b67a9d6cb0c2eb1caf0",
            "value": "Downloading: 100%"
          }
        },
        "e9d9f826219443d196b743a0f3a8ecab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca4be93523c42bda7654f6fbb6e6805",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd0e6ead2522442fa8331b4cb9def645",
            "value": 43
          }
        },
        "10d248fcf1814a6182aa870df88f5ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8768308ef0a94071b81c84312ed6ea7c",
            "placeholder": "​",
            "style": "IPY_MODEL_82626805895943f5889543f90c3f1c76",
            "value": " 43.0/43.0 [00:00&lt;00:00, 1.21kB/s]"
          }
        },
        "666a8af9160444c29de3c57ccfad8a94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cca1012ce174f29a0c7754bf4eef991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0e9f6bf36754b67a9d6cb0c2eb1caf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ca4be93523c42bda7654f6fbb6e6805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd0e6ead2522442fa8331b4cb9def645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8768308ef0a94071b81c84312ed6ea7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82626805895943f5889543f90c3f1c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7cef69794242fb8475380ec758a8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1a6f9d8533741f495d65a9579e4530c",
              "IPY_MODEL_927f9c0d94334449a60bcd617ac8b7ba",
              "IPY_MODEL_64691dedbd2e421fb3a2e623e2f0033b"
            ],
            "layout": "IPY_MODEL_639fa360ba96492e94e7b714d5b56da7"
          }
        },
        "d1a6f9d8533741f495d65a9579e4530c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98d1a8a41ec4897859ef4fcd4feaaea",
            "placeholder": "​",
            "style": "IPY_MODEL_3c69eab70e3048c68e83798b895b3338",
            "value": "Downloading: 100%"
          }
        },
        "927f9c0d94334449a60bcd617ac8b7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c37d8dd3ae9b4244a3888d37ee0baa26",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe2595f279e4485a86b8d04ac04b558e",
            "value": 647
          }
        },
        "64691dedbd2e421fb3a2e623e2f0033b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6439083817604f55975c0f81c3dd37f2",
            "placeholder": "​",
            "style": "IPY_MODEL_378f5642b1ed45969b631f77369a72a1",
            "value": " 647/647 [00:00&lt;00:00, 18.3kB/s]"
          }
        },
        "639fa360ba96492e94e7b714d5b56da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a98d1a8a41ec4897859ef4fcd4feaaea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c69eab70e3048c68e83798b895b3338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c37d8dd3ae9b4244a3888d37ee0baa26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe2595f279e4485a86b8d04ac04b558e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6439083817604f55975c0f81c3dd37f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378f5642b1ed45969b631f77369a72a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "nome = \"Mateus Oliveira da Silva\"\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed17254-115c-4d87-dbf4-661d2ca5dc06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Mateus Oliveira da Silva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYMc4f06Xk8e",
        "outputId": "e0ecac88-ea26-4f64-968d-d0e50531dbee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.memory_allocated: 0.000000GB\n",
            "torch.cuda.memory_reserved: 0.000000GB\n",
            "torch.cuda.max_memory_reserved: 0.000000GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "19010fa1-f83f-4c3c-a001-ab123b4d3e66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 47.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a0926d-1867-4445-cedc-f36561a1d349"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  1 23:03:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c38cd0-1862-4169-bdd2-e6447a85f91e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "def tokenize(text_list: list, tokenizer, max_seq_length):\n",
        "    \n",
        "    return tokenizer.batch_encode_plus(text_list,padding=True,truncation=True,max_length = max_seq_length).input_ids\n",
        "\n",
        "# def token_with_init_pad(list_text:str, tokenizer, max_seq_length:int):\n",
        "\n",
        "#     list_text = [f'{text}' for text in list_text]\n",
        "\n",
        "#     tokens_ids = tokenize(list_text, tokenizer,max_seq_length)\n",
        "\n",
        "#     text_truncate = text\n",
        "\n",
        "#     # if len(tokens_ids) < max_seq_length:\n",
        "#     #   text_truncate = text[:max_seq_length]\n",
        "\n",
        "#     # add_len_pad = max_seq_length - len(tokens_ids)\n",
        "    \n",
        "#     # tokens_ids = tokens_ids + [0 for x in range(add_len_pad)]\n",
        "\n",
        "#     return tokens_ids\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "token_ids = tokenize(dummy_texts, tokenizer, 100)"
      ],
      "metadata": {
        "id": "2h82fKERqdXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "4679345347e14082bcbbf845405636c7",
            "8ce9ce99454e45958e10c79f315f6d10",
            "17fe576d3b094c6ba115fd904580074a",
            "521629784816452b90a68d4eea017568",
            "a389c17f799f41f5865446bd696a6c18",
            "e1c954576b1e42f4a3b25b0d6bd541d6",
            "79e8e6716eb04041a8d52933d63410fa",
            "ddcd4836edcd4216a58a76b7ab9773d6",
            "a4aa525784b74c2b997af55a5c4f0e96",
            "ec79761606f44562991c79d20f625c88",
            "3659307c94fc44e8b5f442acfebaea83",
            "3112209dc365452cacd8c28ec1beb26d",
            "0c1a59fc8a2d4f43a595bb466ff0b32c",
            "20469f3cf11b4806ba9e6121f9bb4aae",
            "6a7da671353d4146a37df5763077d233",
            "979bd6b6fa9640329661d27eef580675",
            "0e4b565501274204bddaf86c72ffc28e",
            "7aa331bc5ad34d39b612f3b5380b6464",
            "f6c244ad9a7644c8883378f2f66eb481",
            "903c62051ae0492390b2160c1adb59b1",
            "81bfee5f666a48249f753ed6ea181772",
            "8168d0f3b36b478bb43f2397156f5aa3",
            "014ff0f6787347a8b9fe216027192024",
            "150634419a8043398aa83f25f5c8a3cc",
            "c104df965f6b42ffa7b3eba0e4f5ffa7",
            "4742ab827adc4950b4f8d37d10567538",
            "9ef5415f528144f48129dd58185f0875",
            "a1583768ea814cc1a9ae0468a3dd14bc",
            "d0921567ded94d8b83c9dc67bf03d2d1",
            "bb2d6601b8244e608500360211580b49",
            "325f6ee6a985459ba4b5137946bbc524",
            "b23abb8690e649f0a57c8fe54b451383",
            "0d1a74af5a074dbf9b6af1dffc5b66b5",
            "9c0bfc25ae184e51ae7f3414884d9982",
            "5c86a5ce6a85432d991cdd062c798fa3",
            "e9d9f826219443d196b743a0f3a8ecab",
            "10d248fcf1814a6182aa870df88f5ae4",
            "666a8af9160444c29de3c57ccfad8a94",
            "8cca1012ce174f29a0c7754bf4eef991",
            "c0e9f6bf36754b67a9d6cb0c2eb1caf0",
            "0ca4be93523c42bda7654f6fbb6e6805",
            "dd0e6ead2522442fa8331b4cb9def645",
            "8768308ef0a94071b81c84312ed6ea7c",
            "82626805895943f5889543f90c3f1c76",
            "ff7cef69794242fb8475380ec758a8b7",
            "d1a6f9d8533741f495d65a9579e4530c",
            "927f9c0d94334449a60bcd617ac8b7ba",
            "64691dedbd2e421fb3a2e623e2f0033b",
            "639fa360ba96492e94e7b714d5b56da7",
            "a98d1a8a41ec4897859ef4fcd4feaaea",
            "3c69eab70e3048c68e83798b895b3338",
            "c37d8dd3ae9b4244a3888d37ee0baa26",
            "fe2595f279e4485a86b8d04ac04b558e",
            "6439083817604f55975c0f81c3dd37f2",
            "378f5642b1ed45969b631f77369a72a1"
          ]
        },
        "outputId": "1042e487-d9d5-484c-92ed-1579fa856ee1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/205k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4679345347e14082bcbbf845405636c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3112209dc365452cacd8c28ec1beb26d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "014ff0f6787347a8b9fe216027192024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c0bfc25ae184e51ae7f3414884d9982"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff7cef69794242fb8475380ec758a8b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-DixDMNu_rc5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "context_size = 9\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        self.examples = []\n",
        "        self.token_ids = tokenize(texts, tokenizer, max_seq_length)\n",
        "        \n",
        "\n",
        "        for i in range(len(self.token_ids)):\n",
        "\n",
        "          pad_len = (max_seq_length - len(self.token_ids[i]))\n",
        "\n",
        "          self.token_ids[i] = self.token_ids[i] + pad_len*[0]\n",
        "\n",
        "\n",
        "        self.target_id = [x[1:len(self.token_ids[0])+1] + [0] for x in self.token_ids]\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'{self.examples}'\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.LongTensor(self.token_ids[idx]), torch.LongTensor(self.target_id[idx])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_dataset = MyDataset(texts=training_texts[:10], tokenizer=tokenizer, max_seq_length=1024)\n",
        "# training_dataset[2][1][-3:],training_dataset[1][1][-3:]\n",
        "# #train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))"
      ],
      "metadata": {
        "id": "HxXT9Qv5op58"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_seq_length"
      ],
      "metadata": {
        "id": "SVME8dOApWIk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(dummy_texts, tokenizer, max_seq_length=20)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "print(first_batch_target)\n",
        "\n",
        "#assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "#assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae976f21-3425-4f56-852a-43bb3281ac52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "tensor([[ 3396, 10303,   125, 13239,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1660,  5971,   785,   125,  1847, 13779, 15616,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "9661e4f4-4e5c-4901-e2be-d1442731ea6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-01 23:03:32--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 173.194.210.128, 173.194.213.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1.1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1.15G   204MB/s    in 5.4s    \n",
            "\n",
            "2022-06-01 23:03:38 (216 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "\n",
        "max_seq_length = 512\n",
        "\n",
        "texts = open('sample-1gb.txt').readlines()\n",
        "\n",
        "len_max = int(len(texts)/5)\n",
        "\n",
        "train_examples = int(len_max*0.6)\n",
        "valid_examples = int(len_max*0.3)\n",
        "test_examples = int(len_max*0.1)\n",
        "\n",
        "print(f\"train examples: {train_examples}\")\n",
        "print(f\"valid examples: {valid_examples}\")\n",
        "print(f\"test examples: {test_examples}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f'Read {len(texts)} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "texts = texts[:max_lines]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf355f05-091c-4aa9-d002-ecfa25f256f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train examples: 30000\n",
            "valid examples: 15000\n",
            "test examples: 5000\n",
            "Read 250000 lines.\n",
            "Truncating to 50000 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckqT5ad4Ximz",
        "outputId": "c24cea05-fd36-4296-8de2-2f23e242ba0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.memory_allocated: 0.000000GB\n",
            "torch.cuda.memory_reserved: 0.000000GB\n",
            "torch.cuda.max_memory_reserved: 0.000000GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfaf3bc-e706-49a4-a68d-65bf53013ccc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 30000\n",
            "valid examples: 15000\n",
            "test examples: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd.profiler as profiler\n",
        "import time\n",
        "\n",
        "# import time\n",
        "\n",
        "# start = time.time()\n",
        "# print(\"hello\")\n",
        "# end = time.time()\n",
        "# print(end - start)\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "            vocab_size: size of vocabulary\n",
        "            embed_dim: dimension of embeddings\n",
        "        \"\"\"\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            out: embedding vector\n",
        "        \"\"\"\n",
        "        # print(f'embbeding {x.shape}')\n",
        "        #start = time.time()\n",
        "        #with profiler.record_function(\"Embbeding init\"):\n",
        "        out = self.embed(x)\n",
        "        # end = time.time()\n",
        "        # print(f\"embbeding {end - start}\")\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "MWBO0iR9KjGn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,embed_model_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            seq_len: length of input sequence\n",
        "            embed_model_dim: demension of embedding\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_model_dim\n",
        "\n",
        "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0,self.embed_dim,2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            x: output\n",
        "        \"\"\"\n",
        "        #start = time.time()\n",
        "        # make embeddings relatively larger\n",
        "        #with profiler.record_function(\"Positional Embbedings\"):\n",
        "        x = x * math.sqrt(self.embed_dim)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        # print(f\"PositionalEmbedding {x.shape}\")\n",
        "\n",
        "        #end = time.time()\n",
        "        #print(f\"PositionalEmbedding {end - start}\")\n",
        "\n",
        "        return x\n",
        "               \n"
      ],
      "metadata": {
        "id": "Dfl6guxU59_L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads):\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "            embed_dim: dimension of embeding vector output\n",
        "            n_heads: number of self attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim \n",
        "        self.n_heads = n_heads  \n",
        "        self.single_head_dim = int(self.embed_dim / self.n_heads)\n",
        "       \n",
        "        #key,query and value matrixes    \n",
        "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  \n",
        "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
        "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim).to(device)\n",
        "        \n",
        "    def forward(self,key,query,value,mask=None): \n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           key : key vector\n",
        "           query : query vector\n",
        "           value : value vector\n",
        "           mask: mask for decoder\n",
        "        \n",
        "        Returns:\n",
        "           output vector from multihead attention\n",
        "        \"\"\"\n",
        "        #start = time.time()\n",
        "        #with profiler.record_function(\"loader MUltihead attetion\"):\n",
        "        batch_size = key.size(0)\n",
        "        seq_length = key.size(1)\n",
        "        \n",
        "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  \n",
        "        query = query.view(batch_size, seq_length, self.n_heads, self.single_head_dim) \n",
        "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) \n",
        "      \n",
        "        #with profiler.record_function(\"linear MUltihead attetion\"):\n",
        "        k = self.key_matrix(key).to(device)       \n",
        "        q = self.query_matrix(query).to(device)   \n",
        "        v = self.value_matrix(value).to(device)\n",
        "\n",
        "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim) \n",
        "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
        "      \n",
        "        # computes attention\n",
        "        # adjust key for matrix multiplication\n",
        "        k_adjusted = k.transpose(-1,-2)  \n",
        "        product = torch.matmul(q, k_adjusted)  \n",
        "      \n",
        "        #with profiler.record_function(\"operation MUltihead attetion\"):\n",
        "        if mask is not None:\n",
        "            product = product.masked_fill(mask == 0, float(\"-1e20\")).to(device)\n",
        "\n",
        "        product = product / math.sqrt(self.single_head_dim)\n",
        "\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "\n",
        "        scores = torch.matmul(scores, v)  \n",
        "        \n",
        "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length, self.single_head_dim*self.n_heads).to(device) \n",
        "\n",
        "\n",
        "        output = self.out(concat)\n",
        "        \n",
        "        #end = time.time()\n",
        "        #print(f\"Multiread attention {end - start}\")\n",
        "        # print(f\"MultiHeadAttention {output.shape}\")\n",
        "       \n",
        "        return output\n"
      ],
      "metadata": {
        "id": "lb0yqCM_5-Eo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor, n_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: fator ehich determines output dimension of linear layer\n",
        "           n_heads: number of attention heads\n",
        "        \n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(embed_dim) \n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        self.feed_forward = nn.Sequential(\n",
        "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self,key,query,value,mask=None):\n",
        "        \n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "           mask: mask to be given for multi head attnetion(used only for the decoder)\n",
        "        Returns:\n",
        "           norm2_out: output of transformer block\n",
        "        \n",
        "        \"\"\"\n",
        "        #start = time.time()\n",
        "        #with profiler.record_function(\"TransformerBlock\"):\n",
        "        attention_out = self.attention(key,query,value,mask)\n",
        "        attention_residual_out = attention_out + value \n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) \n",
        "\n",
        "        feed_fwd_out = self.feed_forward(norm1_out)\n",
        "        feed_fwd_residual_out = feed_fwd_out + norm1_out \n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) \n",
        "\n",
        "        # print(f\"TransformerBlock {norm2_out.shape}\")\n",
        "        #end = time.time()\n",
        "        #print(f\"TransformerBlock {end - start}\")\n",
        "\n",
        "        return norm2_out\n",
        "\n"
      ],
      "metadata": {
        "id": "J4lw7gVNaCw4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor, n_heads):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: fator ehich determines output dimension of linear layer\n",
        "           n_heads: number of attention heads\n",
        "        \n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads=1)\n",
        "        self.norm = nn.LayerNorm(embed_dim).to(device)# <<<\n",
        "        self.dropout = nn.Dropout(0.2)# <<<<\n",
        "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
        "        \n",
        "    \n",
        "    def forward(self, key, query, x, mask):\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "           mask: mask to be given for multi head attention \n",
        "        Returns:\n",
        "           out: output of transformer block\n",
        "    \n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        #with profiler.record_function(\"Decoder Block\"):\n",
        "        attention = self.attention(x,x,x,mask).to(device) #32x10x512\n",
        "        residual = attention + x.to(device)\n",
        "        value = self.dropout(self.norm(residual)).to(device)# <<<<<<\n",
        "        out = self.transformer_block(key, query, value, mask)\n",
        "\n",
        "        # print(f\"DecoderBlock {out.shape}\")\n",
        "        # end = time.time()\n",
        "        # print(f\"Decoder blocks {end - start}\")\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers, expansion_factor, n_heads):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        \"\"\"  \n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           target_vocab_size: vocabulary size of taget\n",
        "           embed_dim: dimension of embedding\n",
        "           seq_len : length of input sequence\n",
        "           num_layers: number of encoder layers\n",
        "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
        "           n_heads: number of heads in multihead attention\n",
        "        \n",
        "        \"\"\"\n",
        "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
        "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, expansion_factor=1, n_heads=1) \n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, enc_out, trg_mask):\n",
        "        \n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "            x: input vector from target\n",
        "            enc_out : output from encoder layer\n",
        "            trg_mask: mask for decoder self attention\n",
        "        Returns:\n",
        "            out: output vector\n",
        "        \"\"\"\n",
        "        #start = time.time()\n",
        "        #with profiler.record_function(\"TransformerDecoder\"):\n",
        "        batch_size, seq_length = x.shape[0],x.shape[1] \n",
        "\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.position_embedding(x) \n",
        "        x = self.dropout(x)\n",
        "    \n",
        "        for layer in self.layers:\n",
        "            x = layer(enc_out, enc_out, x, trg_mask)\n",
        "\n",
        "        out = F.softmax(self.fc_out(x),dim=1)\n",
        "\n",
        "        # end = time.time()\n",
        "        # print(f\"TransformerDecoder {end - start}\")\n",
        "\n",
        "          #print(f\"Transformer Decoder {out.shape}\")\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "l2IInWJkIyCR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers, expansion_factor, n_heads):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        \"\"\"  \n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "           embed_dim:  dimension of embedding \n",
        "           src_vocab_size: vocabulary size of source\n",
        "           target_vocab_size: vocabulary size of target\n",
        "           seq_length : length of input sequence\n",
        "           num_layers: number of encoder layers\n",
        "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
        "           n_heads: number of heads in multihead attention\n",
        "        \n",
        "        \"\"\"\n",
        "        self.embedding = Embedding(src_vocab_size, embed_dim)\n",
        "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \"\"\"\n",
        "        @Mateus Oliveira\n",
        "        Args:\n",
        "            trg: target sequence\n",
        "        Returns:\n",
        "            trg_mask: target mask\n",
        "        \"\"\"\n",
        "        batch_size, trg_len = trg.shape\n",
        "\n",
        "        #print(f\"before make_trg_mask\")\n",
        "\n",
        "        # returns the lower triangular part of matrix filled with ones\n",
        "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "            batch_size, 1, trg_len, trg_len\n",
        "        ).to(device)\n",
        "\n",
        "        #print(f\"after make_trg_mask {trg_mask.shape}\")\n",
        "\n",
        "        return trg_mask    \n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: input to encoder \n",
        "            trg: input to decoder\n",
        "        out:\n",
        "            out: final vector which returns probabilities of each target word\n",
        "        \"\"\"\n",
        "\n",
        "        #start = time.time()\n",
        "        #with profiler.record_function(\"Transformer\"):\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.embedding(src).to(device)\n",
        "        \n",
        "        out = self.decoder(trg, enc_src, trg_mask)\n",
        "      # end = time.time()\n",
        "#        print(f\"Transformer {end - start}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "SEk8iCjMIyEb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #src = torch.rand(64, 32, 512)\n",
        "# #tgt = torch.rand(64, 16, 512)\n",
        "# src_vocab_size = tokenizer.vocab_size\n",
        "# target_vocab_size = tokenizer.vocab_size\n",
        "# num_layers = 1\n",
        "# seq_length= 12\n",
        "\n",
        "# text = ['a cidade vai para outra capital','a vida é feita para ser vivida diaria']\n",
        "\n",
        "# t_tokens = torch.LongTensor(tokenize(text,tokenizer,seq_length)).to(device)\n",
        "# print(t_tokens)\n",
        "# t_tokens_mask = torch.LongTensor(tokenize(text,tokenizer,seq_length)).to(device)\n",
        "\n",
        "# # embedding_1 = nn.Embedding(tokenizer.vocab_size, hidden_size).to(device)\n",
        "\n",
        "# # emb_t_tokens = embedding_1(t_tokens)\n",
        "\n",
        "# #sample_input, target_input = next(iter(DataLoader(training_dataset)))\n",
        "\n",
        "# # x = torch.tensor([[1, 5, 6, 4, 3, 9, 500, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n",
        "# # target = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0, 1,9], [1, 5, 6, 2, 4, 7, 6, 2, 0,2]]).to(device)\n",
        "\n",
        "# model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size,\n",
        "#                     target_vocab_size=target_vocab_size,\n",
        "#                     seq_length=seq_length, num_layers=num_layers, expansion_factor=1, n_heads=1).to(device)\n",
        "\n",
        "# out = model(t_tokens, t_tokens_mask)\n",
        "\n",
        "# with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "#     out = model(t_tokens, t_tokens_mask)\n",
        "\n",
        "# print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=20))\n"
      ],
      "metadata": {
        "id": "UdtmBC6zf1YR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "\"\"\"\n",
        "Muitas lineares precisaram ser definidas .to(device) como resolverr isso de uma unica vez? \n",
        "\n",
        "\"\"\"\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size: int,max_seq_length: int, dim: int,\n",
        "              n_layers: int, pad_token_id: int,\n",
        "              expansion_factor = 2, n_heads = 1):\n",
        "    #def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.num_layers = n_layers\n",
        "        \n",
        "        self.transformer = Transformer(embed_dim=dim, src_vocab_size = vocab_size,\n",
        "                    target_vocab_size=vocab_size,\n",
        "                    seq_length=max_seq_length, num_layers=self.num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
        "\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, vocab_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        out = self.transformer(src, trg)\n",
        "        \n",
        "        #return F.softmax(out, dim=1).argmax()\n",
        "        return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78002678-7d3e-4604-db74-d3bd943bb865"
      },
      "source": [
        "max_seq_length = 512\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=1,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "sample_input, target_input = next(iter(DataLoader(training_dataset)))\n",
        "target_input = target_input.to(device)\n",
        "sample_input = sample_input.to(device)\n",
        "\n",
        "print(sample_input.is_cuda, target_input.is_cuda)\n",
        "\n",
        "sample_output = model(sample_input,sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True True\n",
            "sample_input.shape: torch.Size([1, 512])\n",
            "sample_output.shape: torch.Size([1, 512, 29794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0271b054-9c02-4816-a969-4b949ddb1af8"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5791842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8erGqsPhTImf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G1qq5iMhT5L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2_VAJ9fSsRP",
        "outputId": "7b35b648-f644-4ded-ecd7-86f34b2d9104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.memory_allocated: 0.000000GB\n",
            "torch.cuda.memory_reserved: 0.000000GB\n",
            "torch.cuda.max_memory_reserved: 0.000000GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.empty_cache\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_len, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_len)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    print(logits.shape, target.shape)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean')\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 4\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids,train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86db6cc-fa1c-4ec2-e68f-e7bc570637b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embbeding torch.Size([4, 512])\n",
            "PositionalEmbedding torch.Size([4, 512, 64])\n",
            "MultiHeadAttention torch.Size([4, 512, 64])\n",
            "MultiHeadAttention torch.Size([4, 512, 64])\n",
            "TransformerBlock torch.Size([4, 512, 64])\n",
            "DecoderBlock torch.Size([4, 512, 64])\n",
            "torch.Size([2048, 29794]) torch.Size([2048])\n",
            "my perplexity:              29795\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape)\n",
        "print(train_target_ids.shape)\n",
        "device"
      ],
      "metadata": {
        "id": "5-gpBIeelxtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4920a7e8-85fb-4156-f741-ffbd6f5b092e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512, 29794])\n",
            "torch.Size([4, 512])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVb7k_10qSl0",
        "outputId": "d20e0c26-293b-4a39-8f22-06c1ccdcb4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SaveBestModel:\n",
        "\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"Best validation loss: {self.best_valid_loss}\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, \"gdrive/MyDrive/Colab Notebooks/\"+\"best_model_1_june.pt\")\n"
      ],
      "metadata": {
        "id": "RjSu3i76qJ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75412984-0814-4956-ce17-d3339f774c09"
      },
      "source": [
        "max_examples = 1500_000_000\n",
        "eval_every_steps = 100\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=32,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=10, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=10)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids,input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    print(logits)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=tokenizer.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids,input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=tokenizer.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device),\n",
        "                          train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device),\n",
        "                                    val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "\n",
        "            last_loss = valid_ppl\n",
        "\n",
        "            train_losses = []\n",
        "            save_best_model(\n",
        "              last_loss, 0, model, optimizer, nn.functional.cross_entropy\n",
        "              )\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 29795.44, valid ppl: 29794.75\n",
            "Best validation loss: 29794.753359300994\n",
            "100 steps; 1000 examples so far; train ppl: 29792.84, valid ppl: 29787.95\n",
            "Best validation loss: 29787.945392938065\n",
            "200 steps; 2000 examples so far; train ppl: 29786.53, valid ppl: 29776.88\n",
            "Best validation loss: 29776.884257494\n",
            "300 steps; 3000 examples so far; train ppl: 29774.22, valid ppl: 29758.11\n",
            "Best validation loss: 29758.113228991515\n",
            "400 steps; 4000 examples so far; train ppl: 29752.80, valid ppl: 29730.11\n",
            "Best validation loss: 29730.10848561431\n",
            "500 steps; 5000 examples so far; train ppl: 29731.15, valid ppl: 29708.70\n",
            "Best validation loss: 29708.704298964145\n",
            "600 steps; 6000 examples so far; train ppl: 29710.85, valid ppl: 29695.10\n",
            "Best validation loss: 29695.102564037104\n",
            "700 steps; 7000 examples so far; train ppl: 29698.55, valid ppl: 29679.22\n",
            "Best validation loss: 29679.22002966567\n",
            "800 steps; 8000 examples so far; train ppl: 29682.60, valid ppl: 29666.97\n",
            "Best validation loss: 29666.967797984475\n",
            "900 steps; 9000 examples so far; train ppl: 29675.33, valid ppl: 29656.83\n",
            "Best validation loss: 29656.834474051484\n",
            "1000 steps; 10000 examples so far; train ppl: 29664.17, valid ppl: 29648.32\n",
            "Best validation loss: 29648.317131022493\n",
            "1100 steps; 11000 examples so far; train ppl: 29654.76, valid ppl: 29640.08\n",
            "Best validation loss: 29640.081235033467\n",
            "1200 steps; 12000 examples so far; train ppl: 29654.72, valid ppl: 29631.73\n",
            "Best validation loss: 29631.725858513746\n",
            "1300 steps; 13000 examples so far; train ppl: 29645.49, valid ppl: 29619.48\n",
            "Best validation loss: 29619.481334272266\n",
            "1400 steps; 14000 examples so far; train ppl: 29635.07, valid ppl: 29602.52\n",
            "Best validation loss: 29602.516993957342\n",
            "1500 steps; 15000 examples so far; train ppl: 29622.25, valid ppl: 29590.03\n",
            "Best validation loss: 29590.034988512492\n",
            "1600 steps; 16000 examples so far; train ppl: 29607.30, valid ppl: 29579.84\n",
            "Best validation loss: 29579.838257170846\n",
            "1700 steps; 17000 examples so far; train ppl: 29598.01, valid ppl: 29571.20\n",
            "Best validation loss: 29571.202210739324\n",
            "1800 steps; 18000 examples so far; train ppl: 29589.90, valid ppl: 29561.62\n",
            "Best validation loss: 29561.62301384636\n",
            "1900 steps; 19000 examples so far; train ppl: 29585.34, valid ppl: 29553.40\n",
            "Best validation loss: 29553.3966994606\n",
            "2000 steps; 20000 examples so far; train ppl: 29578.00, valid ppl: 29543.48\n",
            "Best validation loss: 29543.480746681627\n",
            "2100 steps; 21000 examples so far; train ppl: 29570.25, valid ppl: 29532.52\n",
            "Best validation loss: 29532.515485867723\n",
            "2200 steps; 22000 examples so far; train ppl: 29554.17, valid ppl: 29520.76\n",
            "Best validation loss: 29520.76239829109\n",
            "2300 steps; 23000 examples so far; train ppl: 29539.97, valid ppl: 29508.04\n",
            "Best validation loss: 29508.040038088275\n",
            "2400 steps; 24000 examples so far; train ppl: 29540.25, valid ppl: 29496.19\n",
            "Best validation loss: 29496.191780903926\n",
            "2500 steps; 25000 examples so far; train ppl: 29530.84, valid ppl: 29483.91\n",
            "Best validation loss: 29483.909488686146\n",
            "2600 steps; 26000 examples so far; train ppl: 29515.99, valid ppl: 29467.24\n",
            "Best validation loss: 29467.237544953747\n",
            "2700 steps; 27000 examples so far; train ppl: 29502.97, valid ppl: 29451.60\n",
            "Best validation loss: 29451.59824268355\n",
            "2800 steps; 28000 examples so far; train ppl: 29501.83, valid ppl: 29439.57\n",
            "Best validation loss: 29439.565708251936\n",
            "2900 steps; 29000 examples so far; train ppl: 29490.69, valid ppl: 29429.25\n",
            "Best validation loss: 29429.24907875867\n",
            "3000 steps; 30000 examples so far; train ppl: 29466.75, valid ppl: 29417.46\n",
            "Best validation loss: 29417.459516947012\n",
            "3100 steps; 31000 examples so far; train ppl: 29457.34, valid ppl: 29406.07\n",
            "Best validation loss: 29406.06587263814\n",
            "3200 steps; 32000 examples so far; train ppl: 29462.26, valid ppl: 29394.41\n",
            "Best validation loss: 29394.407813453934\n",
            "3300 steps; 33000 examples so far; train ppl: 29444.50, valid ppl: 29383.22\n",
            "Best validation loss: 29383.222767535197\n",
            "3400 steps; 34000 examples so far; train ppl: 29428.49, valid ppl: 29372.79\n",
            "Best validation loss: 29372.793196020684\n",
            "3500 steps; 35000 examples so far; train ppl: 29423.36, valid ppl: 29361.95\n",
            "Best validation loss: 29361.945456337788\n",
            "3600 steps; 36000 examples so far; train ppl: 29418.58, valid ppl: 29351.58\n",
            "Best validation loss: 29351.581667189712\n",
            "3700 steps; 37000 examples so far; train ppl: 29406.17, valid ppl: 29339.69\n",
            "Best validation loss: 29339.691549901527\n",
            "3800 steps; 38000 examples so far; train ppl: 29398.12, valid ppl: 29328.87\n",
            "Best validation loss: 29328.869971214182\n",
            "3900 steps; 39000 examples so far; train ppl: 29388.99, valid ppl: 29316.45\n",
            "Best validation loss: 29316.454982013584\n",
            "4000 steps; 40000 examples so far; train ppl: 29388.77, valid ppl: 29305.44\n",
            "Best validation loss: 29305.43705954524\n",
            "4100 steps; 41000 examples so far; train ppl: 29377.50, valid ppl: 29293.47\n",
            "Best validation loss: 29293.467515227694\n",
            "4200 steps; 42000 examples so far; train ppl: 29372.02, valid ppl: 29280.47\n",
            "Best validation loss: 29280.46866803332\n",
            "4300 steps; 43000 examples so far; train ppl: 29355.20, valid ppl: 29266.05\n",
            "Best validation loss: 29266.04525710235\n",
            "4400 steps; 44000 examples so far; train ppl: 29357.79, valid ppl: 29256.06\n",
            "Best validation loss: 29256.056437083018\n",
            "4500 steps; 45000 examples so far; train ppl: 29338.26, valid ppl: 29242.13\n",
            "Best validation loss: 29242.127430956134\n",
            "4600 steps; 46000 examples so far; train ppl: 29329.84, valid ppl: 29230.47\n",
            "Best validation loss: 29230.468625833324\n",
            "4700 steps; 47000 examples so far; train ppl: 29314.55, valid ppl: 29216.33\n",
            "Best validation loss: 29216.33226292688\n",
            "4800 steps; 48000 examples so far; train ppl: 29304.90, valid ppl: 29204.94\n",
            "Best validation loss: 29204.93757039885\n",
            "4900 steps; 49000 examples so far; train ppl: 29305.17, valid ppl: 29191.32\n",
            "Best validation loss: 29191.31966360232\n",
            "5000 steps; 50000 examples so far; train ppl: 29296.77, valid ppl: 29180.07\n",
            "Best validation loss: 29180.071933301897\n",
            "5100 steps; 51000 examples so far; train ppl: 29274.80, valid ppl: 29165.11\n",
            "Best validation loss: 29165.10804303534\n",
            "5200 steps; 52000 examples so far; train ppl: 29274.75, valid ppl: 29149.73\n",
            "Best validation loss: 29149.725582913317\n",
            "5300 steps; 53000 examples so far; train ppl: 29258.04, valid ppl: 29139.19\n",
            "Best validation loss: 29139.185053290974\n",
            "5400 steps; 54000 examples so far; train ppl: 29251.63, valid ppl: 29123.16\n",
            "Best validation loss: 29123.158326682493\n",
            "5500 steps; 55000 examples so far; train ppl: 29249.82, valid ppl: 29109.52\n",
            "Best validation loss: 29109.51874835506\n",
            "5600 steps; 56000 examples so far; train ppl: 29243.17, valid ppl: 29093.15\n",
            "Best validation loss: 29093.152071506087\n",
            "5700 steps; 57000 examples so far; train ppl: 29220.99, valid ppl: 29076.51\n",
            "Best validation loss: 29076.510062731057\n",
            "5800 steps; 58000 examples so far; train ppl: 29219.97, valid ppl: 29063.55\n",
            "Best validation loss: 29063.549920850077\n",
            "5900 steps; 59000 examples so far; train ppl: 29215.28, valid ppl: 29050.10\n",
            "Best validation loss: 29050.103730214334\n",
            "6000 steps; 60000 examples so far; train ppl: 29197.55, valid ppl: 29034.28\n",
            "Best validation loss: 29034.281715863235\n",
            "6100 steps; 61000 examples so far; train ppl: 29200.11, valid ppl: 29023.96\n",
            "Best validation loss: 29023.96328916345\n",
            "6200 steps; 62000 examples so far; train ppl: 29173.46, valid ppl: 29007.61\n",
            "Best validation loss: 29007.606961221445\n",
            "6300 steps; 63000 examples so far; train ppl: 29175.54, valid ppl: 28996.59\n",
            "Best validation loss: 28996.585822588844\n",
            "6400 steps; 64000 examples so far; train ppl: 29152.76, valid ppl: 28984.14\n",
            "Best validation loss: 28984.14044448268\n",
            "6500 steps; 65000 examples so far; train ppl: 29161.81, valid ppl: 28966.75\n",
            "Best validation loss: 28966.74714025007\n",
            "6600 steps; 66000 examples so far; train ppl: 29139.52, valid ppl: 28957.95\n",
            "Best validation loss: 28957.946974462535\n",
            "6700 steps; 67000 examples so far; train ppl: 29144.69, valid ppl: 28947.84\n",
            "Best validation loss: 28947.844409571724\n",
            "6800 steps; 68000 examples so far; train ppl: 29124.19, valid ppl: 28930.52\n",
            "Best validation loss: 28930.524514790897\n",
            "6900 steps; 69000 examples so far; train ppl: 29120.79, valid ppl: 28920.41\n",
            "Best validation loss: 28920.408514507206\n",
            "7000 steps; 70000 examples so far; train ppl: 29109.39, valid ppl: 28910.88\n",
            "Best validation loss: 28910.882444450508\n",
            "7100 steps; 71000 examples so far; train ppl: 29102.12, valid ppl: 28898.06\n",
            "Best validation loss: 28898.064274303688\n",
            "7200 steps; 72000 examples so far; train ppl: 29082.46, valid ppl: 28885.73\n",
            "Best validation loss: 28885.729182910065\n",
            "7300 steps; 73000 examples so far; train ppl: 29080.98, valid ppl: 28873.78\n",
            "Best validation loss: 28873.782382518348\n",
            "7400 steps; 74000 examples so far; train ppl: 29070.30, valid ppl: 28860.82\n",
            "Best validation loss: 28860.82211681734\n",
            "7500 steps; 75000 examples so far; train ppl: 29061.46, valid ppl: 28854.00\n",
            "Best validation loss: 28854.003839699668\n",
            "7600 steps; 76000 examples so far; train ppl: 29050.54, valid ppl: 28845.58\n",
            "Best validation loss: 28845.577333168392\n",
            "7700 steps; 77000 examples so far; train ppl: 29049.22, valid ppl: 28832.63\n",
            "Best validation loss: 28832.632779726413\n",
            "7800 steps; 78000 examples so far; train ppl: 29043.16, valid ppl: 28823.74\n",
            "Best validation loss: 28823.74481497473\n",
            "7900 steps; 79000 examples so far; train ppl: 29033.68, valid ppl: 28813.99\n",
            "Best validation loss: 28813.990266744353\n",
            "8000 steps; 80000 examples so far; train ppl: 29033.60, valid ppl: 28802.72\n",
            "Best validation loss: 28802.715904096258\n",
            "8100 steps; 81000 examples so far; train ppl: 29004.41, valid ppl: 28793.07\n",
            "Best validation loss: 28793.06577401869\n",
            "8200 steps; 82000 examples so far; train ppl: 29000.10, valid ppl: 28784.48\n",
            "Best validation loss: 28784.479228694447\n",
            "8300 steps; 83000 examples so far; train ppl: 28995.82, valid ppl: 28775.74\n",
            "Best validation loss: 28775.741941899985\n",
            "8400 steps; 84000 examples so far; train ppl: 28983.36, valid ppl: 28765.51\n",
            "Best validation loss: 28765.509821313623\n",
            "8500 steps; 85000 examples so far; train ppl: 28964.92, valid ppl: 28757.56\n",
            "Best validation loss: 28757.563942652938\n",
            "8600 steps; 86000 examples so far; train ppl: 28980.02, valid ppl: 28751.53\n",
            "Best validation loss: 28751.5339255686\n",
            "8700 steps; 87000 examples so far; train ppl: 28953.98, valid ppl: 28741.50\n",
            "Best validation loss: 28741.502590854427\n",
            "8800 steps; 88000 examples so far; train ppl: 28951.96, valid ppl: 28732.90\n",
            "Best validation loss: 28732.904465916996\n",
            "8900 steps; 89000 examples so far; train ppl: 28943.11, valid ppl: 28728.85\n",
            "Best validation loss: 28728.852554875244\n",
            "9000 steps; 90000 examples so far; train ppl: 28934.87, valid ppl: 28722.40\n",
            "Best validation loss: 28722.3950657576\n",
            "9100 steps; 91000 examples so far; train ppl: 28929.61, valid ppl: 28713.43\n",
            "Best validation loss: 28713.43301469688\n",
            "9200 steps; 92000 examples so far; train ppl: 28936.76, valid ppl: 28708.22\n",
            "Best validation loss: 28708.21840732242\n",
            "9300 steps; 93000 examples so far; train ppl: 28925.58, valid ppl: 28699.38\n",
            "Best validation loss: 28699.38409437088\n",
            "9400 steps; 94000 examples so far; train ppl: 28911.65, valid ppl: 28692.63\n",
            "Best validation loss: 28692.626442353165\n",
            "9500 steps; 95000 examples so far; train ppl: 28909.63, valid ppl: 28686.52\n",
            "Best validation loss: 28686.517586518374\n",
            "9600 steps; 96000 examples so far; train ppl: 28908.11, valid ppl: 28680.46\n",
            "Best validation loss: 28680.4573157128\n",
            "9700 steps; 97000 examples so far; train ppl: 28896.12, valid ppl: 28672.26\n",
            "Best validation loss: 28672.26252882655\n",
            "9800 steps; 98000 examples so far; train ppl: 28896.89, valid ppl: 28667.36\n",
            "Best validation loss: 28667.3570930354\n",
            "9900 steps; 99000 examples so far; train ppl: 28876.78, valid ppl: 28662.56\n",
            "Best validation loss: 28662.55759156606\n",
            "10000 steps; 100000 examples so far; train ppl: 28889.05, valid ppl: 28657.60\n",
            "Best validation loss: 28657.598616305742\n",
            "10100 steps; 101000 examples so far; train ppl: 28880.69, valid ppl: 28651.86\n",
            "Best validation loss: 28651.8591908895\n",
            "10200 steps; 102000 examples so far; train ppl: 28871.56, valid ppl: 28644.55\n",
            "Best validation loss: 28644.55429791635\n",
            "10300 steps; 103000 examples so far; train ppl: 28864.35, valid ppl: 28639.76\n",
            "Best validation loss: 28639.757166511936\n",
            "10400 steps; 104000 examples so far; train ppl: 28859.12, valid ppl: 28635.47\n",
            "Best validation loss: 28635.47314232833\n",
            "10500 steps; 105000 examples so far; train ppl: 28841.78, valid ppl: 28628.86\n",
            "Best validation loss: 28628.85899747523\n",
            "10600 steps; 106000 examples so far; train ppl: 28839.70, valid ppl: 28623.97\n",
            "Best validation loss: 28623.96870593299\n",
            "10700 steps; 107000 examples so far; train ppl: 28831.41, valid ppl: 28618.31\n",
            "Best validation loss: 28618.31069491294\n",
            "10800 steps; 108000 examples so far; train ppl: 28838.30, valid ppl: 28612.83\n",
            "Best validation loss: 28612.83333868123\n",
            "10900 steps; 109000 examples so far; train ppl: 28834.63, valid ppl: 28609.18\n",
            "Best validation loss: 28609.175324689368\n",
            "11000 steps; 110000 examples so far; train ppl: 28840.10, valid ppl: 28602.78\n",
            "Best validation loss: 28602.780381020548\n",
            "11100 steps; 111000 examples so far; train ppl: 28819.97, valid ppl: 28599.02\n",
            "Best validation loss: 28599.023078632352\n",
            "11200 steps; 112000 examples so far; train ppl: 28826.09, valid ppl: 28593.62\n",
            "Best validation loss: 28593.620388228643\n",
            "11300 steps; 113000 examples so far; train ppl: 28818.13, valid ppl: 28592.54\n",
            "Best validation loss: 28592.539983536997\n",
            "11400 steps; 114000 examples so far; train ppl: 28815.00, valid ppl: 28586.60\n",
            "Best validation loss: 28586.604553264446\n",
            "11500 steps; 115000 examples so far; train ppl: 28787.76, valid ppl: 28583.46\n",
            "Best validation loss: 28583.459793815065\n",
            "11600 steps; 116000 examples so far; train ppl: 28797.26, valid ppl: 28578.50\n",
            "Best validation loss: 28578.499193182535\n",
            "11700 steps; 117000 examples so far; train ppl: 28784.67, valid ppl: 28573.91\n",
            "Best validation loss: 28573.908616376706\n",
            "11800 steps; 118000 examples so far; train ppl: 28778.61, valid ppl: 28569.22\n",
            "Best validation loss: 28569.216060379506\n",
            "11900 steps; 119000 examples so far; train ppl: 28786.85, valid ppl: 28564.82\n",
            "Best validation loss: 28564.821131306875\n",
            "12000 steps; 120000 examples so far; train ppl: 28772.86, valid ppl: 28560.20\n",
            "Best validation loss: 28560.197431848137\n",
            "12100 steps; 121000 examples so far; train ppl: 28773.56, valid ppl: 28555.50\n",
            "Best validation loss: 28555.504152405818\n",
            "12200 steps; 122000 examples so far; train ppl: 28779.39, valid ppl: 28552.23\n",
            "Best validation loss: 28552.229504981242\n",
            "12300 steps; 123000 examples so far; train ppl: 28778.61, valid ppl: 28550.90\n",
            "Best validation loss: 28550.90025844271\n",
            "12400 steps; 124000 examples so far; train ppl: 28773.32, valid ppl: 28545.85\n",
            "Best validation loss: 28545.84890327046\n",
            "12500 steps; 125000 examples so far; train ppl: 28741.70, valid ppl: 28542.84\n",
            "Best validation loss: 28542.841320462678\n",
            "12600 steps; 126000 examples so far; train ppl: 28757.30, valid ppl: 28538.30\n",
            "Best validation loss: 28538.2986633452\n",
            "12700 steps; 127000 examples so far; train ppl: 28747.69, valid ppl: 28535.58\n",
            "Best validation loss: 28535.58274086608\n",
            "12800 steps; 128000 examples so far; train ppl: 28730.50, valid ppl: 28532.43\n",
            "Best validation loss: 28532.433533092608\n",
            "12900 steps; 129000 examples so far; train ppl: 28731.71, valid ppl: 28530.15\n",
            "Best validation loss: 28530.14866475097\n",
            "13000 steps; 130000 examples so far; train ppl: 28736.76, valid ppl: 28526.27\n",
            "Best validation loss: 28526.274455293784\n",
            "13100 steps; 131000 examples so far; train ppl: 28737.11, valid ppl: 28522.38\n",
            "Best validation loss: 28522.378079436276\n",
            "13200 steps; 132000 examples so far; train ppl: 28736.38, valid ppl: 28519.20\n",
            "Best validation loss: 28519.201322225315\n",
            "13300 steps; 133000 examples so far; train ppl: 28718.04, valid ppl: 28516.67\n",
            "Best validation loss: 28516.668692953317\n",
            "13400 steps; 134000 examples so far; train ppl: 28721.19, valid ppl: 28512.95\n",
            "Best validation loss: 28512.946101082187\n",
            "13500 steps; 135000 examples so far; train ppl: 28696.10, valid ppl: 28510.29\n",
            "Best validation loss: 28510.2887240762\n",
            "13600 steps; 136000 examples so far; train ppl: 28706.10, valid ppl: 28508.81\n",
            "Best validation loss: 28508.809053866986\n",
            "13700 steps; 137000 examples so far; train ppl: 28698.75, valid ppl: 28502.54\n",
            "Best validation loss: 28502.542215224552\n",
            "13800 steps; 138000 examples so far; train ppl: 28721.33, valid ppl: 28501.48\n",
            "Best validation loss: 28501.482430375072\n",
            "13900 steps; 139000 examples so far; train ppl: 28714.64, valid ppl: 28496.97\n",
            "Best validation loss: 28496.96926567411\n",
            "14000 steps; 140000 examples so far; train ppl: 28715.09, valid ppl: 28493.74\n",
            "Best validation loss: 28493.738035743878\n",
            "14100 steps; 141000 examples so far; train ppl: 28696.34, valid ppl: 28492.12\n",
            "Best validation loss: 28492.12201133488\n",
            "14200 steps; 142000 examples so far; train ppl: 28706.15, valid ppl: 28489.57\n",
            "Best validation loss: 28489.567442700416\n",
            "14300 steps; 143000 examples so far; train ppl: 28681.62, valid ppl: 28484.02\n",
            "Best validation loss: 28484.01730831174\n",
            "14400 steps; 144000 examples so far; train ppl: 28672.88, valid ppl: 28482.51\n",
            "Best validation loss: 28482.512028664612\n",
            "14500 steps; 145000 examples so far; train ppl: 28679.85, valid ppl: 28481.89\n",
            "Best validation loss: 28481.893519373723\n",
            "14600 steps; 146000 examples so far; train ppl: 28680.30, valid ppl: 28476.59\n",
            "Best validation loss: 28476.5924117525\n",
            "14700 steps; 147000 examples so far; train ppl: 28686.36, valid ppl: 28474.46\n",
            "Best validation loss: 28474.455544427696\n",
            "14800 steps; 148000 examples so far; train ppl: 28682.82, valid ppl: 28471.47\n",
            "Best validation loss: 28471.47074331139\n",
            "14900 steps; 149000 examples so far; train ppl: 28689.73, valid ppl: 28468.95\n",
            "Best validation loss: 28468.946309849856\n",
            "15000 steps; 150000 examples so far; train ppl: 28687.14, valid ppl: 28466.27\n",
            "Best validation loss: 28466.269028347873\n",
            "15100 steps; 151000 examples so far; train ppl: 28674.18, valid ppl: 28464.00\n",
            "Best validation loss: 28464.003404379277\n",
            "15200 steps; 152000 examples so far; train ppl: 28663.14, valid ppl: 28462.83\n",
            "Best validation loss: 28462.833523232897\n",
            "15300 steps; 153000 examples so far; train ppl: 28665.39, valid ppl: 28458.98\n",
            "Best validation loss: 28458.982974953975\n",
            "15400 steps; 154000 examples so far; train ppl: 28654.73, valid ppl: 28456.60\n",
            "Best validation loss: 28456.602993030476\n",
            "15500 steps; 155000 examples so far; train ppl: 28668.28, valid ppl: 28452.98\n",
            "Best validation loss: 28452.98354651185\n",
            "15600 steps; 156000 examples so far; train ppl: 28655.34, valid ppl: 28451.57\n",
            "Best validation loss: 28451.573320712756\n",
            "15700 steps; 157000 examples so far; train ppl: 28658.36, valid ppl: 28449.43\n",
            "Best validation loss: 28449.434179687374\n",
            "15800 steps; 158000 examples so far; train ppl: 28652.87, valid ppl: 28446.69\n",
            "Best validation loss: 28446.69215879978\n",
            "15900 steps; 159000 examples so far; train ppl: 28641.81, valid ppl: 28443.51\n",
            "Best validation loss: 28443.50803050547\n",
            "16000 steps; 160000 examples so far; train ppl: 28648.13, valid ppl: 28440.74\n",
            "Best validation loss: 28440.74463811005\n",
            "16100 steps; 161000 examples so far; train ppl: 28617.14, valid ppl: 28437.48\n",
            "Best validation loss: 28437.479571060965\n",
            "16200 steps; 162000 examples so far; train ppl: 28619.01, valid ppl: 28437.68\n",
            "16300 steps; 163000 examples so far; train ppl: 28635.78, valid ppl: 28434.03\n",
            "Best validation loss: 28434.03395493168\n",
            "16400 steps; 164000 examples so far; train ppl: 28625.70, valid ppl: 28431.64\n",
            "Best validation loss: 28431.640712631663\n",
            "16500 steps; 165000 examples so far; train ppl: 28623.15, valid ppl: 28429.80\n",
            "Best validation loss: 28429.804982947076\n",
            "16600 steps; 166000 examples so far; train ppl: 28608.59, valid ppl: 28426.65\n",
            "Best validation loss: 28426.650003744468\n",
            "16700 steps; 167000 examples so far; train ppl: 28631.06, valid ppl: 28423.41\n",
            "Best validation loss: 28423.40931083878\n",
            "16800 steps; 168000 examples so far; train ppl: 28607.20, valid ppl: 28421.90\n",
            "Best validation loss: 28421.899014446437\n",
            "16900 steps; 169000 examples so far; train ppl: 28620.85, valid ppl: 28419.68\n",
            "Best validation loss: 28419.678179567407\n",
            "17000 steps; 170000 examples so far; train ppl: 28598.35, valid ppl: 28416.88\n",
            "Best validation loss: 28416.87620772956\n",
            "17100 steps; 171000 examples so far; train ppl: 28611.21, valid ppl: 28416.76\n",
            "Best validation loss: 28416.760705867786\n",
            "17200 steps; 172000 examples so far; train ppl: 28603.85, valid ppl: 28413.61\n",
            "Best validation loss: 28413.605521308436\n",
            "17300 steps; 173000 examples so far; train ppl: 28616.29, valid ppl: 28411.98\n",
            "Best validation loss: 28411.977899309113\n",
            "17400 steps; 174000 examples so far; train ppl: 28620.29, valid ppl: 28409.69\n",
            "Best validation loss: 28409.69425768077\n",
            "17500 steps; 175000 examples so far; train ppl: 28584.24, valid ppl: 28406.63\n",
            "Best validation loss: 28406.63168819437\n",
            "17600 steps; 176000 examples so far; train ppl: 28620.11, valid ppl: 28406.27\n",
            "Best validation loss: 28406.268397792242\n",
            "17700 steps; 177000 examples so far; train ppl: 28608.16, valid ppl: 28402.26\n",
            "Best validation loss: 28402.257165490024\n",
            "17800 steps; 178000 examples so far; train ppl: 28587.83, valid ppl: 28402.18\n",
            "Best validation loss: 28402.17536436821\n",
            "17900 steps; 179000 examples so far; train ppl: 28593.83, valid ppl: 28400.36\n",
            "Best validation loss: 28400.36267675385\n",
            "18000 steps; 180000 examples so far; train ppl: 28590.81, valid ppl: 28397.34\n",
            "Best validation loss: 28397.34187131653\n",
            "18100 steps; 181000 examples so far; train ppl: 28586.40, valid ppl: 28395.93\n",
            "Best validation loss: 28395.925459981543\n",
            "18200 steps; 182000 examples so far; train ppl: 28571.72, valid ppl: 28394.38\n",
            "Best validation loss: 28394.38337093725\n",
            "18300 steps; 183000 examples so far; train ppl: 28609.71, valid ppl: 28390.96\n",
            "Best validation loss: 28390.95874814953\n",
            "18400 steps; 184000 examples so far; train ppl: 28583.63, valid ppl: 28389.06\n",
            "Best validation loss: 28389.056471219767\n",
            "18500 steps; 185000 examples so far; train ppl: 28559.36, valid ppl: 28386.76\n",
            "Best validation loss: 28386.763112294804\n",
            "18600 steps; 186000 examples so far; train ppl: 28591.13, valid ppl: 28386.28\n",
            "Best validation loss: 28386.28353393587\n",
            "18700 steps; 187000 examples so far; train ppl: 28588.66, valid ppl: 28383.46\n",
            "Best validation loss: 28383.46370722889\n",
            "18800 steps; 188000 examples so far; train ppl: 28561.76, valid ppl: 28381.96\n",
            "Best validation loss: 28381.95987764886\n",
            "18900 steps; 189000 examples so far; train ppl: 28583.11, valid ppl: 28378.85\n",
            "Best validation loss: 28378.849586380387\n",
            "19000 steps; 190000 examples so far; train ppl: 28565.55, valid ppl: 28377.48\n",
            "Best validation loss: 28377.484156923358\n",
            "19100 steps; 191000 examples so far; train ppl: 28582.92, valid ppl: 28376.55\n",
            "Best validation loss: 28376.553344507443\n",
            "19200 steps; 192000 examples so far; train ppl: 28569.03, valid ppl: 28373.32\n",
            "Best validation loss: 28373.324741855085\n",
            "19300 steps; 193000 examples so far; train ppl: 28576.68, valid ppl: 28371.94\n",
            "Best validation loss: 28371.941645797466\n",
            "19400 steps; 194000 examples so far; train ppl: 28563.97, valid ppl: 28371.05\n",
            "Best validation loss: 28371.047575577475\n",
            "19500 steps; 195000 examples so far; train ppl: 28560.05, valid ppl: 28369.01\n",
            "Best validation loss: 28369.013576937672\n",
            "19600 steps; 196000 examples so far; train ppl: 28551.68, valid ppl: 28367.88\n",
            "Best validation loss: 28367.880909781125\n",
            "19700 steps; 197000 examples so far; train ppl: 28566.54, valid ppl: 28365.89\n",
            "Best validation loss: 28365.892943631552\n",
            "19800 steps; 198000 examples so far; train ppl: 28522.44, valid ppl: 28364.54\n",
            "Best validation loss: 28364.53536682767\n",
            "19900 steps; 199000 examples so far; train ppl: 28543.98, valid ppl: 28361.86\n",
            "Best validation loss: 28361.85841727821\n",
            "20000 steps; 200000 examples so far; train ppl: 28558.56, valid ppl: 28359.32\n",
            "Best validation loss: 28359.31636632958\n",
            "20100 steps; 201000 examples so far; train ppl: 28553.56, valid ppl: 28357.74\n",
            "Best validation loss: 28357.735043490993\n",
            "20200 steps; 202000 examples so far; train ppl: 28563.67, valid ppl: 28355.73\n",
            "Best validation loss: 28355.730488134923\n",
            "20300 steps; 203000 examples so far; train ppl: 28540.01, valid ppl: 28354.47\n",
            "Best validation loss: 28354.473692345106\n",
            "20400 steps; 204000 examples so far; train ppl: 28531.50, valid ppl: 28353.50\n",
            "Best validation loss: 28353.50435229742\n",
            "20500 steps; 205000 examples so far; train ppl: 28542.64, valid ppl: 28351.11\n",
            "Best validation loss: 28351.108938543228\n",
            "20600 steps; 206000 examples so far; train ppl: 28518.94, valid ppl: 28349.82\n",
            "Best validation loss: 28349.820924406897\n",
            "20700 steps; 207000 examples so far; train ppl: 28523.03, valid ppl: 28348.20\n",
            "Best validation loss: 28348.20307285034\n",
            "20800 steps; 208000 examples so far; train ppl: 28511.46, valid ppl: 28347.58\n",
            "Best validation loss: 28347.57638928626\n",
            "20900 steps; 209000 examples so far; train ppl: 28515.63, valid ppl: 28345.83\n",
            "Best validation loss: 28345.829320863813\n",
            "21000 steps; 210000 examples so far; train ppl: 28530.76, valid ppl: 28343.24\n",
            "Best validation loss: 28343.238653338627\n",
            "21100 steps; 211000 examples so far; train ppl: 28523.35, valid ppl: 28342.01\n",
            "Best validation loss: 28342.013893287043\n",
            "21200 steps; 212000 examples so far; train ppl: 28536.16, valid ppl: 28339.31\n",
            "Best validation loss: 28339.308765791928\n",
            "21300 steps; 213000 examples so far; train ppl: 28497.87, valid ppl: 28336.16\n",
            "Best validation loss: 28336.155783110884\n",
            "21400 steps; 214000 examples so far; train ppl: 28520.60, valid ppl: 28336.70\n",
            "21500 steps; 215000 examples so far; train ppl: 28523.83, valid ppl: 28334.96\n",
            "Best validation loss: 28334.96122255334\n",
            "21600 steps; 216000 examples so far; train ppl: 28517.65, valid ppl: 28332.37\n",
            "Best validation loss: 28332.36688739352\n",
            "21700 steps; 217000 examples so far; train ppl: 28516.95, valid ppl: 28330.82\n",
            "Best validation loss: 28330.817104924754\n",
            "21800 steps; 218000 examples so far; train ppl: 28531.90, valid ppl: 28328.91\n",
            "Best validation loss: 28328.90738913183\n",
            "21900 steps; 219000 examples so far; train ppl: 28510.69, valid ppl: 28327.72\n",
            "Best validation loss: 28327.71778079443\n",
            "22000 steps; 220000 examples so far; train ppl: 28499.42, valid ppl: 28326.90\n",
            "Best validation loss: 28326.89653753313\n",
            "22100 steps; 221000 examples so far; train ppl: 28500.38, valid ppl: 28325.62\n",
            "Best validation loss: 28325.62059758451\n",
            "22200 steps; 222000 examples so far; train ppl: 28492.28, valid ppl: 28322.70\n",
            "Best validation loss: 28322.699996752548\n",
            "22300 steps; 223000 examples so far; train ppl: 28517.15, valid ppl: 28321.69\n",
            "Best validation loss: 28321.689796890376\n",
            "22400 steps; 224000 examples so far; train ppl: 28487.79, valid ppl: 28320.19\n",
            "Best validation loss: 28320.19188030778\n",
            "22500 steps; 225000 examples so far; train ppl: 28488.34, valid ppl: 28319.34\n",
            "Best validation loss: 28319.34133609369\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ab6313640e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                     validation_step(val_input_ids.to(device),\n\u001b[1;32m     59\u001b[0m                                     val_target_ids.to(device))\n\u001b[0;32m---> 60\u001b[0;31m                     for val_input_ids, val_target_ids in validation_loader]))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ab6313640e3a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     validation_step(val_input_ids.to(device),\n\u001b[1;32m     59\u001b[0m                                     val_target_ids.to(device))\n\u001b[0;32m---> 60\u001b[0;31m                     for val_input_ids, val_target_ids in validation_loader]))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-ab6313640e3a>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(input_ids, target_ids)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5eb959b113d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Escreva seu código aqui.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#return F.softmax(out, dim=1).argmax()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-3c13e943e2f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8e58006b41d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_out, trg_mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8e58006b41d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, key, query, x, mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#32x10x512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2484\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         )\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=32,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "def validation_step(input, target):\n",
        "    model.eval()\n",
        "    logits = model(input, input)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, ignore_index=tokenizer.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "load_dict = torch.load(\"gdrive/MyDrive/Colab Notebooks/\"+\"best_model_31_may.pt\")\n",
        "model.load_state_dict(load_dict['model_state_dict'])\n",
        "model.to(device)\n",
        "\n",
        "test_loader = DataLoader(valid_dataset, batch_size=20)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "metadata": {
        "id": "oqR65m_cM-YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b1bee5-227e-4956-8c16-04b42261f72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 28221.039085231117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEVp1xFmzSX6",
        "outputId": "4aa87e7c-d4e6-47e6-9f86-ad382ecf65f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cuda.memory_allocated: 12.653975GB\n",
            "torch.cuda.memory_reserved: 12.814453GB\n",
            "torch.cuda.max_memory_reserved: 12.830078GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_examples = 1500_000_000\n",
        "eval_every_steps = 1000\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=1,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=15, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=15)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids,input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=tokenizer.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids,input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=tokenizer.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device),\n",
        "                          train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device),\n",
        "                                    val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "\n",
        "            last_loss = valid_ppl\n",
        "\n",
        "            train_losses = []\n",
        "            save_best_model(\n",
        "              last_loss, 0, model, optimizer, nn.functional.cross_entropy\n",
        "              )\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "mrbKSa9MykmD",
        "outputId": "bebc6e82-207a-4b87-8ac7-121cdc4ddf4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 29794.16, valid ppl: 29794.25\n",
            "Best validation loss: 29794.24899925026\n",
            "1000 steps; 15000 examples so far; train ppl: 29568.64, valid ppl: 29251.80\n",
            "Best validation loss: 29251.797627909014\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5931dc77723a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         loss = train_step(train_input_ids.to(device),\n\u001b[0;32m---> 50\u001b[0;31m                           train_target_ids.to(device)) \n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-5931dc77723a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_ids, target_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}