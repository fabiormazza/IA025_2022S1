{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Exercício - Template",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1758e3b995d94ec5875f9ab7b973192b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_072c279a73234c23b0ad6bf31009b277",
              "IPY_MODEL_81bb4df0c7f74618bc62ef31882bef8e",
              "IPY_MODEL_414ce44f152c41bd8962d4ce2df19375"
            ],
            "layout": "IPY_MODEL_1f892b9fb51543da9642777a43966954"
          }
        },
        "072c279a73234c23b0ad6bf31009b277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_480b8974efc44ba59ef6ad5f793fe0c7",
            "placeholder": "​",
            "style": "IPY_MODEL_b19ea554751f41e298bff8d51cbe513e",
            "value": "100%"
          }
        },
        "81bb4df0c7f74618bc62ef31882bef8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d4068b129ea45d4a25733635937b777",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebba2d2bf6864940b823f49efacbea3b",
            "value": 2
          }
        },
        "414ce44f152c41bd8962d4ce2df19375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b534c2e7a059484a9297336d246cdad5",
            "placeholder": "​",
            "style": "IPY_MODEL_2704cb8364514623b14178ebda32ce36",
            "value": " 2/2 [00:00&lt;00:00, 60.11it/s]"
          }
        },
        "1f892b9fb51543da9642777a43966954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480b8974efc44ba59ef6ad5f793fe0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b19ea554751f41e298bff8d51cbe513e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d4068b129ea45d4a25733635937b777": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebba2d2bf6864940b823f49efacbea3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b534c2e7a059484a9297336d246cdad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2704cb8364514623b14178ebda32ce36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7616243f22a14d0e926c2253aa8fc20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f6dd876999740b7887d65bca381d93c",
              "IPY_MODEL_fbdef16fe49b41a7873f5d3140417f33",
              "IPY_MODEL_7cee710326954028b621d1c6fbc13cbc"
            ],
            "layout": "IPY_MODEL_91b299375c434d61a156209b1c80cc68"
          }
        },
        "2f6dd876999740b7887d65bca381d93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ede3e878164a35b4d8be9019bb9d6a",
            "placeholder": "​",
            "style": "IPY_MODEL_434d247651cd4849a2080abb22abcd15",
            "value": "100%"
          }
        },
        "fbdef16fe49b41a7873f5d3140417f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f8dcc24ac141eeaf1d1aa9f4f8e4c9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_326cf04cf8894988b54a6fc6e3cab764",
            "value": 2
          }
        },
        "7cee710326954028b621d1c6fbc13cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff0c6bb5ceff47c5b9db3fb25a8cb13d",
            "placeholder": "​",
            "style": "IPY_MODEL_9d1549a4442e43d7a39845b02c721e36",
            "value": " 2/2 [00:00&lt;00:00, 60.42it/s]"
          }
        },
        "91b299375c434d61a156209b1c80cc68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ede3e878164a35b4d8be9019bb9d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434d247651cd4849a2080abb22abcd15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9f8dcc24ac141eeaf1d1aa9f4f8e4c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "326cf04cf8894988b54a6fc6e3cab764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff0c6bb5ceff47c5b9db3fb25a8cb13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d1549a4442e43d7a39845b02c721e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a563312efc5a4dee80da84452129fec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b8cbc0c11564f4894af80cd49980799",
              "IPY_MODEL_112cc84318644cdb85a3e7e970edebaf",
              "IPY_MODEL_3da31296e5c549eb81247492d86d0471"
            ],
            "layout": "IPY_MODEL_53088749485b4fbbab7b7c30e8f36bbb"
          }
        },
        "6b8cbc0c11564f4894af80cd49980799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89829e0523bf482cb534900730073cdd",
            "placeholder": "​",
            "style": "IPY_MODEL_396d8f8790174c1fa91f9b4e2de94cf3",
            "value": " 24%"
          }
        },
        "112cc84318644cdb85a3e7e970edebaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d697da96e893474c9d921caa879f3c63",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05ac4d0b466f4a85965ef553aa98c6a2",
            "value": 24355
          }
        },
        "3da31296e5c549eb81247492d86d0471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bceaeff715a14b2f80d53a8371ff8fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_395e7c1ba98c4bc4bc99b73295db6514",
            "value": " 24355/100000 [12:28&lt;46:19, 27.21it/s]"
          }
        },
        "53088749485b4fbbab7b7c30e8f36bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89829e0523bf482cb534900730073cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396d8f8790174c1fa91f9b4e2de94cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d697da96e893474c9d921caa879f3c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05ac4d0b466f4a85965ef553aa98c6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bceaeff715a14b2f80d53a8371ff8fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "395e7c1ba98c4bc4bc99b73295db6514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiormazza/IA025_2022S1/blob/main/ex09/fabio_mazza/Aula_9_Exerc%C3%ADcio_Final4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Fabio Renato Zocal Mazza'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7901d170-ed4d-4d29-e172-bfcf1fed1bc7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Fabio Renato Zocal Mazza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "8dbb7099-99e2-40c5-85bc-6e6ccc5bcbee"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "\n",
        "from collections import OrderedDict\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c3ddc5-6b95-4c39-859b-a736746ae8ae"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 11 01:22:23 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    34W / 250W |   9619MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e0caf5-21c2-4eef-f8b7-771611761059"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        \n",
        "        self.max_seq_length = max_seq_length\n",
        "        cls = 101\n",
        "        sep = 100\n",
        "        tokens_all = torch.tensor([])\n",
        "        inputs = torch.tensor([])\n",
        "        targets = torch.tensor([])\n",
        " \n",
        "        for text in tqdm.notebook.tqdm(texts):\n",
        "            tokens = tokenize(text, tokenizer)\n",
        "            tokens = [cls] + tokens + [sep]\n",
        "            tokens = torch.FloatTensor(tokens)\n",
        "            tokens_all = torch.cat((tokens_all, tokens))\n",
        "            #print(tokens_all)\n",
        "        \n",
        "        pad_size = max_seq_length - tokens_all.size(dim=0) % max_seq_length\n",
        "        tokens_all = torch.cat((tokens_all, torch.zeros(pad_size)), dim=0) #pad_token_id = 0 (review if otherwise) / tokens_uns is the tokens tensor before reshaping\n",
        "        inputs = tokens_all.reshape((-1, max_seq_length))\n",
        "\n",
        "        t_tokens = torch.roll(tokens_all, -1, 0)\n",
        "        t_tokens[-1] = 0\n",
        "        targets = t_tokens.reshape((-1, max_seq_length))\n",
        "        \n",
        "        self.inputs = torch.LongTensor(inputs.numpy())\n",
        "        self.targets = torch.LongTensor(targets.numpy())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "#assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "#assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "#print('Passou no assert de dataset.')\n",
        "\n",
        "print(first_batch_input, '\\n', first_batch_target)"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "1758e3b995d94ec5875f9ab7b973192b",
            "072c279a73234c23b0ad6bf31009b277",
            "81bb4df0c7f74618bc62ef31882bef8e",
            "414ce44f152c41bd8962d4ce2df19375",
            "1f892b9fb51543da9642777a43966954",
            "480b8974efc44ba59ef6ad5f793fe0c7",
            "b19ea554751f41e298bff8d51cbe513e",
            "4d4068b129ea45d4a25733635937b777",
            "ebba2d2bf6864940b823f49efacbea3b",
            "b534c2e7a059484a9297336d246cdad5",
            "2704cb8364514623b14178ebda32ce36"
          ]
        },
        "outputId": "ed97de06-c010-43f1-f542-dad1886e0995"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1758e3b995d94ec5875f9ab7b973192b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "tensor([[  101,  3396, 10303,   125, 13239,   100,   101,  1660,  5971],\n",
            "        [  785,   125,  1847, 13779, 15616,   100,     0,     0,     0]]) \n",
            " tensor([[ 3396, 10303,   125, 13239,   100,   101,  1660,  5971,   785],\n",
            "        [  125,  1847, 13779, 15616,   100,     0,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Teste com frases longas\n",
        "\n",
        "dummy_texts = ['Os primeiros socorros foram feitos no local e a vítima foi levada \\\n",
        "ao Hospital de Base de Brasília com ferimentos leves, consciente e com estado de saúde estável.', \n",
        "'Um veículo capotou na Via Estrutural na tarde desta sexta-feira (3), por volta das 15h. O Corpo\\\n",
        " de Bombeiros Militar do Distrito Federal (CBMDF) foi chamado ao local para realizar os primeiros atendimentos à vítima.']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "print(first_batch_input)\n",
        "print(first_batch_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "7616243f22a14d0e926c2253aa8fc20a",
            "2f6dd876999740b7887d65bca381d93c",
            "fbdef16fe49b41a7873f5d3140417f33",
            "7cee710326954028b621d1c6fbc13cbc",
            "91b299375c434d61a156209b1c80cc68",
            "b4ede3e878164a35b4d8be9019bb9d6a",
            "434d247651cd4849a2080abb22abcd15",
            "b9f8dcc24ac141eeaf1d1aa9f4f8e4c9",
            "326cf04cf8894988b54a6fc6e3cab764",
            "ff0c6bb5ceff47c5b9db3fb25a8cb13d",
            "9d1549a4442e43d7a39845b02c721e36"
          ]
        },
        "id": "hV8mOhS90Mdt",
        "outputId": "34ab7c1b-41e1-4c10-bc37-f598fbc7f3c2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7616243f22a14d0e926c2253aa8fc20a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,   533,  1867, 19279,   444,   506,  5159,   202,  1238],\n",
            "        [  122,   123,  8329,   262, 11067,   320,  6297,   125,  8831],\n",
            "        [  125,  6191,   170, 16811, 14698,   117, 19054,   122,   170],\n",
            "        [ 1177,   125,  3231, 12055,   119,   100,   101,  1263,  7594],\n",
            "        [  853,  3356,   203,   229,  7811,  9009,   381,  1441,   229],\n",
            "        [ 1373,  1014,  7250,   118, 14258,   113,   511,   114,   117]])\n",
            "tensor([[  533,  1867, 19279,   444,   506,  5159,   202,  1238,   122],\n",
            "        [  123,  8329,   262, 11067,   320,  6297,   125,  8831,   125],\n",
            "        [ 6191,   170, 16811, 14698,   117, 19054,   122,   170,  1177],\n",
            "        [  125,  3231, 12055,   119,   100,   101,  1263,  7594,   853],\n",
            "        [ 3356,   203,   229,  7811,  9009,   381,  1441,   229,  1373],\n",
            "        [ 1014,  7250,   118, 14258,   113,   511,   114,   117,   240]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Teste dos métodos\n",
        "\n",
        "print(len(first_batch_input))\n",
        "print(dummy_dataset[0])"
      ],
      "metadata": {
        "id": "vXB2k2cXqghe",
        "outputId": "a99a90f7-f506-4164-ca1b-f12e21d08bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "(tensor([  101,   533,  1867, 19279,   444,   506,  5159,   202,  1238]), tensor([  533,  1867, 19279,   444,   506,  5159,   202,  1238,   122]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "1626d255-51be-49d7-a60f-eeb3969adc35"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample-1gb.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = 18\n",
        "\n",
        "train_examples = 100000\n",
        "valid_examples = 10000\n",
        "test_examples = 10000\n",
        "\n",
        "texts = open('sample-1gb.txt').readlines()\n",
        "\n",
        "print(f'Read {len(texts)} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "texts = texts[:max_lines]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a563312efc5a4dee80da84452129fec9",
            "6b8cbc0c11564f4894af80cd49980799",
            "112cc84318644cdb85a3e7e970edebaf",
            "3da31296e5c549eb81247492d86d0471",
            "53088749485b4fbbab7b7c30e8f36bbb",
            "89829e0523bf482cb534900730073cdd",
            "396d8f8790174c1fa91f9b4e2de94cf3",
            "d697da96e893474c9d921caa879f3c63",
            "05ac4d0b466f4a85965ef553aa98c6a2",
            "bceaeff715a14b2f80d53a8371ff8fb4",
            "395e7c1ba98c4bc4bc99b73295db6514"
          ]
        },
        "outputId": "81daef31-dea2-4c41-dc23-aa35f7cba89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 250000 lines.\n",
            "Truncating to 120000 lines.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a563312efc5a4dee80da84452129fec9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Número médio de palavras em datasets\n",
        "avlen = 0\n",
        "\n",
        "for text in training_texts:\n",
        "    avlen += len(text.split())/len(training_texts)\n",
        "\n",
        "print(avlen)\n",
        "print(training_texts[-10])\n",
        "print(len(training_texts[-10]))"
      ],
      "metadata": {
        "id": "tuxN3xE0sVp8",
        "outputId": "3d91280b-1075-4b1b-c514-162753432d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "825.7911000000006\n",
            "Content on this page requires a newer version of Adobe Flash Player. Workshop discute matérias primas para bioenergia e energias renováveis do Brasil Para fomentar a discussão em torno das oportunidades de energias renováveis no Brasil e as matérias primas de bioenergia, a Agência Paulista de Tecnologia dos Agronegócios – APTA – realiza o X Worshop Agroenergia, em Ribeirão Preto. O evento teve início nesta quarta-feira (22) e segue até esta quinta (23), com palestras, mesas redondas e apresentações de trabalhos técnicos e visitas a campos experimentais, reunindo lideranças de diferentes segmentos e apresentando novidades tecnológicas. O tema \"Integração Cana-de-Açúcar x Grãos x Pecuária\" foi um dos assuntos abordados no primeiro dia do workshop, por meio da palestra ministrada pelo presidente da Fundação MS, Luis Alberto Moraes Novaes. \"A integração de diferentes culturas possibilita a melhora da produtividade e, consequentemente, ganhos para o produtor rural. Por isso, é importante o que tema seja amplamente discutido\", comenta. Outra prioridade do Workshop é mostrar o potencial de matérias primas para a produção de biomassa, bem como discutir as barreiras comerciais, econômicas, políticas e sanitárias que possam impedir o avanço do programa energético brasileiro. Também estão em pauta os avanços tecnológicos nas culturas potenciais para a bioenergia, com ênfase na conservação dos solos e aspectos fitossanitários. A programação prevê, ainda, apresentação de cases de Pesquisa e Desenvolvimento de matérias primas para produção e uso de Biocombustíveis, realçando a relação dos mesmos com a questão ambiental, além de provocar a intensificação do consórcio de culturas, alimentos e energia.\n",
            "\n",
            "1715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47930e6f-71c5-4be5-b1c4-d62e0b7ce116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 1318908\n",
            "valid examples: 126117\n",
            "test examples: 119748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class AttentionBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dim: int, L: int, V: int):\n",
        "\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        \n",
        "        self.D = dim\n",
        "        n_ff = 128\n",
        "\n",
        "        self.L = L\n",
        "        self.wq = nn.Linear(dim, dim, bias = False, device=device)\n",
        "        self.wk = nn.Linear(dim, dim, bias = False, device=device)\n",
        "        self.wv = nn.Linear(dim, dim, bias = False, device=device)\n",
        "        self.wo = nn.Linear(dim, dim, bias = False, device=device)\n",
        "\n",
        "        self.ff = nn.Sequential(OrderedDict([\n",
        "                              ('ff1', torch.nn.Linear(dim, n_ff, device=device)),\n",
        "                              ('relu', torch.nn.ReLU()),\n",
        "                              ('dropout', torch.nn.Dropout(p=0.20)),\n",
        "                              ('ff2', torch.nn.Linear(n_ff, dim, device=device, bias = False))\n",
        "        ]))\n",
        "    \n",
        "    def forward(self, X):\n",
        "\n",
        "        residual = X\n",
        "\n",
        "        Q = self.wq(X)\n",
        "        K = self.wk(X)\n",
        "        V = self.wv(X)\n",
        "        scores = torch.matmul(Q, K.permute(0, 2, 1))/math.sqrt(self.D)\n",
        "        \n",
        "        #causal mask\n",
        "        mask = torch.ones((self.L, self.L), dtype=torch.bool)\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        scores[:, mask] = -1000000000\n",
        "\n",
        "        probs = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        E = torch.matmul(probs, V)\n",
        "        E = self.wo(E)\n",
        "        out = self.ff(E)\n",
        "        out = out + residual\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "        \"\"\"\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.V = vocab_size\n",
        "        self.L = max_seq_length\n",
        "        self.D = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        n_linear_1 = 128\n",
        "        n_linear_2 = 64\n",
        "\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, dim, device=device)\n",
        "        self.positional = torch.nn.Parameter(torch.randn(max_seq_length, dim, device=device)/10000)\n",
        "\n",
        "        self.attention = nn.Sequential(OrderedDict([\n",
        "                          ('attention_1', AttentionBlock(dim, self.L, self.V)),\n",
        "                          ('attention_2', AttentionBlock(dim, self.L, self.V))\n",
        "        ]))\n",
        "\n",
        "        self.linear = nn.Sequential(OrderedDict([\n",
        "                              ('l1', torch.nn.Linear(dim, n_linear_2, device=device)),\n",
        "                              ('relu', torch.nn.ReLU()),\n",
        "                              ('dropout', torch.nn.Dropout(p=0.20)),\n",
        "                              ('l2', torch.nn.Linear(n_linear_2, vocab_size, device=device, bias = False))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        X = self.embedding(inputs)\n",
        "        X = X + self.positional\n",
        "        E = self.attention(X)\n",
        "        out = self.linear(E)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09c4032-95b2-47b4-f7f9-165158e5241b"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f35aebe-7399-484c-e09a-18b0e62486b6"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 3884160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4c539b-66cf-43a4-9a03-240f58f3bb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              30278\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ff3d26-b065-4a9f-9aec-120a1eba485e"
      },
      "source": [
        "max_examples = 100_000_000\n",
        "eval_every_steps = 1000\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=128,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=1024)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 30339.61, valid ppl: 30257.52\n",
            "10000 steps; 640000 examples so far; train ppl: 676.22, valid ppl: 291.14\n",
            "20000 steps; 1280000 examples so far; train ppl: 251.62, valid ppl: 164.42\n",
            "30000 steps; 1920000 examples so far; train ppl: 140.22, valid ppl: 83.75\n",
            "40000 steps; 2560000 examples so far; train ppl: 107.84, valid ppl: 59.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "outputId": "633a9645-bf5f-4fca-f5c7-afc448c3cf8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 48.250447037509566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'O presidente do Senado, Rodrigo Pacheco, se'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "outputId": "954d2217-f398-4c7d-ef1b-b906fac934c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O presidente do Senado, Rodrigo Pacheco, se -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - - - - - -\n",
            "O presidente do Senado, Rodrigo Pacheco, se - - - - - - - - - - - - - - - - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Houve uma compreensão de que todas essas iniciativas deveriam'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "qRj9Gwdh6ZKh",
        "outputId": "726651db-527f-44db-f0d3-0e2344016e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD],\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Houve uma compreensão de que todas essas iniciativas deveriam [PAD] [PAD] [PAD] [PAD], [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}