{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Exercício - Leonardo de Lellis Rossi RA261900",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09e80b5d1a554fe8834bbf48e1b5af86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b682d6e457b2412b826d43e24da9df52",
              "IPY_MODEL_0edac4daed3047ee9848069dca661d22",
              "IPY_MODEL_3953f955822a4480a2b3da8297c83920"
            ],
            "layout": "IPY_MODEL_973acdc571cc438291cf95bdc616a832"
          }
        },
        "b682d6e457b2412b826d43e24da9df52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b6c76c2c8d40c6942bfcc2d7d38178",
            "placeholder": "​",
            "style": "IPY_MODEL_f97d60d662c341419c75dd686eb69bed",
            "value": "Downloading: 100%"
          }
        },
        "0edac4daed3047ee9848069dca661d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7879537cabf74caebfbbf849cf0d8c70",
            "max": 209528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9efa61bb1fdd4904886f0ceda53f5863",
            "value": 209528
          }
        },
        "3953f955822a4480a2b3da8297c83920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b007b09c8a4e01af9b9371349100e1",
            "placeholder": "​",
            "style": "IPY_MODEL_c60ced1691024dad857b78760f6d23df",
            "value": " 205k/205k [00:00&lt;00:00, 964kB/s]"
          }
        },
        "973acdc571cc438291cf95bdc616a832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b6c76c2c8d40c6942bfcc2d7d38178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97d60d662c341419c75dd686eb69bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7879537cabf74caebfbbf849cf0d8c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9efa61bb1fdd4904886f0ceda53f5863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12b007b09c8a4e01af9b9371349100e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60ced1691024dad857b78760f6d23df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73f8f649a09649b485aed5afe8234bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d484eaa3aa934afba2c7872d67ad16c1",
              "IPY_MODEL_2ec85663e23a4915a2de6cee4b6e85d3",
              "IPY_MODEL_ea6043764d9448c0bef107e0637691c7"
            ],
            "layout": "IPY_MODEL_73e001e064c143499a731447fa11c329"
          }
        },
        "d484eaa3aa934afba2c7872d67ad16c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e482bda732f840ca8ea2c568b8af3cb2",
            "placeholder": "​",
            "style": "IPY_MODEL_640821a343ca4b479206d9c1271d1433",
            "value": "Downloading: 100%"
          }
        },
        "2ec85663e23a4915a2de6cee4b6e85d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9b02cffecd47ccbb4ab3f9ccda07a5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3e055e447e0405dae5af65647784252",
            "value": 2
          }
        },
        "ea6043764d9448c0bef107e0637691c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_547f1c163828434e880734a71226ed19",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b3116920614a86a52529ff45a30e31",
            "value": " 2.00/2.00 [00:00&lt;00:00, 58.5B/s]"
          }
        },
        "73e001e064c143499a731447fa11c329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e482bda732f840ca8ea2c568b8af3cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640821a343ca4b479206d9c1271d1433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9b02cffecd47ccbb4ab3f9ccda07a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e055e447e0405dae5af65647784252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "547f1c163828434e880734a71226ed19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b3116920614a86a52529ff45a30e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11e0f4f9f5294cac83045812067d6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1611076685b04d57a5a0add8b211a40f",
              "IPY_MODEL_365b84286b3d4dcb9c9b858888051180",
              "IPY_MODEL_3f411e8985b34443b38679d12a4e0208"
            ],
            "layout": "IPY_MODEL_6298849545e5430facc3037005fd9a99"
          }
        },
        "1611076685b04d57a5a0add8b211a40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57d93bda92a447ee8a4b5fc510a078b5",
            "placeholder": "​",
            "style": "IPY_MODEL_182c3b13491946669f73159317b3a127",
            "value": "Downloading: 100%"
          }
        },
        "365b84286b3d4dcb9c9b858888051180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c49ac5c9a64158bfaca571de8599b8",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d86a723e26b140a6bec9d5bef89c4d6c",
            "value": 112
          }
        },
        "3f411e8985b34443b38679d12a4e0208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24166ae9413a4a90be0b1fdaf24f1a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_d3a7568c034749399307fb7a12701141",
            "value": " 112/112 [00:00&lt;00:00, 1.55kB/s]"
          }
        },
        "6298849545e5430facc3037005fd9a99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57d93bda92a447ee8a4b5fc510a078b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182c3b13491946669f73159317b3a127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c49ac5c9a64158bfaca571de8599b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d86a723e26b140a6bec9d5bef89c4d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24166ae9413a4a90be0b1fdaf24f1a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3a7568c034749399307fb7a12701141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1e89ad739cd4838a32ec7140c23c392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_537915261caf4b879feae4f8e7895339",
              "IPY_MODEL_0e625ebc129542b7ae7ed98cc8cf05bb",
              "IPY_MODEL_54bc6e21cd6b4da58cc17de06a643e32"
            ],
            "layout": "IPY_MODEL_d2535a1eb79a4224943b9bf9c0927ffa"
          }
        },
        "537915261caf4b879feae4f8e7895339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ff8c36ec804397ab48f90c90c5ef83",
            "placeholder": "​",
            "style": "IPY_MODEL_5e1f9e30243040919d19dc3beec83005",
            "value": "Downloading: 100%"
          }
        },
        "0e625ebc129542b7ae7ed98cc8cf05bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21aa93cf72e74d0292df604b5abb16c4",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_570ae333238b4d5aaafa4cd6e2070862",
            "value": 43
          }
        },
        "54bc6e21cd6b4da58cc17de06a643e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6db80031e584191a1b28c24fe9b245c",
            "placeholder": "​",
            "style": "IPY_MODEL_5fdbc1a5aa4641da98394db7b5374372",
            "value": " 43.0/43.0 [00:00&lt;00:00, 372B/s]"
          }
        },
        "d2535a1eb79a4224943b9bf9c0927ffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ff8c36ec804397ab48f90c90c5ef83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e1f9e30243040919d19dc3beec83005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21aa93cf72e74d0292df604b5abb16c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570ae333238b4d5aaafa4cd6e2070862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6db80031e584191a1b28c24fe9b245c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdbc1a5aa4641da98394db7b5374372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f14d59c0c5b4cf88edb2619fc013673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25945bb060b44ca1b1c97eb9d61ca899",
              "IPY_MODEL_4a61c3f355974eb78cb8575fad8638ce",
              "IPY_MODEL_8f018e22d8fb4e2fbba7ac90d66f2371"
            ],
            "layout": "IPY_MODEL_d093fc64685543dd9071cea26c152db2"
          }
        },
        "25945bb060b44ca1b1c97eb9d61ca899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3102e97d9939479891e1838ac6eb289b",
            "placeholder": "​",
            "style": "IPY_MODEL_5e78157ef4f04674b39e5cd0ed4e16a4",
            "value": "Downloading: 100%"
          }
        },
        "4a61c3f355974eb78cb8575fad8638ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f83c1a864bd447c6a6440146d4c5c08a",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72ebf98e821b4e78b2b3cf0103473a6b",
            "value": 647
          }
        },
        "8f018e22d8fb4e2fbba7ac90d66f2371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f4f14aa05e24af2ae7095ad0b5a4549",
            "placeholder": "​",
            "style": "IPY_MODEL_e88d6a7c7d684e6a9912a270d77cb9e4",
            "value": " 647/647 [00:00&lt;00:00, 5.86kB/s]"
          }
        },
        "d093fc64685543dd9071cea26c152db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3102e97d9939479891e1838ac6eb289b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e78157ef4f04674b39e5cd0ed4e16a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f83c1a864bd447c6a6440146d4c5c08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ebf98e821b4e78b2b3cf0103473a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f4f14aa05e24af2ae7095ad0b5a4549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88d6a7c7d684e6a9912a270d77cb9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex09/leonardo_lellis/Aula_9_Exerc%C3%ADcio_Leonardo_de_Lellis_Rossi_RA261900_batch_encode_plus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Leonardo de Lellis Rossi RA261900\"\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0c61f1-23c0-4afb-f68d-21ed78d68c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Leonardo de Lellis Rossi RA261900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "69ab5067-9ed1-46e5-ec4a-5c22c1e01adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "checkinpoint = True\n",
        "save_in_drive = True\n",
        "\n",
        "params = {\n",
        "    'max_examples': 150_000_000,\n",
        "    'eval_every_steps': 10000,\n",
        "    'lr': 3e-4,\n",
        "    'batch_size': 64,\n",
        "    'embedding_dim': 128,\n",
        "    'hidden_size': 2*128,\n",
        "    'optimizer': 'Adam',\n",
        "    'retrain': False,\n",
        "    'path_saved_model': 'gdrive/MyDrive/Colab Notebooks/best_model_',\n",
        "    'path_saved_datasets': 'gdrive/MyDrive/Colab Notebooks/ds_',\n",
        "    'download_ds': False,\n",
        "    'aula': 'Aula9',\n",
        "    'max_seq_length': 9,\n",
        "    'train_examples': 175_000,\n",
        "    'valid_examples': 50_000,\n",
        "    'test_examples': 25_000,\n",
        "    'n_heads':4\n",
        "}\n",
        "params['path_saved_model'] = params['path_saved_model']+params['aula']+'_BS'+str(params['batch_size'])+'_HS'+str(params['hidden_size'])+'_EmbDim'+str(params['embedding_dim'])+'_MaxEx'+str(params['max_examples'])+'.pt'"
      ],
      "metadata": {
        "id": "dnfTE2XvSr3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68135a51-f653-4732-c1cb-502dc3882d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  1 12:00:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bb2f27-bb7c-4a20-8948-00259f095f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "  torch.cuda.manual_seed(123)\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "-O1Gy-aeUbv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neptune config"
      ],
      "metadata": {
        "id": "9D9O0IdFUKdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install -U neptune-client\n",
        " import neptune.new as neptune"
      ],
      "metadata": {
        "id": "Ua-dPVPgUM3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e5725c-0aed-4a7e-dffe-295804ed910c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.16.3.tar.gz (317 kB)\n",
            "\u001b[K     |████████████████████████████████| 317 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.24.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.0\n",
            "  Downloading botocore-1.27.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 52.3 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.0->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2022.5.18.1)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 68.3 MB/s \n",
            "\u001b[?25hCollecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2022.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.8.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.6)\n",
            "Building wheels for collected packages: neptune-client, future\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.16.3-py2.py3-none-any.whl size=570148 sha256=61979eb81cca1cfe67953fe2ebb84356f5d32c94d7260652aa51add9f2d5e2eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/67/63/794a7079b23b633de6a77fb7e9427e368980a755c1bc52a814\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=83897d6521694913c9ada9fd6656bf86fadf5b6e6b569d7c7da338ff97c0bf43\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built neptune-client future\n",
            "Installing collected packages: arrow, webcolors, urllib3, uri-template, rfc3987, rfc3339-validator, jsonpointer, jmespath, isoduration, fqdn, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 PyJWT-2.4.0 arrow-1.2.2 boto3-1.24.0 botocore-1.27.0 bravado-11.0.3 bravado-core-5.17.0 fqdn-1.5.1 future-0.18.2 gitdb-4.0.9 isoduration-20.11.0 jmespath-1.0.0 jsonpointer-2.3 jsonref-0.2 monotonic-1.6 neptune-client-0.16.3 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.0 simplejson-3.17.6 smmap-5.0.0 swagger-spec-validator-2.7.4 uri-template-1.2.0 urllib3-1.25.11 webcolors-1.12 websocket-client-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = neptune.init(name= params['aula'], tags=[params['aula'], 'Auto-atenção', 'Self-Attention', 'batch_encode_plus', 'checkinpoint', 'CrossEntropy', 'Adam', 'perplexity', 'BrWaC'],\n",
        "    project=\"leolellisr/dl-ia025\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NjY1YmJkZi1hYmM5LTQ3M2QtOGU1ZC1iZTFlNWY4NjE1NDQifQ==\",\n",
        ")"
      ],
      "metadata": {
        "id": "KNvj9TrsUOdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abefc749-c66e-4a22-df34-1ebb4838936b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-107\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['parameters'] = params"
      ],
      "metadata": {
        "id": "2Ld1XqRPVh21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import itertools"
      ],
      "metadata": {
        "id": "K2517Ek9ZBv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(texts, tokenizer, max_length):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    # return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "    # tokenizer.batch_encode_plus returns dict with input_ids and attention_mask, pads and trucates the sentences \n",
        "    return tokenizer.batch_encode_plus(\n",
        "                    texts,       # Batch of sentences to encode.\n",
        "                    add_special_tokens = True,  # Add '[CLS]'\n",
        "                    padding = 'longest',        # Pad to longest in batch.\n",
        "                    truncation = True,          # Truncate sentences to `max_length`.\n",
        "                    max_length = max_length,           \n",
        "                    return_attention_mask = True, # Construct attn. masks.\n",
        "                    return_tensors = 'pt'        # Return pytorch tensors.\n",
        "            )\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        \n",
        "\n",
        "        # smart batching. Como sugestão do prof. Rodrigo na última aula, os \n",
        "        # textos foram ordenados de acordo com seu tamanho para otimizar operações\n",
        "        self.tokenizer = tokenizer\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "        self.att = []\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.sorted_texts = sorted(texts, key=lambda x: len(x[0]))\n",
        "        sorted_texts = self.sorted_texts\n",
        "        aux_doc = 0\n",
        "        self.aux_seq = 0\n",
        "        if debug: print(f'len sorted_texts: {len(sorted_texts)}')\n",
        "        while aux_doc <= len(texts)-1:\n",
        "          if debug: print(f'aux_doc: {aux_doc} <= (texts-1):{len(texts)}')\n",
        "          batch_encoded = tokenize(texts=sorted_texts[aux_doc:aux_doc+batch_size], tokenizer=tokenizer, max_length=max_seq_length)\n",
        "          aux_text = 0\n",
        "          for i, text in enumerate(batch_encoded['input_ids']):\n",
        "            if aux_text < len(text) - 2:\n",
        "              if debug: print(f'aux_text: {aux_text} < (len(text)-2):{len(text) - 2}')\n",
        "\n",
        "              while True:\n",
        "                if aux_text <  len(text) - 2: \n",
        "                  if debug: print(f'aux_text: {aux_text} < (len(text)-2):{len(text) - 2}')\n",
        "\n",
        "                  if aux_text + self.max_seq_length <= len(text): # não precisa completar com pad\n",
        "                    if debug: print(f'(aux_text + self.max_seq_length): {aux_text + self.max_seq_length} < len(text):{len(text)}')\n",
        "\n",
        "                    self.X.extend(batch_encoded['input_ids'][aux_text:aux_text + self.max_seq_length])\n",
        "                    self.att.extend(batch_encoded['attention_mask'][aux_text:aux_text + self.max_seq_length])\n",
        "\n",
        "                  else:\n",
        "                    self.X.extend(batch_encoded['input_ids'][aux_text:aux_text] + [self.pad_token_id for i in range(max_seq_length-(len(text)-aux_text))])\n",
        "                    self.att.extend(batch_encoded['attention_mask'][aux_text:aux_text] + [0 for i in range(max_seq_length-(len(text)-aux_text))])\n",
        "                  aux_text += max_seq_length\n",
        "                  self.aux_seq += 1\n",
        "                else:\n",
        "                  if debug: print(f'aux_text: {aux_text} > (len(text)-2):{len(text) - 2}. Break')\n",
        "                  break\n",
        "          aux_doc += params['batch_size']\n",
        "          if aux_doc % 1000 == 0:\n",
        "            print(f'\\MyDataset Doc {aux_doc+1}; Seq: {self.aux_seq}; Date/Time: {time.strftime(\"[%Y-%b-%d %H:%M:%S]\")}')\n",
        "        \n",
        "        #self.X = X\n",
        "        #self.X = att\n",
        "        if debug: \n",
        "          print(f'X: {self.X}')\n",
        "          print(f'len X: {len(self.X)}')\n",
        "             \n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if idx == (len(self.X)-1) or (self.X[idx+1][0] == self.tokenizer.cls_token_id):\n",
        "          y = torch.cat((torch.tensor(self.X[idx][1:]), torch.tensor([self.tokenizer.pad_token_id])),0)\n",
        "        else: \n",
        "          y = torch.cat((torch.tensor(self.X[idx][1:]),self.X[idx+1][0].unsqueeze(0)),0) \n",
        "        return torch.tensor(self.X[idx]), y, torch.tensor(self.att[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n"
      ],
      "metadata": {
        "id": "NOwJMEllqm4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "09e80b5d1a554fe8834bbf48e1b5af86",
            "b682d6e457b2412b826d43e24da9df52",
            "0edac4daed3047ee9848069dca661d22",
            "3953f955822a4480a2b3da8297c83920",
            "973acdc571cc438291cf95bdc616a832",
            "f6b6c76c2c8d40c6942bfcc2d7d38178",
            "f97d60d662c341419c75dd686eb69bed",
            "7879537cabf74caebfbbf849cf0d8c70",
            "9efa61bb1fdd4904886f0ceda53f5863",
            "12b007b09c8a4e01af9b9371349100e1",
            "c60ced1691024dad857b78760f6d23df",
            "73f8f649a09649b485aed5afe8234bf0",
            "d484eaa3aa934afba2c7872d67ad16c1",
            "2ec85663e23a4915a2de6cee4b6e85d3",
            "ea6043764d9448c0bef107e0637691c7",
            "73e001e064c143499a731447fa11c329",
            "e482bda732f840ca8ea2c568b8af3cb2",
            "640821a343ca4b479206d9c1271d1433",
            "ee9b02cffecd47ccbb4ab3f9ccda07a5",
            "d3e055e447e0405dae5af65647784252",
            "547f1c163828434e880734a71226ed19",
            "d7b3116920614a86a52529ff45a30e31",
            "11e0f4f9f5294cac83045812067d6336",
            "1611076685b04d57a5a0add8b211a40f",
            "365b84286b3d4dcb9c9b858888051180",
            "3f411e8985b34443b38679d12a4e0208",
            "6298849545e5430facc3037005fd9a99",
            "57d93bda92a447ee8a4b5fc510a078b5",
            "182c3b13491946669f73159317b3a127",
            "63c49ac5c9a64158bfaca571de8599b8",
            "d86a723e26b140a6bec9d5bef89c4d6c",
            "24166ae9413a4a90be0b1fdaf24f1a4a",
            "d3a7568c034749399307fb7a12701141",
            "d1e89ad739cd4838a32ec7140c23c392",
            "537915261caf4b879feae4f8e7895339",
            "0e625ebc129542b7ae7ed98cc8cf05bb",
            "54bc6e21cd6b4da58cc17de06a643e32",
            "d2535a1eb79a4224943b9bf9c0927ffa",
            "02ff8c36ec804397ab48f90c90c5ef83",
            "5e1f9e30243040919d19dc3beec83005",
            "21aa93cf72e74d0292df604b5abb16c4",
            "570ae333238b4d5aaafa4cd6e2070862",
            "d6db80031e584191a1b28c24fe9b245c",
            "5fdbc1a5aa4641da98394db7b5374372",
            "7f14d59c0c5b4cf88edb2619fc013673",
            "25945bb060b44ca1b1c97eb9d61ca899",
            "4a61c3f355974eb78cb8575fad8638ce",
            "8f018e22d8fb4e2fbba7ac90d66f2371",
            "d093fc64685543dd9071cea26c152db2",
            "3102e97d9939479891e1838ac6eb289b",
            "5e78157ef4f04674b39e5cd0ed4e16a4",
            "f83c1a864bd447c6a6440146d4c5c08a",
            "72ebf98e821b4e78b2b3cf0103473a6b",
            "1f4f14aa05e24af2ae7095ad0b5a4549",
            "e88d6a7c7d684e6a9912a270d77cb9e4"
          ]
        },
        "outputId": "34e01cc9-cace-4b97-81b2-b15a233b5f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/205k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e80b5d1a554fe8834bbf48e1b5af86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73f8f649a09649b485aed5afe8234bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11e0f4f9f5294cac83045812067d6336"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e89ad739cd4838a32ec7140c23c392"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f14d59c0c5b4cf88edb2619fc013673"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params['batch_size'] = 10\n",
        "batch_size = params['batch_size'] \n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "print(f'len(dummy_dataset): {len(dummy_dataset)}')\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target, first_batch_att = next(iter(dummy_loader))\n",
        "\n",
        "# correct_first_batch_input = torch.LongTensor(\n",
        "#    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "#     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "# correct_first_batch_target = torch.LongTensor(\n",
        "#    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "#     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "# Agradecimentos ao Marcus Borella que disponibilizou assert com SEP no final da sentença\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     102,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     102]])\n",
        "\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     102,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,    102,     0]])\n",
        "\n",
        "if debug: print(first_batch_input)\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "\n",
        "if debug:  print(first_batch_target)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57c5605-18ae-458c-dae5-02c0f9ef5ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(dummy_dataset): 2\n",
            "Passou no assert de tamanho do dataset.\n",
            "Passou no assert de dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "if save_in_drive: drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIwYycaugsxY",
        "outputId": "3a464a1f-cd52-4e04-c347-660458a1296d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "839ef846-6293-488d-9b29-58e5887e15f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-01 12:09:22--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.181.128, 173.194.194.128, 64.233.191.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.181.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1230909256 (1.1G) [text/plain]\n",
            "Saving to: ‘sample-1gb.txt’\n",
            "\n",
            "sample-1gb.txt      100%[===================>]   1.15G   237MB/s    in 4.7s    \n",
            "\n",
            "2022-06-01 12:09:27 (252 MB/s) - ‘sample-1gb.txt’ saved [1230909256/1230909256]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = params['max_seq_length']\n",
        "\n",
        "train_examples = params['train_examples']\n",
        "valid_examples = params['valid_examples']\n",
        "test_examples = params['test_examples']\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "\n",
        "if params['download_ds']:\n",
        "  texts = open('sample-1gb.txt').readlines()\n",
        "  print(f'Read {len(texts)} lines.')\n",
        "  texts = texts[:max_lines]  \n",
        "\n",
        "  training_texts = texts[:-(valid_examples + test_examples)]\n",
        "  valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "  test_texts = texts[-test_examples:]\n",
        "\n",
        "  training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "  test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "else:\n",
        "  path = params['path_saved_datasets']+'train.pt'\n",
        "  training_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'val.pt'\n",
        "  valid_dataset = torch.load(path)\n",
        "  path = params['path_saved_datasets']+'test.pt'\n",
        "  test_dataset = torch.load(path)\n",
        "params['batch_size'] = 64\n",
        "batch_size = params['batch_size'] \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDari2UL78_5",
        "outputId": "65a71246-3129-4797-9c8c-79445cd1c3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncating to 250000 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b1b40d-cf96-4e3f-f65a-5474dd309da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 24615\n",
            "valid examples: 7038\n",
            "test examples: 3519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if save_in_drive and params['download_ds']:\n",
        "  torch.save(training_dataset, params['path_saved_datasets']+\"train.pt\")\n",
        "  torch.save(valid_dataset, params['path_saved_datasets']+\"val.pt\")\n",
        "  torch.save(test_dataset, params['path_saved_datasets']+\"test.pt\")"
      ],
      "metadata": {
        "id": "VeNzN6ZjWP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "CmjYdmqANlRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "# garantimos que os pesos vao ser sempre os mesmos.\n",
        "set_seeds()\n",
        "\n",
        "class SelfAttentionLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, padding_idx, n_heads, dim, max_length):\n",
        "        super().__init__()\n",
        "        # n_heads: H\n",
        "        # dim: D\n",
        "        # max_lenght: L\n",
        "        # vocab_lenght: V\n",
        "        self.H = n_heads\n",
        "        self.D = dim\n",
        "        self.L = max_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad = padding_idx\n",
        "\n",
        "        self.W_q = torch.nn.Linear(self.D, self.D, bias=False) # (D, D)\n",
        "        self.W_k = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_v = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "        self.W_o = torch.nn.Linear(self.D, self.D, bias=False)\n",
        "\n",
        "        self.layer_norm1  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.D, self.D*10),  \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(self.D*10, self.D)\n",
        "        )\n",
        "        self.layer_norm2  = torch.nn.LayerNorm(self.D, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, x, attMask, device):\n",
        "        B = len(x)\n",
        "        padMask = x != self.pad # B, L\n",
        "      \n",
        "        # multi-head self-attention\n",
        "\n",
        "        fQ = self.W_q(x).reshape(B, self.L, self.H, self.D_H) # (B, L, H, D/H)\n",
        "        fK = self.W_k(x).reshape(B, self.L, self.H, self.D_H)\n",
        "        fV = self.W_v(x).reshape(B, self.L, self.H, self.D_H)\n",
        "\n",
        "        # (B, L, H, D/H) -> (B, H, L, D/H)\n",
        "        fQ_transposed = fQ.transpose(1, 2)                    # (B, H, L, D/H)\n",
        "        fK_transposed = fK.transpose(1, 2)\n",
        "        fV_transposed = fV.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(fQ_transposed, fK_transposed.transpose(-2, -1)) / math.sqrt(self.D_H) # (B, H, L, L)\n",
        "        if attMask is not None:\n",
        "          if debug: \n",
        "            print(f'scores.shape: {scores.shape}')              # (B, H, L, L)\n",
        "            print(f'attMask.shape: {attMask.shape}')            # (B, L)\n",
        "          if scores.shape[0] == attMask.shape[0]: attMask = attMask[:, None, None, :].expand_as(scores)  # (B, 1, 1, L) -> (B, H, L, L)\n",
        "          if debug: print(f'mask.shape: {attMask.shape}')                  # (B, H, L, L)\n",
        "          #scores.masked_fill_(~mask, float('-inf'))           # (B, H, L, L)\n",
        "          #mask = attMask.unsqueeze(0)          \n",
        "          #scores = scores.reshape(-1, scores.shape[0], scores.shape[2])\n",
        "          if scores.shape[0] == attMask.shape[0]: scores = scores + attMask\n",
        "          if debug: print(f'scores.masked_fill_: {scores.shape}') # (B, H, L, L)\n",
        "          \n",
        "        scores *= torch.FloatTensor([self.D_H]).to(device) ** (-.5)      # scaling \n",
        "        if debug: print(f\"scores (B, H, L, L): {scores.shape}\")  \n",
        "\n",
        "        probs = F.softmax(scores, dim=-1) # shape = B, L, L\n",
        "        if debug: print(f\"probs (B, H, L, L): {probs.shape}\")  \n",
        "\n",
        "        E = torch.matmul(probs, fV_transposed)\n",
        "        if debug: print(f\"E (B, H, L, D/H): {E.shape}\")  \n",
        "\n",
        "        out = E.transpose(1, 2).contiguous()                   # (B, L, H, D/H)\n",
        "        out = out.reshape(B, self.L, self.D) # (B, L, D) \n",
        "        out = self.W_o(out)\n",
        "        if debug: print(f\"out (B, L, D): {out.shape}\")  # (B, L, D)    \n",
        "\n",
        "        out = self.layer_norm1(x+out)               # (B, L, D) \n",
        "        out = self.feed_forward(out)\n",
        "        out = self.layer_norm2(x+out)           # (B, L, D) \n",
        "        #att_mean = att_norm2 * padMask.unsqueeze(-1)      \n",
        "        #mean_embeddings = att_mean.sum(dim=1) / padMask.count_nonzero(-1).unsqueeze(1)  \n",
        "        return out"
      ],
      "metadata": {
        "id": "GgdE_7xnMp7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MsAs84KmQVXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "set_seeds()\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, hidden: int, n_heads):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "          \n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super().__init__()\n",
        "        self.H = n_heads\n",
        "        self.D = dim\n",
        "        self.L = max_seq_length\n",
        "        self.D_H = self.D // self.H # D / H\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.V = vocab_size\n",
        "        self.hidden_size = hidden\n",
        "\n",
        "        # word embedding \n",
        "        self.embeddings_c = nn.Embedding(self.V, self.D,padding_idx=pad_token_id)\n",
        "        self.embeddings_p = torch.nn.Linear(self.D, self.L, bias=False)\n",
        "\n",
        "        self.att_layer = SelfAttentionLayer(padding_idx=pad_token_id, n_heads=self.H, dim=self.D, max_length=self.L)\n",
        "\n",
        "        self.feed_forward = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim, hidden),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden, vocab_size)\n",
        "        )\n",
        "        \n",
        "\n",
        "        \n",
        "    def forward(self, inputs, attMask, device):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            B: batch_size\n",
        "            L: max_seq_length\n",
        "            D: embedding_dim\n",
        "            V: vocab_size\n",
        "            input shape: (B, L)\n",
        "            pos shape: (B, L)\n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        # input shape: (B, L)\n",
        "        x_emb = self.embeddings_c(inputs) + self.embeddings_p.weight  # (B, L, D)\n",
        "        if debug: print(f'shape x_emb: {x_emb.shape}')                # (B, L, D)\n",
        "        attMask = attMask.to(device) \n",
        "        logits =  self.att_layer(x_emb, attMask, device)              # (B, L, D)\n",
        "        if debug: print(f'shape logits: {logits.shape}')              # (B, L, D)\n",
        "        logits = self.feed_forward(logits)                            # (B, L, V)\n",
        "        if debug: print(f'shape logits (B, L, V): {logits.shape}')             # (B, L, V)\n",
        "        \n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4700a19-9ab8-40f9-aef1-21e0858c1b0b"
      },
      "source": [
        "debug = True\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    hidden = 128,\n",
        "    n_heads = 4,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "\n",
        "sample_input, _, attMask = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input,attMask,device)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1, 9, 64])\n",
            "scores.shape: torch.Size([1, 4, 9, 9])\n",
            "attMask.shape: torch.Size([1, 9])\n",
            "mask.shape: torch.Size([1, 4, 9, 9])\n",
            "scores.masked_fill_: torch.Size([1, 4, 9, 9])\n",
            "scores (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1, 9, 64])\n",
            "shape logits: torch.Size([1, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1, 9, 29794])\n",
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd48617b-70dd-4134-8d9f-b75a1221be94"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5858402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    if debug: \n",
        "      print(f'logits: {logits.shape}')\n",
        "      print(f'train_target_ids: {target.shape}')\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids, attMask = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model.forward(train_input_ids, attMask, device)\n",
        "if debug:\n",
        "  print(f'logits: {logits.shape}')\n",
        "  print(f'train_target_ids: {train_target_ids.shape}')\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "if debug:\n",
        "  print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "  print(f'my perplexity:              {int(my_perplexity)}')\n",
        "\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')\n",
        "run['perplexity'].log(my_perplexity) # Envia perplexity para o Neptune.\n"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ba5f1d-9de2-43d5-fd32-b76ba9d3137a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape x_emb: torch.Size([1000, 9, 64])\n",
            "scores.shape: torch.Size([1000, 4, 9, 9])\n",
            "attMask.shape: torch.Size([1000, 9])\n",
            "mask.shape: torch.Size([1000, 4, 9, 9])\n",
            "scores.masked_fill_: torch.Size([1000, 4, 9, 9])\n",
            "scores (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "probs (B, H, L, L): torch.Size([1000, 4, 9, 9])\n",
            "E (B, H, L, D/H): torch.Size([1000, 4, 9, 16])\n",
            "out (B, L, D): torch.Size([1000, 9, 64])\n",
            "shape logits: torch.Size([1000, 9, 64])\n",
            "shape logits (B, L, V): torch.Size([1000, 9, 29794])\n",
            "logits: torch.Size([1000, 9, 29794])\n",
            "train_target_ids: torch.Size([1000, 9])\n",
            "logits: torch.Size([9000, 29794])\n",
            "train_target_ids: torch.Size([9000])\n",
            "correct initial perplexity: 29794\n",
            "my perplexity:              29829\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YyE_DJkfJ8sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = params['batch_size']\n",
        "\n",
        "debug = False\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=params['max_seq_length'],\n",
        "    dim=params['embedding_dim'],\n",
        "    n_layers=2,\n",
        "    hidden = params['hidden_size'],\n",
        "    n_heads = params['n_heads'],\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "best_valid_ppl = 10e9\n",
        "\n",
        "if params['retrain']:\n",
        "  best_model = params['path_saved_model']\n",
        "  model = torch.load(best_model)\n",
        "\n",
        "def train_step(input_ids, target_ids, attMask, device):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids, attMask, device)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids, attMask, device):\n",
        "    model.eval()\n",
        "    logits = model(input_ids, attMask, device)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "MI7smWjSLY93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9964b1-c377-473f-eb42-ee752eb11fe1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < params['max_examples']:\n",
        "    for train_input_ids, train_target_ids, attMask in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device), attMask, device) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step %  params['eval_every_steps'] == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "            run['train/ppl'].log(train_ppl) # Envia train ppl para o Neptune.\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device), attMask, device)\n",
        "                    for val_input_ids, val_target_ids,_ in validation_loader]))\n",
        "            run['valid/ppl'].log(valid_ppl) # Envia valid ppl para o Neptune.\n",
        "            if checkinpoint and valid_ppl < best_valid_ppl:\n",
        "              torch.save(model.state_dict(), 'best_model.pt')\n",
        "              if save_in_drive: torch.save(model, params['path_saved_model'])\n",
        "              print(f\"Best model found in step {step}. valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f} \")\n",
        "              best_valid_ppl = valid_ppl\n",
        "            ex_least = n_examples/params['max_examples']*100\n",
        "            print(f'{step} steps; {n_examples} examples so far; {ex_least:.2f} % ; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= params['max_examples']:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model found in step 0. valid ppl: 30142.95, best_valid_ppl: 10000000000.00 \n",
            "0 steps; 0 examples so far; 0.00 % ; train ppl: 30710.86, valid ppl: 30142.95, best_valid_ppl: 30142.95\n",
            "Best model found in step 10000. valid ppl: 7.25, best_valid_ppl: 30142.95 \n",
            "10000 steps; 640000 examples so far; 0.43 % ; train ppl: 3.14, valid ppl: 7.25, best_valid_ppl: 7.25\n",
            "Best model found in step 20000. valid ppl: 4.50, best_valid_ppl: 7.25 \n",
            "20000 steps; 1280000 examples so far; 0.85 % ; train ppl: 1.00, valid ppl: 4.50, best_valid_ppl: 4.50\n",
            "30000 steps; 1920000 examples so far; 1.28 % ; train ppl: 1.00, valid ppl: 4.88, best_valid_ppl: 4.50\n",
            "Best model found in step 40000. valid ppl: 2.86, best_valid_ppl: 4.50 \n",
            "40000 steps; 2560000 examples so far; 1.71 % ; train ppl: 1.00, valid ppl: 2.86, best_valid_ppl: 2.86\n",
            "Best model found in step 50000. valid ppl: 2.32, best_valid_ppl: 2.86 \n",
            "50000 steps; 3200000 examples so far; 2.13 % ; train ppl: 1.00, valid ppl: 2.32, best_valid_ppl: 2.32\n",
            "Best model found in step 60000. valid ppl: 2.16, best_valid_ppl: 2.32 \n",
            "60000 steps; 3840000 examples so far; 2.56 % ; train ppl: 1.00, valid ppl: 2.16, best_valid_ppl: 2.16\n",
            "Best model found in step 70000. valid ppl: 2.06, best_valid_ppl: 2.16 \n",
            "70000 steps; 4480000 examples so far; 2.99 % ; train ppl: 1.00, valid ppl: 2.06, best_valid_ppl: 2.06\n",
            "Best model found in step 80000. valid ppl: 1.96, best_valid_ppl: 2.06 \n",
            "80000 steps; 5120000 examples so far; 3.41 % ; train ppl: 1.00, valid ppl: 1.96, best_valid_ppl: 1.96\n",
            "90000 steps; 5760000 examples so far; 3.84 % ; train ppl: 1.00, valid ppl: 1.98, best_valid_ppl: 1.96\n",
            "Best model found in step 100000. valid ppl: 1.83, best_valid_ppl: 1.96 \n",
            "100000 steps; 6400000 examples so far; 4.27 % ; train ppl: 1.00, valid ppl: 1.83, best_valid_ppl: 1.83\n",
            "Best model found in step 110000. valid ppl: 1.82, best_valid_ppl: 1.83 \n",
            "110000 steps; 7040000 examples so far; 4.69 % ; train ppl: 1.00, valid ppl: 1.82, best_valid_ppl: 1.82\n",
            "Best model found in step 120000. valid ppl: 1.76, best_valid_ppl: 1.82 \n",
            "120000 steps; 7680000 examples so far; 5.12 % ; train ppl: 1.00, valid ppl: 1.76, best_valid_ppl: 1.76\n",
            "130000 steps; 8320000 examples so far; 5.55 % ; train ppl: 1.00, valid ppl: 1.77, best_valid_ppl: 1.76\n",
            "Best model found in step 140000. valid ppl: 1.65, best_valid_ppl: 1.76 \n",
            "140000 steps; 8960000 examples so far; 5.97 % ; train ppl: 1.00, valid ppl: 1.65, best_valid_ppl: 1.65\n",
            "150000 steps; 9600000 examples so far; 6.40 % ; train ppl: 1.00, valid ppl: 1.72, best_valid_ppl: 1.65\n",
            "160000 steps; 10240000 examples so far; 6.83 % ; train ppl: 1.00, valid ppl: 1.84, best_valid_ppl: 1.65\n",
            "170000 steps; 10880000 examples so far; 7.25 % ; train ppl: 1.00, valid ppl: 1.72, best_valid_ppl: 1.65\n",
            "180000 steps; 11520000 examples so far; 7.68 % ; train ppl: 1.00, valid ppl: 1.74, best_valid_ppl: 1.65\n",
            "190000 steps; 12160000 examples so far; 8.11 % ; train ppl: 1.00, valid ppl: 1.71, best_valid_ppl: 1.65\n",
            "Best model found in step 200000. valid ppl: 1.57, best_valid_ppl: 1.65 \n",
            "200000 steps; 12800000 examples so far; 8.53 % ; train ppl: 1.00, valid ppl: 1.57, best_valid_ppl: 1.57\n",
            "210000 steps; 13440000 examples so far; 8.96 % ; train ppl: 1.00, valid ppl: 1.73, best_valid_ppl: 1.57\n",
            "220000 steps; 14080000 examples so far; 9.39 % ; train ppl: 1.00, valid ppl: 1.69, best_valid_ppl: 1.57\n",
            "Best model found in step 230000. valid ppl: 1.51, best_valid_ppl: 1.57 \n",
            "230000 steps; 14720000 examples so far; 9.81 % ; train ppl: 1.00, valid ppl: 1.51, best_valid_ppl: 1.51\n",
            "240000 steps; 15360000 examples so far; 10.24 % ; train ppl: 1.00, valid ppl: 1.68, best_valid_ppl: 1.51\n",
            "250000 steps; 16000000 examples so far; 10.67 % ; train ppl: 1.00, valid ppl: 1.67, best_valid_ppl: 1.51\n",
            "260000 steps; 16640000 examples so far; 11.09 % ; train ppl: 1.00, valid ppl: 2.01, best_valid_ppl: 1.51\n",
            "270000 steps; 17280000 examples so far; 11.52 % ; train ppl: 1.00, valid ppl: 1.70, best_valid_ppl: 1.51\n",
            "280000 steps; 17920000 examples so far; 11.95 % ; train ppl: 1.00, valid ppl: 1.75, best_valid_ppl: 1.51\n",
            "290000 steps; 18560000 examples so far; 12.37 % ; train ppl: 1.00, valid ppl: 1.79, best_valid_ppl: 1.51\n",
            "300000 steps; 19200000 examples so far; 12.80 % ; train ppl: 1.00, valid ppl: 1.66, best_valid_ppl: 1.51\n",
            "310000 steps; 19840000 examples so far; 13.23 % ; train ppl: 1.00, valid ppl: 1.67, best_valid_ppl: 1.51\n",
            "320000 steps; 20480000 examples so far; 13.65 % ; train ppl: 1.00, valid ppl: 1.60, best_valid_ppl: 1.51\n",
            "330000 steps; 21120000 examples so far; 14.08 % ; train ppl: 1.00, valid ppl: 1.60, best_valid_ppl: 1.51\n",
            "340000 steps; 21760000 examples so far; 14.51 % ; train ppl: 1.00, valid ppl: 1.64, best_valid_ppl: 1.51\n",
            "350000 steps; 22400000 examples so far; 14.93 % ; train ppl: 1.00, valid ppl: 1.59, best_valid_ppl: 1.51\n",
            "360000 steps; 23040000 examples so far; 15.36 % ; train ppl: 1.00, valid ppl: 1.68, best_valid_ppl: 1.51\n",
            "370000 steps; 23680000 examples so far; 15.79 % ; train ppl: 1.00, valid ppl: 1.65, best_valid_ppl: 1.51\n",
            "380000 steps; 24320000 examples so far; 16.21 % ; train ppl: 1.00, valid ppl: 1.59, best_valid_ppl: 1.51\n",
            "390000 steps; 24960000 examples so far; 16.64 % ; train ppl: 1.00, valid ppl: 1.62, best_valid_ppl: 1.51\n",
            "400000 steps; 25600000 examples so far; 17.07 % ; train ppl: 1.00, valid ppl: 1.65, best_valid_ppl: 1.51\n",
            "410000 steps; 26240000 examples so far; 17.49 % ; train ppl: 1.00, valid ppl: 1.62, best_valid_ppl: 1.51\n",
            "420000 steps; 26880000 examples so far; 17.92 % ; train ppl: 1.00, valid ppl: 1.79, best_valid_ppl: 1.51\n",
            "430000 steps; 27520000 examples so far; 18.35 % ; train ppl: 1.00, valid ppl: 1.76, best_valid_ppl: 1.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params['retrain'] = True\n",
        "if params['retrain']:\n",
        "  best_model = params['path_saved_model']\n",
        "  model = torch.load(best_model)"
      ],
      "metadata": {
        "id": "wKxF1aykIYdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GmR9IFQ1IYXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93192178-f809-43e0-fbb2-e172a854748d"
      },
      "source": [
        "#best_model = 'best_model.pt'\n",
        "#model.load_state_dict(torch.load(best_model))\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device), attMask, device)\n",
        "        for test_input_ids, test_target_ids, attMask in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 1.4770410999112824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run['test/perplexity'].log(test_ppl)\n"
      ],
      "metadata": {
        "id": "dqKk8eGXKgfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()"
      ],
      "metadata": {
        "id": "ZLT8x2WYKicU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a924088-51d5-4e62-f1e4-a6fcc3c4d20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 1 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ['Ouviram do Ipiranga nas margens plácidas um grito']\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(texts=prompt, tokenizer=tokenizer, max_length=max_seq_length).input_ids\n",
        "\n",
        "    att_map = tokenize(texts=prompt, tokenizer=tokenizer, max_length=max_seq_length).attention_mask\n",
        "\n",
        "    print(f'input_ids: {input_ids.shape}')\n",
        "    print(f'att_map: {att_map.shape}')\n",
        "    \n",
        "    #input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor(input_ids).to(device), att_map, device)\n",
        "    print(f'logits: {logits.shape}')\n",
        "    \n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    print(f'logits: {logits.shape}')\n",
        "\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    print(input_ids)\n",
        "    input_ids = input_ids.transpose(1,0)\n",
        "    print(input_ids.shape)\n",
        "    pred_tensor = torch.tensor([predicted_id]).long()\n",
        "    print(pred_tensor.shape)\n",
        "    input_ids = torch.cat((input_ids, pred_tensor.unsqueeze(1)), dim=0)  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    # tokens: print(input_ids)\n",
        "    print(f'pred: {predicted_id}')\n",
        "    print(f'input_ids: {input_ids.shape}')\n",
        "    input_ids = input_ids[1:].reshape(len(input_ids)-1)\n",
        "    print(input_ids)\n",
        "    prompt = [tokenizer.decode(input_ids)] \n",
        "    print(prompt)\n",
        "    "
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019c4f04-4386-4cd7-c460-a8c575dbe36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n",
            "input_ids: torch.Size([1, 9])\n",
            "att_map: torch.Size([1, 9])\n",
            "logits: torch.Size([1, 9, 29794])\n",
            "logits: torch.Size([1, 29794])\n",
            "tensor([[  101,  1311, 12343, 22287,   171, 18795,   364, 12674,   102]])\n",
            "torch.Size([9, 1])\n",
            "torch.Size([1])\n",
            "pred: 171\n",
            "input_ids: torch.Size([10, 1])\n",
            "tensor([ 1311, 12343, 22287,   171, 18795,   364, 12674,   102,   171])\n",
            "['Ouviram do Ipiranga [SEP] do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}