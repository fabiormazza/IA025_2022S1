{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 9 - Exercício - Karen",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex09/karen_rosero/Aula_9_Exerc%C3%ADcio_Karen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Karen Rosero\"\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "outputId": "9999dbab-d33b-4f3c-b88f-9b81ba28ec40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Karen Rosero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da Aula 8, mas iremos agora treinar uma rede neural com **duas camadas** de auto-atenção **causais** para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Iremos também trabalhar com sequencias de tamanho variável.\n",
        "\n",
        "Na camada de auto-atenção, não se esqueça de implementar:\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Conexões residuais\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "\n",
        "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "add2dcb4-d8ff-4e70-e22e-314d9fd61622"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in ./anaconda3/lib/python3.8/site-packages (4.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
            "Requirement already satisfied: pyyaml in ./anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in ./anaconda3/lib/python3.8/site-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in ./anaconda3/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in ./anaconda3/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in ./anaconda3/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in ./anaconda3/lib/python3.8/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in ./anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in ./anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef56f9ed-690b-42fc-efbb-011fd1022252"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  1 19:23:48 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\n",
            "|   0  TITAN V             Off  | 00000000:02:00.0  On |                  N/A |\n",
            "| 33%   48C    P2    40W / 250W |    585MiB / 12064MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1682      G   /usr/lib/xorg/Xorg                181MiB |\n",
            "|    0   N/A  N/A      2081      G   /usr/bin/gnome-shell               52MiB |\n",
            "|    0   N/A  N/A      3264      G   ...mviewer/tv_bin/TeamViewer       25MiB |\n",
            "|    0   N/A  N/A      4935      G   /usr/lib/firefox/firefox          302MiB |\n",
            "|    0   N/A  N/A      5583      G   /usr/lib/firefox/firefox            5MiB |\n",
            "|    0   N/A  N/A      5684      G   /usr/lib/firefox/firefox            5MiB |\n",
            "|    0   N/A  N/A      6481      G   /usr/lib/firefox/firefox            5MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c066ac83-4c6a-4b1b-a237-05d492e1f206"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        # Escreva seu código aqui\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.features = self._token_window(texts)\n",
        "\n",
        "    def _token_window(self, texts):\n",
        "        feat = []\n",
        "        y = []\n",
        "        # bucle para ler cada uma das frases de entrada\n",
        "\n",
        "        # Dataloader inspirado no notebook do Pedro Gengo\n",
        "        for text in texts:\n",
        "          # tokeniza uma frase\n",
        "          tokens_from_text = tokenize(f'[CLS]{text}', self.tokenizer)\n",
        "          tokens_from_text +=  [tokenizer.vocab['[PAD]']] * max(0, 1 + self.max_seq_length - len(tokens_from_text))\n",
        "          for i in range(0, len(tokens_from_text)-1, self.max_seq_length): \n",
        "            if i+self.max_seq_length < len(tokens_from_text):\n",
        "              feat.append(tokens_from_text[i:i+self.max_seq_length+1])\n",
        "            else:\n",
        "              feat.append(tokens_from_text[-self.max_seq_length-1:])\n",
        "        return torch.tensor(feat).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        feat = self.features[idx]\n",
        "        #print(feat)\n",
        "        return feat[:-1], feat[1:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=9)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "#print('input',first_batch_input)\n",
        "#print('target',first_batch_target)\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05459f98-6bfa-48a2-b241-db5c02a03998"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "833d4fd5-f47d-4eb7-a7e6-9dd412583433"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample-1gb.txt’ already there; not retrieving.\r\n",
            "\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = 9\n",
        "\n",
        "train_examples = 500\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "\n",
        "texts = open('sample-1gb.txt').readlines()\n",
        "\n",
        "print(f'Read {len(texts)} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "#print(f'Truncating to {max_lines} lines.')\n",
        "#texts = texts[:max_lines]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e8eba9-071d-47c2-bca9-e0101ed176e5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 250000 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4550853-3cac-4ebe-f881-f6f2644dd362"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 31158155\n",
            "valid examples: 14755\n",
            "test examples: 8302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, max_seq_length: int, dim: int, heads:int, pad_token_id:int):\n",
        "    super().__init__()\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.heads = heads\n",
        "    self.pad_token_id = pad_token_id\n",
        "\n",
        "    # Linear projections, input D output D\n",
        "    self.W_Q = nn.Linear(dim, dim, bias=False)\n",
        "    self.W_K = nn.Linear(dim, dim, bias=False)\n",
        "    self.W_V = nn.Linear(dim, dim, bias=False)\n",
        "    # Baseado na criação da função generate_square_subsequent_mask() \n",
        "    # https://jamesmccaffrey.wordpress.com/2020/12/16/generating-pytorch-transformer-masks/\n",
        "    # crio a matriz com 1s da diagonal para baixo\n",
        "    self.mask = torch.triu(torch.ones(self.max_seq_length, self.max_seq_length)).transpose(0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "  def attention(self, Q, K, V, mask):\n",
        "    s_i = torch.matmul(Q, K.transpose(-2, -1))  \n",
        "    # https://jamesmccaffrey.wordpress.com/2020/12/16/generating-pytorch-transformer-masks/\n",
        "    s_i = s_i.masked_fill(mask.unsqueeze(1) == 0, -float(\"inf\"))\n",
        "    probs = nn.functional.softmax(s_i, dim=-2)  \n",
        "    output = torch.matmul(probs, V).transpose(1, 2) \n",
        "    return output \n",
        "  \n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    batch_size = inputs.shape[0]\n",
        "    q = self.W_Q(inputs)  \n",
        "    q = q.view(batch_size, self.max_seq_length, self.heads, -1)  \n",
        "    q = q.transpose(1, 2)  \n",
        "    k = self.W_K(inputs) \n",
        "    k = k.view(batch_size, self.max_seq_length, self.heads, -1)\n",
        "    k = k.transpose(1, 2)  \n",
        "    v = self.W_V(inputs) \n",
        "    v = v.view(batch_size, self.max_seq_length, self.heads, -1)\n",
        "    v = v.transpose(1, 2)  \n",
        "    \n",
        "    output = self.attention(q, k, v, mask)\n",
        "    output = output.reshape(batch_size, self.max_seq_length, -1)  \n",
        "    \n",
        "    return output  "
      ],
      "metadata": {
        "id": "HyZFGi4OOKa-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id \n",
        "        self.embedding_ = nn.Embedding(vocab_size, dim, padding_idx=pad_token_id)\n",
        "        self.embedding_P = nn.Embedding(vocab_size, dim, padding_idx=pad_token_id)\n",
        "        self.heads = MultiHead(max_seq_length, dim, n_layers, pad_token_id)\n",
        "        self.W_O = torch.nn.Linear(dim, dim, bias = False)\n",
        "        self.dense_layers = torch.nn.Sequential(\n",
        "            OrderedDict([\n",
        "                         ('L1', torch.nn.Linear(dim, dim)),\n",
        "                         ('relu', torch.nn.ReLU()),\n",
        "                         ('L2', torch.nn.Linear(dim, vocab_size, bias=False))\n",
        "            ])\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size = inputs.shape[0]\n",
        "\n",
        "        embds = self.embedding_(inputs) + self.embedding_P(inputs)  \n",
        "        \n",
        "        mask = torch.triu(torch.ones(batch_size, self.max_seq_length, self.max_seq_length)).transpose(1, 2).to(device)\n",
        "        mask = mask.masked_fill(inputs.unsqueeze(1) == self.pad_token_id, 0)\n",
        "        mask = mask.masked_fill(inputs.unsqueeze(2) == self.pad_token_id, 0)\n",
        "\n",
        "        out = self.heads.forward(embds, mask)  \n",
        "        out = self.W_O(out)\n",
        "\n",
        "        outputL = self.dense_layers(out) \n",
        "        return outputL\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210ffd8a-ead0-48e4-8231-21fdb5ad2de4"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 9])\n",
            "sample_output.shape: torch.Size([1, 9, 29794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa0504c-01b1-410c-d0f2-4e4de1ed634d"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5740992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69790e9b-765b-435c-f851-74ee555b1498"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              30105\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b449a572-6c5a-49ff-c8f4-d0918382614a"
      },
      "source": [
        "max_examples = 120_000_000\n",
        "eval_every_steps = 10000\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=64,\n",
        "    n_layers=2,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=128)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 29781.80, valid ppl: 29779.30\n",
            "10000 steps; 1280000 examples so far; train ppl: 874.72, valid ppl: 567.15\n",
            "20000 steps; 2560000 examples so far; train ppl: 426.70, valid ppl: 410.54\n",
            "30000 steps; 3840000 examples so far; train ppl: 343.27, valid ppl: 355.32\n",
            "40000 steps; 5120000 examples so far; train ppl: 306.42, valid ppl: 322.42\n",
            "50000 steps; 6400000 examples so far; train ppl: 283.74, valid ppl: 303.20\n",
            "60000 steps; 7680000 examples so far; train ppl: 266.94, valid ppl: 286.62\n",
            "70000 steps; 8960000 examples so far; train ppl: 254.58, valid ppl: 274.56\n",
            "80000 steps; 10240000 examples so far; train ppl: 244.75, valid ppl: 264.71\n",
            "90000 steps; 11520000 examples so far; train ppl: 236.30, valid ppl: 256.49\n",
            "100000 steps; 12800000 examples so far; train ppl: 229.19, valid ppl: 249.07\n",
            "110000 steps; 14080000 examples so far; train ppl: 223.30, valid ppl: 243.34\n",
            "120000 steps; 15360000 examples so far; train ppl: 218.09, valid ppl: 238.51\n",
            "130000 steps; 16640000 examples so far; train ppl: 213.22, valid ppl: 233.75\n",
            "140000 steps; 17920000 examples so far; train ppl: 208.84, valid ppl: 229.75\n",
            "150000 steps; 19200000 examples so far; train ppl: 205.67, valid ppl: 225.35\n",
            "160000 steps; 20480000 examples so far; train ppl: 202.01, valid ppl: 222.18\n",
            "170000 steps; 21760000 examples so far; train ppl: 198.64, valid ppl: 219.41\n",
            "180000 steps; 23040000 examples so far; train ppl: 195.83, valid ppl: 216.33\n",
            "190000 steps; 24320000 examples so far; train ppl: 193.22, valid ppl: 212.98\n",
            "200000 steps; 25600000 examples so far; train ppl: 190.77, valid ppl: 211.09\n",
            "210000 steps; 26880000 examples so far; train ppl: 188.28, valid ppl: 208.33\n",
            "220000 steps; 28160000 examples so far; train ppl: 186.13, valid ppl: 205.85\n",
            "230000 steps; 29440000 examples so far; train ppl: 184.05, valid ppl: 203.38\n",
            "240000 steps; 30720000 examples so far; train ppl: 181.94, valid ppl: 201.52\n",
            "250000 steps; 32000000 examples so far; train ppl: 179.09, valid ppl: 199.60\n",
            "260000 steps; 33280000 examples so far; train ppl: 177.53, valid ppl: 197.31\n",
            "270000 steps; 34560000 examples so far; train ppl: 176.28, valid ppl: 195.78\n",
            "280000 steps; 35840000 examples so far; train ppl: 174.40, valid ppl: 194.13\n",
            "290000 steps; 37120000 examples so far; train ppl: 172.85, valid ppl: 192.65\n",
            "300000 steps; 38400000 examples so far; train ppl: 171.63, valid ppl: 190.73\n",
            "310000 steps; 39680000 examples so far; train ppl: 169.70, valid ppl: 189.58\n",
            "320000 steps; 40960000 examples so far; train ppl: 168.81, valid ppl: 188.21\n",
            "330000 steps; 42240000 examples so far; train ppl: 167.59, valid ppl: 186.93\n",
            "340000 steps; 43520000 examples so far; train ppl: 166.60, valid ppl: 185.31\n",
            "350000 steps; 44800000 examples so far; train ppl: 165.19, valid ppl: 184.27\n",
            "360000 steps; 46080000 examples so far; train ppl: 163.94, valid ppl: 183.43\n",
            "370000 steps; 47360000 examples so far; train ppl: 163.14, valid ppl: 182.11\n",
            "380000 steps; 48640000 examples so far; train ppl: 161.98, valid ppl: 181.10\n",
            "390000 steps; 49920000 examples so far; train ppl: 161.12, valid ppl: 180.21\n",
            "400000 steps; 51200000 examples so far; train ppl: 159.93, valid ppl: 178.97\n",
            "410000 steps; 52480000 examples so far; train ppl: 159.13, valid ppl: 178.36\n",
            "420000 steps; 53760000 examples so far; train ppl: 158.36, valid ppl: 177.11\n",
            "430000 steps; 55040000 examples so far; train ppl: 157.65, valid ppl: 175.98\n",
            "440000 steps; 56320000 examples so far; train ppl: 156.85, valid ppl: 175.71\n",
            "450000 steps; 57600000 examples so far; train ppl: 156.11, valid ppl: 174.57\n",
            "460000 steps; 58880000 examples so far; train ppl: 155.39, valid ppl: 174.21\n",
            "470000 steps; 60160000 examples so far; train ppl: 154.52, valid ppl: 172.49\n",
            "480000 steps; 61440000 examples so far; train ppl: 153.91, valid ppl: 171.83\n",
            "490000 steps; 62720000 examples so far; train ppl: 153.00, valid ppl: 171.33\n",
            "500000 steps; 64000000 examples so far; train ppl: 151.76, valid ppl: 170.56\n",
            "510000 steps; 65280000 examples so far; train ppl: 151.38, valid ppl: 169.90\n",
            "520000 steps; 66560000 examples so far; train ppl: 150.62, valid ppl: 168.96\n",
            "530000 steps; 67840000 examples so far; train ppl: 150.29, valid ppl: 168.26\n",
            "540000 steps; 69120000 examples so far; train ppl: 149.41, valid ppl: 167.80\n",
            "550000 steps; 70400000 examples so far; train ppl: 149.13, valid ppl: 167.10\n",
            "560000 steps; 71680000 examples so far; train ppl: 148.72, valid ppl: 166.90\n",
            "570000 steps; 72960000 examples so far; train ppl: 148.31, valid ppl: 166.15\n",
            "580000 steps; 74240000 examples so far; train ppl: 147.69, valid ppl: 165.89\n",
            "590000 steps; 75520000 examples so far; train ppl: 147.11, valid ppl: 165.23\n",
            "600000 steps; 76800000 examples so far; train ppl: 146.86, valid ppl: 164.25\n",
            "610000 steps; 78080000 examples so far; train ppl: 146.23, valid ppl: 163.88\n",
            "620000 steps; 79360000 examples so far; train ppl: 145.72, valid ppl: 163.54\n",
            "630000 steps; 80640000 examples so far; train ppl: 145.21, valid ppl: 162.66\n",
            "640000 steps; 81920000 examples so far; train ppl: 144.72, valid ppl: 162.44\n",
            "650000 steps; 83200000 examples so far; train ppl: 144.46, valid ppl: 162.36\n",
            "660000 steps; 84480000 examples so far; train ppl: 144.07, valid ppl: 161.46\n",
            "670000 steps; 85760000 examples so far; train ppl: 143.54, valid ppl: 160.95\n",
            "680000 steps; 87040000 examples so far; train ppl: 143.47, valid ppl: 160.74\n",
            "690000 steps; 88320000 examples so far; train ppl: 142.92, valid ppl: 160.21\n",
            "700000 steps; 89600000 examples so far; train ppl: 142.50, valid ppl: 159.58\n",
            "710000 steps; 90880000 examples so far; train ppl: 142.20, valid ppl: 159.37\n",
            "720000 steps; 92160000 examples so far; train ppl: 141.37, valid ppl: 158.63\n",
            "730000 steps; 93440000 examples so far; train ppl: 141.26, valid ppl: 158.10\n",
            "740000 steps; 94720000 examples so far; train ppl: 140.07, valid ppl: 157.93\n",
            "750000 steps; 96000000 examples so far; train ppl: 139.94, valid ppl: 157.91\n",
            "760000 steps; 97280000 examples so far; train ppl: 139.89, valid ppl: 157.18\n",
            "770000 steps; 98560000 examples so far; train ppl: 139.52, valid ppl: 156.63\n",
            "780000 steps; 99840000 examples so far; train ppl: 139.23, valid ppl: 156.30\n",
            "790000 steps; 101120000 examples so far; train ppl: 138.93, valid ppl: 156.11\n",
            "800000 steps; 102400000 examples so far; train ppl: 138.35, valid ppl: 155.82\n",
            "810000 steps; 103680000 examples so far; train ppl: 138.31, valid ppl: 155.73\n",
            "820000 steps; 104960000 examples so far; train ppl: 138.25, valid ppl: 155.42\n",
            "830000 steps; 106240000 examples so far; train ppl: 137.81, valid ppl: 154.98\n",
            "840000 steps; 107520000 examples so far; train ppl: 137.58, valid ppl: 154.19\n",
            "850000 steps; 108800000 examples so far; train ppl: 137.11, valid ppl: 153.98\n",
            "860000 steps; 110080000 examples so far; train ppl: 137.14, valid ppl: 153.72\n",
            "870000 steps; 111360000 examples so far; train ppl: 136.52, valid ppl: 153.32\n",
            "880000 steps; 112640000 examples so far; train ppl: 136.58, valid ppl: 153.15\n",
            "890000 steps; 113920000 examples so far; train ppl: 136.29, valid ppl: 152.76\n",
            "900000 steps; 115200000 examples so far; train ppl: 136.31, valid ppl: 152.96\n",
            "910000 steps; 116480000 examples so far; train ppl: 135.65, valid ppl: 152.76\n",
            "920000 steps; 117760000 examples so far; train ppl: 135.59, valid ppl: 152.00\n",
            "930000 steps; 119040000 examples so far; train ppl: 135.05, valid ppl: 151.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a7cacd-e914-460c-af7d-49b211bb36fe"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 126.74236672149605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d26326b-93ed-4208-943d-16bd2e68c569"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de comer pizza pois me faz com\n",
            "Eu gosto de comer pizza pois me faz com os\n",
            "Eu gosto de comer pizza pois me faz com os que\n",
            "Eu gosto de comer pizza pois me faz com os que os\n",
            "Eu gosto de comer pizza pois me faz com os que os que\n",
            "Eu gosto de comer pizza pois me faz com os que os que a\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua.\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua..\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua...\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua....\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua.....\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua......\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua.......\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua........\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua.........\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua..........\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua...........\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua............\n",
            "Eu gosto de comer pizza pois me faz com os que os que a sua.............\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}