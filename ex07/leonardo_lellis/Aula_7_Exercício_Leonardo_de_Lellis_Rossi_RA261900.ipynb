{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 7 - Exercício - Leonardo de Lellis Rossi RA261900",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex07/leonardo_lellis/Aula_7_Exerc%C3%ADcio_Leonardo_de_Lellis_Rossi_RA261900.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Leonardo de Lellis Rossi RA261900\"\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56240b74-ca00-406c-d5fc-959c69442621"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Leonardo de Lellis Rossi RA261900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neptune charts: https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-59/charts\n",
        "\n",
        "parameters: https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-59/all?path=parameters%2F"
      ],
      "metadata": {
        "id": "cJZR2ZrFk78G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "checkinpoint = True"
      ],
      "metadata": {
        "id": "NJSkx1gPxORT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3twP0YJC4jmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f80bd81-9f41-4905-815a-baba5d08f226"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install -U neptune-client\n",
        " import neptune.new as neptune"
      ],
      "metadata": {
        "id": "ir0MUg89XzSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0093e32f-a0e2-46ef-81a0-a47e19002d52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.7/dist-packages (0.16.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.7.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.4.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Requirement already satisfied: bravado in /usr/local/lib/python3.7/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.27)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.25.11)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.23.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: botocore<1.27.0,>=1.26.2 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (1.26.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (0.5.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3>=1.16.0->neptune-client) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.2->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.10.8)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (5.17.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (3.17.6)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2022.1)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.3)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.11.1)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.7/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.8.0)\n",
            "Requirement already satisfied: cached-property>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = neptune.init(name= 'Ex. Aula 7', tags=['Aula 7', 'BrWaC', 'MLP', 'bengio2003', 'checkinpoint', 'CrossEntropy', 'Adam', 'perplexity'],\n",
        "    project=\"leolellisr/dl-ia025\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NjY1YmJkZi1hYmM5LTQ3M2QtOGU1ZC1iZTFlNWY4NjE1NDQifQ==\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "0pNdE5fU1Xkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae54132-0723-47ee-ef26-f4298ec89a80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-59\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm, tqdm_notebook\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c0f27b-142b-4189-f255-7ffc8dfaf6f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 18 14:53:05 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda5f817-1226-4af9-d756-fa660ac5d641"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds():\n",
        "  random.seed(123)\n",
        "  np.random.seed(123)\n",
        "  torch.manual_seed(123)\n",
        "  torch.cuda.manual_seed(123)\n",
        "set_seeds()"
      ],
      "metadata": {
        "id": "hZsMk2ZPX1EH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
        "        # Escreva seu código aqui\n",
        "      self.X = []\n",
        "      self.y = []\n",
        "      for text in texts:\n",
        "        tokens_ids = tokenize(text, tokenizer)\n",
        "        if len(tokens_ids) > context_size:\n",
        "          for i in range(len(tokens_ids)-context_size):\n",
        "              self.X.append(tokens_ids[i:i+context_size])      \n",
        "              self.y.append(tokens_ids[i+context_size])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        return torch.tensor(self.X[idx]).long(), torch.tensor(self.y[idx]).long()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste se sua implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125],\n",
        "     [ 1660,  5971,   785],\n",
        "     [ 5971,   785,   125],\n",
        "     [  785,   125,  1847],\n",
        "     [  125,  1847, 13779]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f7f01b-88f7-45e4-f4a9-c9cac66aa33c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ],
      "metadata": {
        "id": "vGlN1WqrXPA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c1eef5-d918-4553-a394-24de5442adf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample_brwac.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "if debug:\n",
        "  print('Truncating for debugging purposes.')\n",
        "  texts = texts[:500]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7179ee3f-8984-48ac-ae97-cc1a3f40c390"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 27675945\n",
            "valid examples: 82070\n",
            "test examples: 166726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size, residual_connection):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        # Aumento de mais uma camada, inspirado na proposta da colega Larissa Santesso\n",
        "        # e uso de conexão residual (bypass), inspirado na proposta do colega Marcus Borela\n",
        "        super().__init__()\n",
        "        self.residual_connection = residual_connection\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear_layer1 = nn.Linear(context_size*embedding_dim, hidden_size)\n",
        "        self.linear_layer2 = nn.Linear(hidden_size, hidden_size*2)\n",
        "        self.linear_layer3 = nn.Linear(hidden_size*2, vocab_size, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        if residual_connection: # residual bypass connection: y = b + Wx + U*tanh(d +Hx)\n",
        "          self.residual = nn.Linear(context_size * embedding_dim, vocab_size, bias=True)\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        out = self.embeddings(inputs)\n",
        "        out = out.view(inputs.shape[0],-1)\n",
        "        if self.residual_connection: embeddings = out\n",
        "        out = self.linear_layer1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear_layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear_layer3(out)\n",
        "        if self.residual_connection:\n",
        "          out += self.residual(embeddings) \n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac654b4-37ba-4127-f39b-1466d2b647e5"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64,\n",
        "    hidden_size=128,\n",
        "    residual_connection=False\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 29794])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e114e2c2-a5f3-4de4-8bb1-78cbd1c54c69"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 9640960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    # Escreva seu código aqui.\n",
        "     \n",
        "    return torch.exp(nn.functional.cross_entropy(logits,target))\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645691a2-60e6-472a-f9e1-0ff4cf3acdc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              30251\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b593a2-47d9-4b82-c8e2-294692419065"
      },
      "source": [
        "\n",
        "params = {\n",
        "    'max_examples': 200_000_000,\n",
        "    'eval_every_steps': 5000,\n",
        "    'lr': 3e-5,\n",
        "    'batch_size': 1024,\n",
        "    'embedding_dim': 128,\n",
        "    'hidden_size': 256,\n",
        "    'optimizer': 'Adam',\n",
        "    'residual_connection': False\n",
        "}\n",
        "run['parameters'] = params\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=params['embedding_dim'],\n",
        "    hidden_size=params['hidden_size'],\n",
        "    residual_connection = params['residual_connection']\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "best_valid_ppl = 10e9\n",
        "while n_examples < params['max_examples']:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        run['train/loss'].log(loss) # Envia loss para o Neptune.\n",
        "        if step % params['eval_every_steps'] == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "            run['train/ppl'].log(train_ppl) # Envia train ppl para o Neptune.\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader]))\n",
        "                run['valid/ppl'].log(valid_ppl) # Envia valid ppl para o Neptune.\n",
        "            \n",
        "                if checkinpoint and valid_ppl < best_valid_ppl:\n",
        "                  torch.save(model.state_dict(), 'best_model.pt')\n",
        "                  print(f\"Best model found in step {step}. valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f} \")\n",
        "                  best_valid_ppl = valid_ppl\n",
        "                  \n",
        "            ex_least = params['max_examples']-n_examples\n",
        "            print(f'{step} steps; {n_examples} examples so far; at least {ex_least} examples to go; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}, best_valid_ppl: {best_valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= params['max_examples']:\n",
        "            break"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model found in step 0. valid ppl: 29894.25, best_valid_ppl: 10000000000.00 \n",
            "0 steps; 0 examples so far; at least 200000000 examples to go; train ppl: 29928.49, valid ppl: 29894.25, best_valid_ppl: 29894.25\n",
            "Best model found in step 5000. valid ppl: 1256.68, best_valid_ppl: 29894.25 \n",
            "5000 steps; 5120000 examples so far; at least 194880000 examples to go; train ppl: 1659.80, valid ppl: 1256.68, best_valid_ppl: 1256.68\n",
            "Best model found in step 10000. valid ppl: 891.30, best_valid_ppl: 1256.68 \n",
            "10000 steps; 10240000 examples so far; at least 189760000 examples to go; train ppl: 1042.63, valid ppl: 891.30, best_valid_ppl: 891.30\n",
            "Best model found in step 15000. valid ppl: 739.36, best_valid_ppl: 891.30 \n",
            "15000 steps; 15360000 examples so far; at least 184640000 examples to go; train ppl: 806.36, valid ppl: 739.36, best_valid_ppl: 739.36\n",
            "Best model found in step 20000. valid ppl: 642.67, best_valid_ppl: 739.36 \n",
            "20000 steps; 20480000 examples so far; at least 179520000 examples to go; train ppl: 689.21, valid ppl: 642.67, best_valid_ppl: 642.67\n",
            "Best model found in step 25000. valid ppl: 570.85, best_valid_ppl: 642.67 \n",
            "25000 steps; 25600000 examples so far; at least 174400000 examples to go; train ppl: 609.74, valid ppl: 570.85, best_valid_ppl: 570.85\n",
            "Best model found in step 30000. valid ppl: 516.38, best_valid_ppl: 570.85 \n",
            "30000 steps; 30720000 examples so far; at least 169280000 examples to go; train ppl: 543.16, valid ppl: 516.38, best_valid_ppl: 516.38\n",
            "Best model found in step 35000. valid ppl: 471.88, best_valid_ppl: 516.38 \n",
            "35000 steps; 35840000 examples so far; at least 164160000 examples to go; train ppl: 491.97, valid ppl: 471.88, best_valid_ppl: 471.88\n",
            "Best model found in step 40000. valid ppl: 433.90, best_valid_ppl: 471.88 \n",
            "40000 steps; 40960000 examples so far; at least 159040000 examples to go; train ppl: 452.40, valid ppl: 433.90, best_valid_ppl: 433.90\n",
            "Best model found in step 45000. valid ppl: 402.63, best_valid_ppl: 433.90 \n",
            "45000 steps; 46080000 examples so far; at least 153920000 examples to go; train ppl: 419.01, valid ppl: 402.63, best_valid_ppl: 402.63\n",
            "Best model found in step 50000. valid ppl: 377.51, best_valid_ppl: 402.63 \n",
            "50000 steps; 51200000 examples so far; at least 148800000 examples to go; train ppl: 390.54, valid ppl: 377.51, best_valid_ppl: 377.51\n",
            "Best model found in step 55000. valid ppl: 355.74, best_valid_ppl: 377.51 \n",
            "55000 steps; 56320000 examples so far; at least 143680000 examples to go; train ppl: 365.38, valid ppl: 355.74, best_valid_ppl: 355.74\n",
            "Best model found in step 60000. valid ppl: 338.80, best_valid_ppl: 355.74 \n",
            "60000 steps; 61440000 examples so far; at least 138560000 examples to go; train ppl: 342.08, valid ppl: 338.80, best_valid_ppl: 338.80\n",
            "Best model found in step 65000. valid ppl: 323.02, best_valid_ppl: 338.80 \n",
            "65000 steps; 66560000 examples so far; at least 133440000 examples to go; train ppl: 326.29, valid ppl: 323.02, best_valid_ppl: 323.02\n",
            "Best model found in step 70000. valid ppl: 309.57, best_valid_ppl: 323.02 \n",
            "70000 steps; 71680000 examples so far; at least 128320000 examples to go; train ppl: 311.92, valid ppl: 309.57, best_valid_ppl: 309.57\n",
            "Best model found in step 75000. valid ppl: 298.28, best_valid_ppl: 309.57 \n",
            "75000 steps; 76800000 examples so far; at least 123200000 examples to go; train ppl: 300.30, valid ppl: 298.28, best_valid_ppl: 298.28\n",
            "Best model found in step 80000. valid ppl: 287.91, best_valid_ppl: 298.28 \n",
            "80000 steps; 81920000 examples so far; at least 118080000 examples to go; train ppl: 289.68, valid ppl: 287.91, best_valid_ppl: 287.91\n",
            "Best model found in step 85000. valid ppl: 279.97, best_valid_ppl: 287.91 \n",
            "85000 steps; 87040000 examples so far; at least 112960000 examples to go; train ppl: 275.54, valid ppl: 279.97, best_valid_ppl: 279.97\n",
            "Best model found in step 90000. valid ppl: 271.29, best_valid_ppl: 279.97 \n",
            "90000 steps; 92160000 examples so far; at least 107840000 examples to go; train ppl: 267.46, valid ppl: 271.29, best_valid_ppl: 271.29\n",
            "Best model found in step 95000. valid ppl: 264.17, best_valid_ppl: 271.29 \n",
            "95000 steps; 97280000 examples so far; at least 102720000 examples to go; train ppl: 261.01, valid ppl: 264.17, best_valid_ppl: 264.17\n",
            "Best model found in step 100000. valid ppl: 257.41, best_valid_ppl: 264.17 \n",
            "100000 steps; 102400000 examples so far; at least 97600000 examples to go; train ppl: 253.89, valid ppl: 257.41, best_valid_ppl: 257.41\n",
            "Best model found in step 105000. valid ppl: 251.47, best_valid_ppl: 257.41 \n",
            "105000 steps; 107520000 examples so far; at least 92480000 examples to go; train ppl: 248.53, valid ppl: 251.47, best_valid_ppl: 251.47\n",
            "Best model found in step 110000. valid ppl: 246.16, best_valid_ppl: 251.47 \n",
            "110000 steps; 112640000 examples so far; at least 87360000 examples to go; train ppl: 241.20, valid ppl: 246.16, best_valid_ppl: 246.16\n",
            "Best model found in step 115000. valid ppl: 241.98, best_valid_ppl: 246.16 \n",
            "115000 steps; 117760000 examples so far; at least 82240000 examples to go; train ppl: 233.26, valid ppl: 241.98, best_valid_ppl: 241.98\n",
            "Best model found in step 120000. valid ppl: 236.52, best_valid_ppl: 241.98 \n",
            "120000 steps; 122880000 examples so far; at least 77120000 examples to go; train ppl: 229.10, valid ppl: 236.52, best_valid_ppl: 236.52\n",
            "Best model found in step 125000. valid ppl: 232.46, best_valid_ppl: 236.52 \n",
            "125000 steps; 128000000 examples so far; at least 72000000 examples to go; train ppl: 225.56, valid ppl: 232.46, best_valid_ppl: 232.46\n",
            "Best model found in step 130000. valid ppl: 228.58, best_valid_ppl: 232.46 \n",
            "130000 steps; 133120000 examples so far; at least 66880000 examples to go; train ppl: 222.63, valid ppl: 228.58, best_valid_ppl: 228.58\n",
            "Best model found in step 135000. valid ppl: 224.57, best_valid_ppl: 228.58 \n",
            "135000 steps; 138240000 examples so far; at least 61760000 examples to go; train ppl: 219.02, valid ppl: 224.57, best_valid_ppl: 224.57\n",
            "Best model found in step 140000. valid ppl: 221.68, best_valid_ppl: 224.57 \n",
            "140000 steps; 143360000 examples so far; at least 56640000 examples to go; train ppl: 210.41, valid ppl: 221.68, best_valid_ppl: 221.68\n",
            "Best model found in step 145000. valid ppl: 218.79, best_valid_ppl: 221.68 \n",
            "145000 steps; 148480000 examples so far; at least 51520000 examples to go; train ppl: 208.47, valid ppl: 218.79, best_valid_ppl: 218.79\n",
            "Best model found in step 150000. valid ppl: 215.77, best_valid_ppl: 218.79 \n",
            "150000 steps; 153600000 examples so far; at least 46400000 examples to go; train ppl: 206.48, valid ppl: 215.77, best_valid_ppl: 215.77\n",
            "Best model found in step 155000. valid ppl: 212.81, best_valid_ppl: 215.77 \n",
            "155000 steps; 158720000 examples so far; at least 41280000 examples to go; train ppl: 203.66, valid ppl: 212.81, best_valid_ppl: 212.81\n",
            "Best model found in step 160000. valid ppl: 209.58, best_valid_ppl: 212.81 \n",
            "160000 steps; 163840000 examples so far; at least 36160000 examples to go; train ppl: 201.34, valid ppl: 209.58, best_valid_ppl: 209.58\n",
            "Best model found in step 165000. valid ppl: 207.92, best_valid_ppl: 209.58 \n",
            "165000 steps; 168960000 examples so far; at least 31040000 examples to go; train ppl: 196.11, valid ppl: 207.92, best_valid_ppl: 207.92\n",
            "Best model found in step 170000. valid ppl: 204.87, best_valid_ppl: 207.92 \n",
            "170000 steps; 174080000 examples so far; at least 25920000 examples to go; train ppl: 193.13, valid ppl: 204.87, best_valid_ppl: 204.87\n",
            "Best model found in step 175000. valid ppl: 203.20, best_valid_ppl: 204.87 \n",
            "175000 steps; 179200000 examples so far; at least 20800000 examples to go; train ppl: 191.17, valid ppl: 203.20, best_valid_ppl: 203.20\n",
            "Best model found in step 180000. valid ppl: 200.54, best_valid_ppl: 203.20 \n",
            "180000 steps; 184320000 examples so far; at least 15680000 examples to go; train ppl: 189.48, valid ppl: 200.54, best_valid_ppl: 200.54\n",
            "Best model found in step 185000. valid ppl: 198.68, best_valid_ppl: 200.54 \n",
            "185000 steps; 189440000 examples so far; at least 10560000 examples to go; train ppl: 188.66, valid ppl: 198.68, best_valid_ppl: 198.68\n",
            "Best model found in step 190000. valid ppl: 196.61, best_valid_ppl: 198.68 \n",
            "190000 steps; 194560000 examples so far; at least 5440000 examples to go; train ppl: 185.74, valid ppl: 196.61, best_valid_ppl: 196.61\n",
            "Best model found in step 195000. valid ppl: 195.32, best_valid_ppl: 196.61 \n",
            "195000 steps; 199680000 examples so far; at least 320000 examples to go; train ppl: 180.12, valid ppl: 195.32, best_valid_ppl: 195.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d0fe77-3814-49ab-8479-c88c8a78c7b5"
      },
      "source": [
        "best_model = 'best_model.pt'\n",
        "model.load_state_dict(torch.load(best_model))\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')\n",
        "run['test/perplexity'].log(test_ppl)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 181.7114435621757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.stop()\n"
      ],
      "metadata": {
        "id": "IM4dj6G_Hhi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eadb95d-bd60-4053-f859-9a11113e5958"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Waiting for the remaining 4 operations to synchronize with Neptune. Do not kill this process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 4 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/leolellisr/dl-ia025/e/DLIA-59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538570a7-555a-4e49-c113-92b60e4bf9c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de comer pizza pois me faz com\n",
            "Eu gosto de comer pizza pois me faz com que\n",
            "Eu gosto de comer pizza pois me faz com que o\n",
            "Eu gosto de comer pizza pois me faz com que o que\n",
            "Eu gosto de comer pizza pois me faz com que o que é\n",
            "Eu gosto de comer pizza pois me faz com que o que é o\n",
            "Eu gosto de comer pizza pois me faz com que o que é o que\n",
            "Eu gosto de comer pizza pois me faz com que o que é o que eu\n",
            "Eu gosto de comer pizza pois me faz com que o que é o que eu não\n",
            "Eu gosto de comer pizza pois me faz com que o que é o que eu não tinha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Ouviram do Ipiranga às margens'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "TEiNZ1311Cl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda046fc-2ed5-4917-f037-61e2f8d233f6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouviram do Ipiranga às margens do\n",
            "Ouviram do Ipiranga às margens do Sul\n",
            "Ouviram do Ipiranga às margens do Sul.\n",
            "Ouviram do Ipiranga às margens do Sul. O\n",
            "Ouviram do Ipiranga às margens do Sul. O que\n",
            "Ouviram do Ipiranga às margens do Sul. O que é\n",
            "Ouviram do Ipiranga às margens do Sul. O que é o\n",
            "Ouviram do Ipiranga às margens do Sul. O que é o caso\n",
            "Ouviram do Ipiranga às margens do Sul. O que é o caso da\n",
            "Ouviram do Ipiranga às margens do Sul. O que é o caso da República\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='A galinha atravessou a rua para chegar'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "Vh6dytMueYjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6224d989-c064-4ee9-8541-59388ca25ee8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A galinha atravessou a rua para chegar a\n",
            "A galinha atravessou a rua para chegar a sua\n",
            "A galinha atravessou a rua para chegar a sua casa\n",
            "A galinha atravessou a rua para chegar a sua casa.\n",
            "A galinha atravessou a rua para chegar a sua casa. O\n",
            "A galinha atravessou a rua para chegar a sua casa. O que\n",
            "A galinha atravessou a rua para chegar a sua casa. O que é\n",
            "A galinha atravessou a rua para chegar a sua casa. O que é o\n",
            "A galinha atravessou a rua para chegar a sua casa. O que é o caso\n",
            "A galinha atravessou a rua para chegar a sua casa. O que é o caso,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Ouça com cuidado, o segredo para a felicidade é'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "wXiQRDQTeJ0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2040235e-fee9-43f1-9d3e-c152e48e208f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ouça com cuidado, o segredo para a felicidade é a\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida.\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é o\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é o que\n",
            "Ouça com cuidado, o segredo para a felicidade é a sua vida. O que é o que é\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt ='Desejo a todas as inimigas vida longa'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "TQe_NdxSepJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3d41c6-70d3-4031-88d8-31ab50821a6d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desejo a todas as inimigas vida longa e\n",
            "Desejo a todas as inimigas vida longa e o\n",
            "Desejo a todas as inimigas vida longa e o que\n",
            "Desejo a todas as inimigas vida longa e o que é\n",
            "Desejo a todas as inimigas vida longa e o que é o\n",
            "Desejo a todas as inimigas vida longa e o que é o caso\n",
            "Desejo a todas as inimigas vida longa e o que é o caso de\n",
            "Desejo a todas as inimigas vida longa e o que é o caso de Deus\n",
            "Desejo a todas as inimigas vida longa e o que é o caso de Deus.\n",
            "Desejo a todas as inimigas vida longa e o que é o caso de Deus. O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mn2UuHvOe_vQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mw1OyXGCe_nj"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}