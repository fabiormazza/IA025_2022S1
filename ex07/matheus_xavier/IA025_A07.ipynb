{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flych3r/IA025_2022S1/blob/main/ex07/matheus_xavier/IA025_A07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4c1fd2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:46:39.014576Z",
          "iopub.status.busy": "2022-05-16T11:46:39.013590Z",
          "iopub.status.idle": "2022-05-16T11:46:39.024737Z",
          "shell.execute_reply": "2022-05-16T11:46:39.024054Z"
        },
        "id": "d4c1fd2c",
        "papermill": {
          "duration": 0.040929,
          "end_time": "2022-05-16T11:46:39.027082",
          "exception": false,
          "start_time": "2022-05-16T11:46:38.986153",
          "status": "completed"
        },
        "tags": [],
        "outputId": "28047bfb-971a-45f1-f23a-5dee370de684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Matheus Xavier Sampaio - 220092\n"
          ]
        }
      ],
      "source": [
        "nome = 'Matheus Xavier Sampaio - 220092'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2e6846",
      "metadata": {
        "id": "1c2e6846",
        "papermill": {
          "duration": 0.024709,
          "end_time": "2022-05-16T11:46:39.078162",
          "exception": false,
          "start_time": "2022-05-16T11:46:39.053453",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23de3e45",
      "metadata": {
        "id": "23de3e45",
        "papermill": {
          "duration": 0.026099,
          "end_time": "2022-05-16T11:46:39.129274",
          "exception": false,
          "start_time": "2022-05-16T11:46:39.103175",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7852b71a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:46:39.181353Z",
          "iopub.status.busy": "2022-05-16T11:46:39.180853Z",
          "iopub.status.idle": "2022-05-16T11:46:49.421000Z",
          "shell.execute_reply": "2022-05-16T11:46:49.419943Z"
        },
        "id": "7852b71a",
        "papermill": {
          "duration": 10.267973,
          "end_time": "2022-05-16T11:46:49.422725",
          "exception": false,
          "start_time": "2022-05-16T11:46:39.154752",
          "status": "completed"
        },
        "tags": [],
        "outputId": "45e4b4ba-9263-4077-ca44-073d66e5ee36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "%pip install -qqq transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41266dc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:46:49.477635Z",
          "iopub.status.busy": "2022-05-16T11:46:49.477412Z",
          "iopub.status.idle": "2022-05-16T11:46:58.994253Z",
          "shell.execute_reply": "2022-05-16T11:46:58.993472Z"
        },
        "papermill": {
          "duration": 9.547058,
          "end_time": "2022-05-16T11:46:58.996186",
          "exception": false,
          "start_time": "2022-05-16T11:46:49.449128",
          "status": "completed"
        },
        "tags": [],
        "id": "e41266dc",
        "outputId": "bcda31e8-7bec-4521-b743-ed398d7b4dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qqq wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb2f573",
      "metadata": {
        "id": "0bb2f573",
        "papermill": {
          "duration": 0.025631,
          "end_time": "2022-05-16T11:46:59.048291",
          "exception": false,
          "start_time": "2022-05-16T11:46:59.022660",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9584ff3f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:46:59.101560Z",
          "iopub.status.busy": "2022-05-16T11:46:59.101300Z",
          "iopub.status.idle": "2022-05-16T11:47:01.035883Z",
          "shell.execute_reply": "2022-05-16T11:47:01.034923Z"
        },
        "id": "9584ff3f",
        "papermill": {
          "duration": 1.964125,
          "end_time": "2022-05-16T11:47:01.038481",
          "exception": false,
          "start_time": "2022-05-16T11:46:59.074356",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131c8ed0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:01.091956Z",
          "iopub.status.busy": "2022-05-16T11:47:01.091693Z",
          "iopub.status.idle": "2022-05-16T11:47:01.791745Z",
          "shell.execute_reply": "2022-05-16T11:47:01.790947Z"
        },
        "id": "131c8ed0",
        "papermill": {
          "duration": 0.728812,
          "end_time": "2022-05-16T11:47:01.793743",
          "exception": false,
          "start_time": "2022-05-16T11:47:01.064931",
          "status": "completed"
        },
        "tags": [],
        "outputId": "b4d1fb00-93af-4bd3-fa37-17c06b9a83aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May 16 11:47:01 2022       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   40C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|  No running processes found                                                 |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc0834b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:01.849141Z",
          "iopub.status.busy": "2022-05-16T11:47:01.848357Z",
          "iopub.status.idle": "2022-05-16T11:47:01.908369Z",
          "shell.execute_reply": "2022-05-16T11:47:01.907685Z"
        },
        "id": "0dc0834b",
        "papermill": {
          "duration": 0.089913,
          "end_time": "2022-05-16T11:47:01.910246",
          "exception": false,
          "start_time": "2022-05-16T11:47:01.820333",
          "status": "completed"
        },
        "tags": [],
        "outputId": "aea253e9-ea7c-40f7-bfc8-3e18b896b4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e39874e",
      "metadata": {
        "id": "7e39874e",
        "papermill": {
          "duration": 0.025893,
          "end_time": "2022-05-16T11:47:01.962730",
          "exception": false,
          "start_time": "2022-05-16T11:47:01.936837",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa87ffef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:02.015940Z",
          "iopub.status.busy": "2022-05-16T11:47:02.015702Z",
          "iopub.status.idle": "2022-05-16T11:47:07.053538Z",
          "shell.execute_reply": "2022-05-16T11:47:07.052821Z"
        },
        "id": "aa87ffef",
        "papermill": {
          "duration": 5.067016,
          "end_time": "2022-05-16T11:47:07.055879",
          "exception": false,
          "start_time": "2022-05-16T11:47:01.988863",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer: AutoTokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer: AutoTokenizer, context_size: int):\n",
        "        # Escreva seu código aqui\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_size = context_size\n",
        "        self.n_grams, self.next_words = self._transform_texts(texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.next_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        return self.n_grams[idx], self.next_words[idx]\n",
        "    \n",
        "    def _transform_texts(self, texts):\n",
        "        tokens = (\n",
        "            tokenize(text, self.tokenizer) for text in tqdm(texts)\n",
        "        )\n",
        "        tokens = [\n",
        "            (tkn[i:i + self.context_size], tkn[i + self.context_size]) \n",
        "                for tkn in tokens\n",
        "            for i in range(len(tkn) - (self.context_size))\n",
        "        ]\n",
        "        n_grams = [tkn[0] for tkn in tokens]\n",
        "        next_words = [tkn[1] for tkn in tokens]\n",
        "        return torch.LongTensor(n_grams), torch.LongTensor(next_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bad1ad",
      "metadata": {
        "id": "f1bad1ad",
        "papermill": {
          "duration": 0.026078,
          "end_time": "2022-05-16T11:47:07.108388",
          "exception": false,
          "start_time": "2022-05-16T11:47:07.082310",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Teste se sua implementação do MyDataset está correta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02446335",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:07.162165Z",
          "iopub.status.busy": "2022-05-16T11:47:07.161548Z",
          "iopub.status.idle": "2022-05-16T11:47:11.050738Z",
          "shell.execute_reply": "2022-05-16T11:47:11.049984Z"
        },
        "id": "02446335",
        "papermill": {
          "duration": 3.917993,
          "end_time": "2022-05-16T11:47:11.052603",
          "exception": false,
          "start_time": "2022-05-16T11:47:07.134610",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "16fa94c5691147c5be19fc9d804333a1",
            "f6e736c7c6b9422e9b88ced3a9534fac",
            "7c44510930344d57845c2455a89fe08e",
            "aecb4d553c734404a7160d1773151c64",
            "4f4037db90a5465abf966c7e8f26d26e",
            "0e8906dc405e47aca625065c78dd701a"
          ]
        },
        "outputId": "4bfccf73-02c7-4b0f-b07c-a8a4a42dfe8f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16fa94c5691147c5be19fc9d804333a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/205k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6e736c7c6b9422e9b88ced3a9534fac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c44510930344d57845c2455a89fe08e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aecb4d553c734404a7160d1773151c64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f4037db90a5465abf966c7e8f26d26e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/647 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e8906dc405e47aca625065c78dd701a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor([\n",
        "    [ 3396, 10303,   125],\n",
        "    [ 1660,  5971,   785],\n",
        "    [ 5971,   785,   125],\n",
        "    [  785,   125,  1847],\n",
        "    [  125,  1847, 13779]\n",
        "])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a18220",
      "metadata": {
        "id": "06a18220",
        "papermill": {
          "duration": 0.029372,
          "end_time": "2022-05-16T11:47:11.111913",
          "exception": false,
          "start_time": "2022-05-16T11:47:11.082541",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8958b7d",
      "metadata": {
        "id": "c8958b7d",
        "papermill": {
          "duration": 0.029273,
          "end_time": "2022-05-16T11:47:11.170492",
          "exception": false,
          "start_time": "2022-05-16T11:47:11.141219",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a6222c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:11.230782Z",
          "iopub.status.busy": "2022-05-16T11:47:11.230543Z",
          "iopub.status.idle": "2022-05-16T11:47:13.316004Z",
          "shell.execute_reply": "2022-05-16T11:47:13.315184Z"
        },
        "id": "32a6222c",
        "papermill": {
          "duration": 2.117893,
          "end_time": "2022-05-16T11:47:13.318127",
          "exception": false,
          "start_time": "2022-05-16T11:47:11.200234",
          "status": "completed"
        },
        "tags": [],
        "outputId": "7b24b7a7-b99c-47b4-a8de-99a6f1c03fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-16 11:47:11--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt\r\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.215.128, 142.251.107.128, 173.194.212.128, ...\r\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.215.128|:443... connected.\r\n",
            "HTTP request sent, awaiting response... 200 OK\r\n",
            "Length: 123983611 (118M) [text/plain]\r\n",
            "Saving to: ‘sample_brwac.txt’\r\n",
            "\r\n",
            "sample_brwac.txt    100%[===================>] 118.24M   108MB/s    in 1.1s    \r\n",
            "\r\n",
            "2022-05-16 11:47:13 (108 MB/s) - ‘sample_brwac.txt’ saved [123983611/123983611]\r\n",
            "\r\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43d0842",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:47:13.385051Z",
          "iopub.status.busy": "2022-05-16T11:47:13.384810Z",
          "iopub.status.idle": "2022-05-16T11:58:27.660211Z",
          "shell.execute_reply": "2022-05-16T11:58:27.659466Z"
        },
        "id": "b43d0842",
        "papermill": {
          "duration": 674.311155,
          "end_time": "2022-05-16T11:58:27.662283",
          "exception": false,
          "start_time": "2022-05-16T11:47:13.351128",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "054b94d76aac4dab982eface38e19572",
            "32eb0f10075f44028ca939a8d6c4dfc5",
            "b72898b0be1a4ec08f47556577b59972"
          ]
        },
        "outputId": "19dcd735-c44b-4185-ffef-bb6d4817e381"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "054b94d76aac4dab982eface38e19572",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32eb0f10075f44028ca939a8d6c4dfc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b72898b0be1a4ec08f47556577b59972",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "# print('Truncating for debugging purposes.')\n",
        "# texts = texts[:500]\n",
        "valid_examples = int(len(texts) * 0.2)\n",
        "test_examples = int(len(texts) * 0.2)\n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c870e0b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:27.732067Z",
          "iopub.status.busy": "2022-05-16T11:58:27.731841Z",
          "iopub.status.idle": "2022-05-16T11:58:27.737115Z",
          "shell.execute_reply": "2022-05-16T11:58:27.736429Z"
        },
        "id": "c870e0b3",
        "papermill": {
          "duration": 0.042396,
          "end_time": "2022-05-16T11:58:27.739045",
          "exception": false,
          "start_time": "2022-05-16T11:58:27.696649",
          "status": "completed"
        },
        "tags": [],
        "outputId": "923dca0e-f157-4856-a2db-1fb0c0240e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training examples: 17196588\n",
            "valid examples: 5119032\n",
            "test examples: 5609121\n"
          ]
        }
      ],
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de linguagem"
      ],
      "metadata": {
        "id": "xmuwtJhmY_DJ"
      },
      "id": "xmuwtJhmY_DJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f25ed85",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:27.809298Z",
          "iopub.status.busy": "2022-05-16T11:58:27.808679Z",
          "iopub.status.idle": "2022-05-16T11:58:27.816734Z",
          "shell.execute_reply": "2022-05-16T11:58:27.816057Z"
        },
        "id": "2f25ed85",
        "papermill": {
          "duration": 0.04491,
          "end_time": "2022-05-16T11:58:27.818387",
          "exception": false,
          "start_time": "2022-05-16T11:58:27.773477",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size: int, context_size: int, embedding_dim: int, hidden_size: int):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.emb = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.hidden = torch.nn.Linear(in_features=context_size * embedding_dim, out_features=hidden_size)\n",
        "        self.output = torch.nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        out = self.emb(inputs)\n",
        "        out = out.flatten(start_dim=1)\n",
        "        out = self.hidden(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.output(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da766ac",
      "metadata": {
        "id": "0da766ac",
        "papermill": {
          "duration": 0.034399,
          "end_time": "2022-05-16T11:58:27.886885",
          "exception": false,
          "start_time": "2022-05-16T11:58:27.852486",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Teste o modelo com um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb903f3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:27.957073Z",
          "iopub.status.busy": "2022-05-16T11:58:27.956621Z",
          "iopub.status.idle": "2022-05-16T11:58:33.510326Z",
          "shell.execute_reply": "2022-05-16T11:58:33.509604Z"
        },
        "id": "ecb903f3",
        "papermill": {
          "duration": 5.591192,
          "end_time": "2022-05-16T11:58:33.512083",
          "exception": false,
          "start_time": "2022-05-16T11:58:27.920891",
          "status": "completed"
        },
        "tags": [],
        "outputId": "8238a5c0-16eb-4b54-9e39-ad7af6fe9611"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 29794])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a40268",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:33.582986Z",
          "iopub.status.busy": "2022-05-16T11:58:33.582361Z",
          "iopub.status.idle": "2022-05-16T11:58:33.587028Z",
          "shell.execute_reply": "2022-05-16T11:58:33.586327Z"
        },
        "id": "c2a40268",
        "papermill": {
          "duration": 0.041395,
          "end_time": "2022-05-16T11:58:33.588877",
          "exception": false,
          "start_time": "2022-05-16T11:58:33.547482",
          "status": "completed"
        },
        "tags": [],
        "outputId": "f2746f97-37f9-4ad0-e06a-98cc09527a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of model parameters: 5824098\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d3000c",
      "metadata": {
        "id": "61d3000c",
        "papermill": {
          "duration": 0.034272,
          "end_time": "2022-05-16T11:58:33.657388",
          "exception": false,
          "start_time": "2022-05-16T11:58:33.623116",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Assert da Perplexidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dead33be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:33.728616Z",
          "iopub.status.busy": "2022-05-16T11:58:33.727908Z",
          "iopub.status.idle": "2022-05-16T11:58:33.758266Z",
          "shell.execute_reply": "2022-05-16T11:58:33.757304Z"
        },
        "id": "dead33be",
        "papermill": {
          "duration": 0.067752,
          "end_time": "2022-05-16T11:58:33.759983",
          "exception": false,
          "start_time": "2022-05-16T11:58:33.692231",
          "status": "completed"
        },
        "tags": [],
        "outputId": "5495e66e-47c2-4df1-9187-4fed60e8dd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my perplexity:              30890\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    # Escreva seu código aqui.\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba380d01",
      "metadata": {
        "id": "ba380d01",
        "papermill": {
          "duration": 0.034411,
          "end_time": "2022-05-16T11:58:33.829858",
          "exception": false,
          "start_time": "2022-05-16T11:58:33.795447",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Laço de Treinamento e Validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090c1ccf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:36.146824Z",
          "iopub.status.busy": "2022-05-16T11:58:36.146250Z",
          "iopub.status.idle": "2022-05-16T11:58:41.421832Z",
          "shell.execute_reply": "2022-05-16T11:58:41.421132Z"
        },
        "id": "090c1ccf",
        "papermill": {
          "duration": 5.314351,
          "end_time": "2022-05-16T11:58:41.423699",
          "exception": false,
          "start_time": "2022-05-16T11:58:36.109348",
          "status": "completed"
        },
        "tags": [],
        "outputId": "d95d9450-d10e-4830-b5ee-6ac56f01f6d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflych3r\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/kaggle/working/wandb/run-20220516_115836-yydlxu03</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/flych3r/bengio-lm/runs/yydlxu03\" target=\"_blank\">treasured-dragon-4</a></strong> to <a href=\"https://wandb.ai/flych3r/bengio-lm\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/flych3r/bengio-lm/runs/yydlxu03?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4da85c0850>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "from copy import deepcopy\n",
        "\n",
        "wandb.init(project=\"bengio-lm\", anonymous=\"allow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9d0a22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T11:58:41.519026Z",
          "iopub.status.busy": "2022-05-16T11:58:41.518747Z",
          "iopub.status.idle": "2022-05-16T16:29:50.361165Z",
          "shell.execute_reply": "2022-05-16T16:29:50.360347Z"
        },
        "id": "5b9d0a22",
        "papermill": {
          "duration": 16268.896757,
          "end_time": "2022-05-16T16:29:50.363437",
          "exception": false,
          "start_time": "2022-05-16T11:58:41.466680",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "e0bb6c4ad8f9482ea49e7e9db89f9464"
          ]
        },
        "outputId": "3859402c-6279-4090-acea-e3b925fa6ee1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0bb6c4ad8f9482ea49e7e9db89f9464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500000000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 steps; 0 examples so far; train ppl: 30588.04, valid ppl: 30488.00\n",
            "1000 steps; 12288000 examples so far; train ppl: 4635.47, valid ppl: 2047.36\n",
            "2000 steps; 24576000 examples so far; train ppl: 1736.74, valid ppl: 1516.08\n",
            "3000 steps; 36864000 examples so far; train ppl: 1342.08, valid ppl: 1222.12\n",
            "4000 steps; 49152000 examples so far; train ppl: 1103.76, valid ppl: 1036.56\n",
            "5000 steps; 61440000 examples so far; train ppl: 947.22, valid ppl: 911.11\n",
            "6000 steps; 73728000 examples so far; train ppl: 838.23, valid ppl: 819.33\n",
            "7000 steps; 86016000 examples so far; train ppl: 755.27, valid ppl: 747.76\n",
            "8000 steps; 98304000 examples so far; train ppl: 689.13, valid ppl: 690.20\n",
            "9000 steps; 110592000 examples so far; train ppl: 635.29, valid ppl: 642.09\n",
            "10000 steps; 122880000 examples so far; train ppl: 590.50, valid ppl: 600.98\n",
            "11000 steps; 135168000 examples so far; train ppl: 551.14, valid ppl: 565.24\n",
            "12000 steps; 147456000 examples so far; train ppl: 517.04, valid ppl: 534.18\n",
            "13000 steps; 159744000 examples so far; train ppl: 486.71, valid ppl: 506.84\n",
            "14000 steps; 172032000 examples so far; train ppl: 461.43, valid ppl: 482.51\n",
            "15000 steps; 184320000 examples so far; train ppl: 436.74, valid ppl: 461.10\n",
            "16000 steps; 196608000 examples so far; train ppl: 417.12, valid ppl: 442.00\n",
            "17000 steps; 208896000 examples so far; train ppl: 398.54, valid ppl: 424.91\n",
            "18000 steps; 221184000 examples so far; train ppl: 382.38, valid ppl: 409.48\n",
            "19000 steps; 233472000 examples so far; train ppl: 366.56, valid ppl: 395.68\n",
            "20000 steps; 245760000 examples so far; train ppl: 353.99, valid ppl: 383.27\n",
            "21000 steps; 258048000 examples so far; train ppl: 342.34, valid ppl: 371.83\n",
            "22000 steps; 270336000 examples so far; train ppl: 330.60, valid ppl: 361.58\n",
            "23000 steps; 282624000 examples so far; train ppl: 320.69, valid ppl: 352.27\n",
            "24000 steps; 294912000 examples so far; train ppl: 312.09, valid ppl: 343.65\n",
            "25000 steps; 307200000 examples so far; train ppl: 303.50, valid ppl: 335.70\n",
            "26000 steps; 319488000 examples so far; train ppl: 295.60, valid ppl: 328.49\n",
            "27000 steps; 331776000 examples so far; train ppl: 288.38, valid ppl: 321.85\n",
            "28000 steps; 344064000 examples so far; train ppl: 282.49, valid ppl: 315.60\n",
            "29000 steps; 356352000 examples so far; train ppl: 275.79, valid ppl: 309.88\n",
            "30000 steps; 368640000 examples so far; train ppl: 270.41, valid ppl: 304.62\n",
            "31000 steps; 380928000 examples so far; train ppl: 265.18, valid ppl: 299.66\n",
            "32000 steps; 393216000 examples so far; train ppl: 260.30, valid ppl: 295.02\n",
            "33000 steps; 405504000 examples so far; train ppl: 255.25, valid ppl: 290.71\n",
            "34000 steps; 417792000 examples so far; train ppl: 251.35, valid ppl: 286.72\n",
            "35000 steps; 430080000 examples so far; train ppl: 247.73, valid ppl: 282.81\n",
            "36000 steps; 442368000 examples so far; train ppl: 243.43, valid ppl: 279.27\n",
            "37000 steps; 454656000 examples so far; train ppl: 239.64, valid ppl: 275.97\n",
            "38000 steps; 466944000 examples so far; train ppl: 236.38, valid ppl: 272.79\n",
            "39000 steps; 479232000 examples so far; train ppl: 233.47, valid ppl: 269.76\n",
            "40000 steps; 491520000 examples so far; train ppl: 229.91, valid ppl: 266.92\n"
          ]
        }
      ],
      "source": [
        "max_examples = 500_000_000\n",
        "eval_every_steps = 10_000\n",
        "lr = 3e-5\n",
        "batch_size = 4096 * 3\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=256,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "\n",
        "best_ppl = torch.inf\n",
        "best_model = deepcopy(model.state_dict())\n",
        "\n",
        "wandb.watch(model, log_freq=100)\n",
        "pbar = tqdm(total=max_examples)\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        model.train()\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "\n",
        "        if step % eval_every_steps == 0:\n",
        "            train_loss = np.average(train_losses)\n",
        "            train_ppl = np.exp(train_loss)\n",
        "    \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader\n",
        "                ]))\n",
        "            \n",
        "            wandb.log({\n",
        "                \"train/loss\": loss,\n",
        "                \"train/perplexity\": train_ppl,\n",
        "                \"eval/perplexity\": valid_ppl\n",
        "            }, step=step)\n",
        "            if valid_ppl < best_ppl:\n",
        "                best_ppl = valid_ppl\n",
        "                best_model = deepcopy(model.state_dict())\n",
        "                torch.save(best_model, 'best_model.pth')\n",
        "                \n",
        "                artifact = wandb.Artifact(\n",
        "                    'model',\n",
        "                    type='model',\n",
        "                    metadata={\n",
        "                        \"step\": step,\n",
        "                        \"train_loss\": train_loss,\n",
        "                        \"train_perplexity\": train_ppl,\n",
        "                        \"valid_perplexity\": valid_ppl\n",
        "                    }\n",
        "                )\n",
        "                artifact.add_file('best_model.pth')\n",
        "                wandb.run.log_artifact(artifact)\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        pbar.update(len(input))\n",
        "        if n_examples >= max_examples:\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ff95e2",
      "metadata": {
        "id": "37ff95e2",
        "papermill": {
          "duration": 0.051063,
          "end_time": "2022-05-16T16:29:50.469899",
          "exception": false,
          "start_time": "2022-05-16T16:29:50.418836",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d9b991",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T16:29:50.574684Z",
          "iopub.status.busy": "2022-05-16T16:29:50.574467Z",
          "iopub.status.idle": "2022-05-16T16:31:04.928558Z",
          "shell.execute_reply": "2022-05-16T16:31:04.927822Z"
        },
        "id": "e3d9b991",
        "papermill": {
          "duration": 74.461707,
          "end_time": "2022-05-16T16:31:04.982814",
          "exception": false,
          "start_time": "2022-05-16T16:29:50.521107",
          "status": "completed"
        },
        "tags": [],
        "outputId": "4ad0d9fe-0399-4aac-ec8c-2f5884880048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test perplexity: 263.5974020288667\n"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6697c196",
      "metadata": {
        "id": "6697c196",
        "papermill": {
          "duration": 0.052455,
          "end_time": "2022-05-16T16:31:05.090586",
          "exception": false,
          "start_time": "2022-05-16T16:31:05.038131",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26587c6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T16:31:05.194963Z",
          "iopub.status.busy": "2022-05-16T16:31:05.194718Z",
          "iopub.status.idle": "2022-05-16T16:31:06.891296Z",
          "shell.execute_reply": "2022-05-16T16:31:06.890458Z"
        },
        "id": "26587c6d",
        "papermill": {
          "duration": 1.75083,
          "end_time": "2022-05-16T16:31:06.893084",
          "exception": false,
          "start_time": "2022-05-16T16:31:05.142254",
          "status": "completed"
        },
        "tags": [],
        "outputId": "14850961-572e-45f7-f108-c80e7694f9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eu gosto de comer pizza pois me faz com\n",
            "Eu gosto de comer pizza pois me faz com a\n",
            "Eu gosto de comer pizza pois me faz com a minha\n",
            "Eu gosto de comer pizza pois me faz com a minha vida\n",
            "Eu gosto de comer pizza pois me faz com a minha vida,\n",
            "Eu gosto de comer pizza pois me faz com a minha vida, mas\n",
            "Eu gosto de comer pizza pois me faz com a minha vida, mas não\n",
            "Eu gosto de comer pizza pois me faz com a minha vida, mas não me\n",
            "Eu gosto de comer pizza pois me faz com a minha vida, mas não me engan\n",
            "Eu gosto de comer pizza pois me faz com a minha vida, mas não me engano\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "model.eval()\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5133c9b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-16T16:31:07.020520Z",
          "iopub.status.busy": "2022-05-16T16:31:07.020260Z",
          "iopub.status.idle": "2022-05-16T16:31:13.380404Z",
          "shell.execute_reply": "2022-05-16T16:31:13.379533Z"
        },
        "papermill": {
          "duration": 6.42491,
          "end_time": "2022-05-16T16:31:13.382303",
          "exception": false,
          "start_time": "2022-05-16T16:31:06.957393",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "0b9b096e7f124dabab0fc1a593fd1aa4"
          ]
        },
        "id": "c5133c9b",
        "outputId": "aba10b51-e8d9-4c24-cc3a-184facaa1246"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b9b096e7f124dabab0fc1a593fd1aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='1840.274 MB of 1840.274 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/perplexity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/perplexity</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/perplexity</td><td>266.91794</td></tr><tr><td>train/loss</td><td>5.47605</td></tr><tr><td>train/perplexity</td><td>229.90546</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">treasured-dragon-4</strong>: <a href=\"https://wandb.ai/flych3r/bengio-lm/runs/yydlxu03\" target=\"_blank\">https://wandb.ai/flych3r/bengio-lm/runs/yydlxu03</a><br/>Synced 5 W&B file(s), 0 media file(s), 41 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220516_115836-yydlxu03/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96f0e6b",
      "metadata": {
        "papermill": {
          "duration": 0.066064,
          "end_time": "2022-05-16T16:31:13.509784",
          "exception": false,
          "start_time": "2022-05-16T16:31:13.443720",
          "status": "completed"
        },
        "tags": [],
        "id": "c96f0e6b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 17085.557092,
      "end_time": "2022-05-16T16:31:16.429814",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-05-16T11:46:30.872722",
      "version": "2.3.4"
    },
    "colab": {
      "name": "IA025 - A07",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}