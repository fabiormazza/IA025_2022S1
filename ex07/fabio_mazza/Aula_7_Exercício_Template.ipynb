{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 7 - Exercício - Template",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiormazza/IA025_2022S1/blob/main/ex07/fabio_mazza/Aula_7_Exerc%C3%ADcio_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Fabio Renato Zocal Mazza'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "5565320c-d639-469e-e3e2-05893a2bb9ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Fabio Renato Zocal Mazza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3twP0YJC4jmJ",
        "outputId": "934fec35-6421-4f47-e440-9946aa38df8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "outputId": "e207b36b-caef-4a7a-8fff-e72f43a9247f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 19 12:39:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    28W /  70W |   2474MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "outputId": "004c251e-0710-4118-9522-b8f599072c98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
        "        self.context_size = context_size\n",
        "        inputs = []\n",
        "        targets = []\n",
        "\n",
        "        for text in texts:\n",
        "          token = tokenize(text, tokenizer)\n",
        "          for i in range(len(token) - context_size):\n",
        "            inputs.append(token[i:i+context_size])\n",
        "            targets.append(token[i+context_size])\n",
        "        \n",
        "        self.inputs = torch.LongTensor(inputs)\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste se sua implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125],\n",
        "     [ 1660,  5971,   785],\n",
        "     [ 5971,   785,   125],\n",
        "     [  785,   125,  1847],\n",
        "     [  125,  1847, 13779]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "outputId": "efb783e3-f031-40f7-c9bd-8595d2e5dc64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ],
      "metadata": {
        "id": "vGlN1WqrXPA6",
        "outputId": "c342a426-774c-4f5a-ace9-a31e4518ae95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample_brwac.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "#print('Truncating for debugging purposes.')\n",
        "#texts = texts[:500]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "outputId": "d9d7f081-f2a2-4e45-e2eb-0fa477e78098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 27675945\n",
            "valid examples: 82070\n",
            "test examples: 166726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        #Embedding matrix\n",
        "        #self.embedding_matrix = torch.rand((vocab_size, embedding_dim), requires_grad=True, device=device, dtype=torch.float)\n",
        "        self.embedding_matrix = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #Model\n",
        "        self.hidden_layer_1 = torch.nn.Linear(context_size * embedding_dim, hidden_size)\n",
        "        self.hidden_layer_2 = torch.nn.Linear(hidden_size, hidden_size * 2)\n",
        "        #self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "        self.out_layer = torch.nn.Linear(hidden_size * 2, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        ## Convert word tokens in input for input layer\n",
        "        #batch_size = inputs.shape[0]\n",
        "        #embedded_inputs = torch.zeros((batch_size, context_size * self.embedding_dim)).to(device)\n",
        "        #embedding = torch.zeros((context_size * self.embedding_dim)).to(device)\n",
        "        \n",
        "        #for batch in range(batch_size):\n",
        "        #  i=0\n",
        "        #  \n",
        "        #  for token in inputs[batch, :]:\n",
        "        #    embedding[i*self.embedding_dim:(i+1)*self.embedding_dim]=self.embedding_matrix[token, :]\n",
        "        #    i+=1\n",
        "        #  embedded_inputs[batch, :] = embedding\n",
        "        #\n",
        "        #embedded_inputs.to(device)\n",
        "\n",
        "        #print(embedding)\n",
        "\n",
        "        ## Calculate outputs\n",
        "        outputs = self.embedding_matrix(inputs).view(inputs.shape[0], -1)\n",
        "        outputs = self.hidden_layer_1(outputs)\n",
        "        #outputs = self.relu(outputs)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.hidden_layer_2(outputs)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.out_layer(outputs)\n",
        "        T = 4.0 #Temperature\n",
        "        outputs = outputs / T\n",
        "       \n",
        "        return outputs"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "outputId": "8a048b89-8726-470b-ea33-84cfc2424020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 29794])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f845c992-faf7-483b-9a40-e2ef0f08bc2f"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 5824098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    criteria = torch.nn.CrossEntropyLoss()\n",
        "    loss = criteria(logits, target)\n",
        "    perplexity = torch.exp(loss)\n",
        "   \n",
        "    return perplexity\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d151bc1d-6719-43ee-f76b-c2c20ec9b0c2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              29659\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79497012-e2d5-4cdd-f417-cfc7d308c709"
      },
      "source": [
        "max_examples = 200_000_000 #10_000_000\n",
        "eval_every_steps = 5000\n",
        "lr = 1e-4\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=512,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=256)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    model.eval()\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 29694.17, valid ppl: 29791.95\n",
            "5000 steps; 1280000 examples so far; train ppl: 1408.75, valid ppl: 886.67\n",
            "10000 steps; 2560000 examples so far; train ppl: 786.47, valid ppl: 671.66\n",
            "15000 steps; 3840000 examples so far; train ppl: 628.22, valid ppl: 555.81\n",
            "20000 steps; 5120000 examples so far; train ppl: 540.58, valid ppl: 481.68\n",
            "25000 steps; 6400000 examples so far; train ppl: 481.06, valid ppl: 431.73\n",
            "30000 steps; 7680000 examples so far; train ppl: 437.27, valid ppl: 391.37\n",
            "35000 steps; 8960000 examples so far; train ppl: 404.01, valid ppl: 364.98\n",
            "40000 steps; 10240000 examples so far; train ppl: 379.85, valid ppl: 343.16\n",
            "45000 steps; 11520000 examples so far; train ppl: 358.83, valid ppl: 324.48\n",
            "50000 steps; 12800000 examples so far; train ppl: 343.57, valid ppl: 311.39\n",
            "55000 steps; 14080000 examples so far; train ppl: 330.59, valid ppl: 298.82\n",
            "60000 steps; 15360000 examples so far; train ppl: 320.15, valid ppl: 288.48\n",
            "65000 steps; 16640000 examples so far; train ppl: 310.66, valid ppl: 281.14\n",
            "70000 steps; 17920000 examples so far; train ppl: 301.11, valid ppl: 272.38\n",
            "75000 steps; 19200000 examples so far; train ppl: 296.10, valid ppl: 264.92\n",
            "80000 steps; 20480000 examples so far; train ppl: 287.40, valid ppl: 260.14\n",
            "85000 steps; 21760000 examples so far; train ppl: 283.13, valid ppl: 253.71\n",
            "90000 steps; 23040000 examples so far; train ppl: 277.82, valid ppl: 248.82\n",
            "95000 steps; 24320000 examples so far; train ppl: 272.58, valid ppl: 245.45\n",
            "100000 steps; 25600000 examples so far; train ppl: 269.53, valid ppl: 241.56\n",
            "105000 steps; 26880000 examples so far; train ppl: 264.62, valid ppl: 238.35\n",
            "110000 steps; 28160000 examples so far; train ppl: 256.60, valid ppl: 234.74\n",
            "115000 steps; 29440000 examples so far; train ppl: 248.58, valid ppl: 232.33\n",
            "120000 steps; 30720000 examples so far; train ppl: 247.30, valid ppl: 229.51\n",
            "125000 steps; 32000000 examples so far; train ppl: 245.43, valid ppl: 225.94\n",
            "130000 steps; 33280000 examples so far; train ppl: 244.92, valid ppl: 224.16\n",
            "135000 steps; 34560000 examples so far; train ppl: 242.17, valid ppl: 222.03\n",
            "140000 steps; 35840000 examples so far; train ppl: 239.93, valid ppl: 219.13\n",
            "145000 steps; 37120000 examples so far; train ppl: 238.39, valid ppl: 217.48\n",
            "150000 steps; 38400000 examples so far; train ppl: 237.09, valid ppl: 216.54\n",
            "155000 steps; 39680000 examples so far; train ppl: 237.07, valid ppl: 212.68\n",
            "160000 steps; 40960000 examples so far; train ppl: 234.95, valid ppl: 211.68\n",
            "165000 steps; 42240000 examples so far; train ppl: 233.00, valid ppl: 210.12\n",
            "170000 steps; 43520000 examples so far; train ppl: 231.34, valid ppl: 208.47\n",
            "175000 steps; 44800000 examples so far; train ppl: 230.08, valid ppl: 206.71\n",
            "180000 steps; 46080000 examples so far; train ppl: 228.29, valid ppl: 204.73\n",
            "185000 steps; 47360000 examples so far; train ppl: 226.43, valid ppl: 203.86\n",
            "190000 steps; 48640000 examples so far; train ppl: 226.20, valid ppl: 202.40\n",
            "195000 steps; 49920000 examples so far; train ppl: 225.78, valid ppl: 200.56\n",
            "200000 steps; 51200000 examples so far; train ppl: 223.16, valid ppl: 198.31\n",
            "205000 steps; 52480000 examples so far; train ppl: 221.19, valid ppl: 197.84\n",
            "210000 steps; 53760000 examples so far; train ppl: 221.28, valid ppl: 196.70\n",
            "215000 steps; 55040000 examples so far; train ppl: 220.34, valid ppl: 194.99\n",
            "220000 steps; 56320000 examples so far; train ppl: 210.23, valid ppl: 195.25\n",
            "225000 steps; 57600000 examples so far; train ppl: 207.93, valid ppl: 194.45\n",
            "230000 steps; 58880000 examples so far; train ppl: 209.46, valid ppl: 194.10\n",
            "235000 steps; 60160000 examples so far; train ppl: 209.41, valid ppl: 192.31\n",
            "240000 steps; 61440000 examples so far; train ppl: 209.31, valid ppl: 191.86\n",
            "245000 steps; 62720000 examples so far; train ppl: 207.89, valid ppl: 191.62\n",
            "250000 steps; 64000000 examples so far; train ppl: 208.58, valid ppl: 189.56\n",
            "255000 steps; 65280000 examples so far; train ppl: 207.74, valid ppl: 188.62\n",
            "260000 steps; 66560000 examples so far; train ppl: 208.26, valid ppl: 188.48\n",
            "265000 steps; 67840000 examples so far; train ppl: 207.73, valid ppl: 186.35\n",
            "270000 steps; 69120000 examples so far; train ppl: 206.85, valid ppl: 185.86\n",
            "275000 steps; 70400000 examples so far; train ppl: 206.68, valid ppl: 185.70\n",
            "280000 steps; 71680000 examples so far; train ppl: 205.80, valid ppl: 184.94\n",
            "285000 steps; 72960000 examples so far; train ppl: 204.76, valid ppl: 183.79\n",
            "290000 steps; 74240000 examples so far; train ppl: 205.67, valid ppl: 184.04\n",
            "295000 steps; 75520000 examples so far; train ppl: 204.41, valid ppl: 182.85\n",
            "300000 steps; 76800000 examples so far; train ppl: 202.48, valid ppl: 182.59\n",
            "305000 steps; 78080000 examples so far; train ppl: 204.27, valid ppl: 182.69\n",
            "310000 steps; 79360000 examples so far; train ppl: 202.56, valid ppl: 182.38\n",
            "315000 steps; 80640000 examples so far; train ppl: 202.20, valid ppl: 181.18\n",
            "320000 steps; 81920000 examples so far; train ppl: 202.80, valid ppl: 179.38\n",
            "325000 steps; 83200000 examples so far; train ppl: 199.55, valid ppl: 180.19\n",
            "330000 steps; 84480000 examples so far; train ppl: 190.10, valid ppl: 179.72\n",
            "335000 steps; 85760000 examples so far; train ppl: 191.58, valid ppl: 179.66\n",
            "340000 steps; 87040000 examples so far; train ppl: 191.73, valid ppl: 179.24\n",
            "345000 steps; 88320000 examples so far; train ppl: 192.82, valid ppl: 178.12\n",
            "350000 steps; 89600000 examples so far; train ppl: 193.72, valid ppl: 177.73\n",
            "355000 steps; 90880000 examples so far; train ppl: 194.12, valid ppl: 176.47\n",
            "360000 steps; 92160000 examples so far; train ppl: 193.17, valid ppl: 176.72\n",
            "365000 steps; 93440000 examples so far; train ppl: 193.94, valid ppl: 175.30\n",
            "370000 steps; 94720000 examples so far; train ppl: 193.46, valid ppl: 175.49\n",
            "375000 steps; 96000000 examples so far; train ppl: 193.34, valid ppl: 175.61\n",
            "380000 steps; 97280000 examples so far; train ppl: 192.51, valid ppl: 173.82\n",
            "385000 steps; 98560000 examples so far; train ppl: 192.34, valid ppl: 173.70\n",
            "390000 steps; 99840000 examples so far; train ppl: 192.64, valid ppl: 172.87\n",
            "395000 steps; 101120000 examples so far; train ppl: 192.29, valid ppl: 173.12\n",
            "400000 steps; 102400000 examples so far; train ppl: 192.93, valid ppl: 172.23\n",
            "405000 steps; 103680000 examples so far; train ppl: 192.17, valid ppl: 171.73\n",
            "410000 steps; 104960000 examples so far; train ppl: 191.40, valid ppl: 170.89\n",
            "415000 steps; 106240000 examples so far; train ppl: 192.30, valid ppl: 170.57\n",
            "420000 steps; 107520000 examples so far; train ppl: 191.26, valid ppl: 169.48\n",
            "425000 steps; 108800000 examples so far; train ppl: 191.42, valid ppl: 169.57\n",
            "430000 steps; 110080000 examples so far; train ppl: 190.00, valid ppl: 169.50\n",
            "435000 steps; 111360000 examples so far; train ppl: 183.45, valid ppl: 169.61\n",
            "440000 steps; 112640000 examples so far; train ppl: 180.60, valid ppl: 168.25\n",
            "445000 steps; 113920000 examples so far; train ppl: 182.12, valid ppl: 169.24\n",
            "450000 steps; 115200000 examples so far; train ppl: 181.78, valid ppl: 169.16\n",
            "455000 steps; 116480000 examples so far; train ppl: 182.98, valid ppl: 169.15\n",
            "460000 steps; 117760000 examples so far; train ppl: 183.58, valid ppl: 168.63\n",
            "465000 steps; 119040000 examples so far; train ppl: 184.07, valid ppl: 167.70\n",
            "470000 steps; 120320000 examples so far; train ppl: 183.65, valid ppl: 167.84\n",
            "475000 steps; 121600000 examples so far; train ppl: 184.50, valid ppl: 167.35\n",
            "480000 steps; 122880000 examples so far; train ppl: 184.66, valid ppl: 166.48\n",
            "485000 steps; 124160000 examples so far; train ppl: 184.64, valid ppl: 166.19\n",
            "490000 steps; 125440000 examples so far; train ppl: 184.78, valid ppl: 166.36\n",
            "495000 steps; 126720000 examples so far; train ppl: 184.84, valid ppl: 165.40\n",
            "500000 steps; 128000000 examples so far; train ppl: 183.90, valid ppl: 165.24\n",
            "505000 steps; 129280000 examples so far; train ppl: 183.94, valid ppl: 165.30\n",
            "510000 steps; 130560000 examples so far; train ppl: 184.45, valid ppl: 164.60\n",
            "515000 steps; 131840000 examples so far; train ppl: 183.68, valid ppl: 163.99\n",
            "520000 steps; 133120000 examples so far; train ppl: 183.79, valid ppl: 164.27\n",
            "525000 steps; 134400000 examples so far; train ppl: 184.96, valid ppl: 163.68\n",
            "530000 steps; 135680000 examples so far; train ppl: 183.60, valid ppl: 164.10\n",
            "535000 steps; 136960000 examples so far; train ppl: 183.13, valid ppl: 163.74\n",
            "540000 steps; 138240000 examples so far; train ppl: 183.90, valid ppl: 162.42\n",
            "545000 steps; 139520000 examples so far; train ppl: 173.19, valid ppl: 163.65\n",
            "550000 steps; 140800000 examples so far; train ppl: 174.49, valid ppl: 163.05\n",
            "555000 steps; 142080000 examples so far; train ppl: 175.36, valid ppl: 162.86\n",
            "560000 steps; 143360000 examples so far; train ppl: 174.75, valid ppl: 162.70\n",
            "565000 steps; 144640000 examples so far; train ppl: 176.98, valid ppl: 162.57\n",
            "570000 steps; 145920000 examples so far; train ppl: 176.13, valid ppl: 162.29\n",
            "575000 steps; 147200000 examples so far; train ppl: 177.15, valid ppl: 162.06\n",
            "580000 steps; 148480000 examples so far; train ppl: 178.48, valid ppl: 161.58\n",
            "585000 steps; 149760000 examples so far; train ppl: 177.78, valid ppl: 160.48\n",
            "590000 steps; 151040000 examples so far; train ppl: 177.69, valid ppl: 160.58\n",
            "595000 steps; 152320000 examples so far; train ppl: 177.90, valid ppl: 161.11\n",
            "600000 steps; 153600000 examples so far; train ppl: 178.58, valid ppl: 160.00\n",
            "605000 steps; 154880000 examples so far; train ppl: 178.22, valid ppl: 160.70\n",
            "610000 steps; 156160000 examples so far; train ppl: 178.72, valid ppl: 160.64\n",
            "615000 steps; 157440000 examples so far; train ppl: 179.00, valid ppl: 159.56\n",
            "620000 steps; 158720000 examples so far; train ppl: 178.22, valid ppl: 159.34\n",
            "625000 steps; 160000000 examples so far; train ppl: 178.47, valid ppl: 158.82\n",
            "630000 steps; 161280000 examples so far; train ppl: 177.79, valid ppl: 159.33\n",
            "635000 steps; 162560000 examples so far; train ppl: 178.35, valid ppl: 158.76\n",
            "640000 steps; 163840000 examples so far; train ppl: 179.34, valid ppl: 158.16\n",
            "645000 steps; 165120000 examples so far; train ppl: 177.82, valid ppl: 158.68\n",
            "650000 steps; 166400000 examples so far; train ppl: 174.81, valid ppl: 159.05\n",
            "655000 steps; 167680000 examples so far; train ppl: 167.34, valid ppl: 158.75\n",
            "660000 steps; 168960000 examples so far; train ppl: 169.39, valid ppl: 158.39\n",
            "665000 steps; 170240000 examples so far; train ppl: 170.15, valid ppl: 158.48\n",
            "670000 steps; 171520000 examples so far; train ppl: 171.60, valid ppl: 158.48\n",
            "675000 steps; 172800000 examples so far; train ppl: 171.85, valid ppl: 157.94\n",
            "680000 steps; 174080000 examples so far; train ppl: 172.09, valid ppl: 157.94\n",
            "685000 steps; 175360000 examples so far; train ppl: 173.86, valid ppl: 157.52\n",
            "690000 steps; 176640000 examples so far; train ppl: 173.13, valid ppl: 157.80\n",
            "695000 steps; 177920000 examples so far; train ppl: 172.70, valid ppl: 157.43\n",
            "700000 steps; 179200000 examples so far; train ppl: 173.08, valid ppl: 157.59\n",
            "705000 steps; 180480000 examples so far; train ppl: 172.66, valid ppl: 156.62\n",
            "710000 steps; 181760000 examples so far; train ppl: 173.33, valid ppl: 155.96\n",
            "715000 steps; 183040000 examples so far; train ppl: 173.59, valid ppl: 156.84\n",
            "720000 steps; 184320000 examples so far; train ppl: 174.16, valid ppl: 156.28\n",
            "725000 steps; 185600000 examples so far; train ppl: 172.77, valid ppl: 156.23\n",
            "730000 steps; 186880000 examples so far; train ppl: 174.59, valid ppl: 155.31\n",
            "735000 steps; 188160000 examples so far; train ppl: 173.87, valid ppl: 155.05\n",
            "740000 steps; 189440000 examples so far; train ppl: 173.99, valid ppl: 154.70\n",
            "745000 steps; 190720000 examples so far; train ppl: 174.25, valid ppl: 154.54\n",
            "750000 steps; 192000000 examples so far; train ppl: 173.72, valid ppl: 154.38\n",
            "755000 steps; 193280000 examples so far; train ppl: 174.31, valid ppl: 154.22\n",
            "760000 steps; 194560000 examples so far; train ppl: 166.08, valid ppl: 154.68\n",
            "765000 steps; 195840000 examples so far; train ppl: 164.46, valid ppl: 154.14\n",
            "770000 steps; 197120000 examples so far; train ppl: 166.39, valid ppl: 154.41\n",
            "775000 steps; 198400000 examples so far; train ppl: 166.08, valid ppl: 154.40\n",
            "780000 steps; 199680000 examples so far; train ppl: 166.62, valid ppl: 154.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "outputId": "ae42d644-59c5-4cef-ac27-adc4cf2cb993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0020ba4a755a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     test_ppl = np.exp(np.average([\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Um cão anda pelo lado direito da rua\"  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}