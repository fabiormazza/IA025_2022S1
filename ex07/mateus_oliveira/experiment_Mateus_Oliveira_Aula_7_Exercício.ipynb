{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "070b36ae-7e2a-4924-fc8a-365e9dc75aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é mateus oliveira\n"
          ]
        }
      ],
      "source": [
        "nome = 'mateus oliveira'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "eb443181-ddb9-454d-8976-da4c854b9c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9f3PfifAwpU",
        "outputId": "889fce0b-2665-4436-8ef4-36ca9dee2533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 18 18:11:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    33W / 250W |   2771MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTCe2i7AtoV",
        "outputId": "ab03aff0-5d7b-4401-d252-3e4867e68b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "torch.cuda.manual_seed(123)"
      ],
      "metadata": {
        "id": "ABQpeV2aOD0M"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def tokenize_timbau(text: str):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.context_size = context_size\n",
        "        self.vocab = list(dict.fromkeys(\" \".join(self.texts).split(' ')))# organize code\n",
        "        self.count = 0\n",
        "        self.tam = 0\n",
        "        \n",
        "        self.x = []\n",
        "        self.y = []\n",
        "\n",
        "        for idx, sentence in enumerate(self.texts):\n",
        "            sentence_tokens = tokenize_timbau(sentence)\n",
        "        \n",
        "            for i, _ in enumerate(sentence_tokens):\n",
        "                if i + context_size >= len(sentence_tokens):\n",
        "                    break\n",
        "\n",
        "                self.x.append(sentence_tokens[i:i+context_size])\n",
        "                self.y.append(sentence_tokens[i+context_size])\n",
        "        \n",
        "        self.x, self.y = torch.LongTensor(self.x), torch.LongTensor(self.y)\n",
        "        \n",
        "        self.tam = self.x.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tam\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOqLLKnNA290",
        "outputId": "f695bb74-591d-4f44-a90c-beb9d29b6808"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13779, 15616]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "tokenize_timbau('pizza')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wew-gFbWeBTq"
      },
      "source": [
        "## Teste se sua implementação do MyDataset está correta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r7jBFFUeApe",
        "outputId": "b09957a8-58c8-4e8c-f537-faa694ca29e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n",
            "tensor([[ 3396, 10303,   125],\n",
            "        [ 1660,  5971,   785],\n",
            "        [ 5971,   785,   125],\n",
            "        [  785,   125,  1847],\n",
            "        [  125,  1847, 13779]])\n",
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "\n",
        "print(first_batch_input)\n",
        "\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125],\n",
        "     [ 1660,  5971,   785],\n",
        "     [ 5971,   785,   125],\n",
        "     [  785,   125,  1847],\n",
        "     [  125,  1847, 13779]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxEF38jtGK9z",
        "outputId": "e2b6db8a-3410-4c79-b24d-48910ea48502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3396, 10303,   125],\n",
              "        [ 1660,  5971,   785],\n",
              "        [ 5971,   785,   125],\n",
              "        [  785,   125,  1847],\n",
              "        [  125,  1847, 13779]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "first_batch_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR2nXUPjz3GX",
        "outputId": "76f4f8f2-2b85-4498-ce83-19843827bbdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13239,   125,  1847, 13779, 15616])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "first_batch_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlN1WqrXPA6",
        "outputId": "2279ab08-e2ac-4809-c126-b829fa386351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘sample_brwac.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "gxa_4gmiA-wE"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "valid_examples = 100\n",
        "test_examples = 100\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "# print('Truncating for debugging purposes.')\n",
        "# texts = texts[:500]  \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCSGJ5m7py4c",
        "outputId": "168cf49c-c2b8-4194-e8bd-48d7dbf31524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 27675945\n",
            "valid examples: 82070\n",
            "test examples: 166726\n"
          ]
        }
      ],
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_size)\n",
        "        \n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size, bias = False)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        \n",
        "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
        "        \n",
        "        out = self.relu(self.linear1(embeds))\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.linear2(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm6_PTH2i98e"
      },
      "source": [
        "## Teste o modelo com um exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwnxfZlrZoT_",
        "outputId": "b567248f-2b7a-4f3b-9853-14f45ca5b210"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 29794])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=256,\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSOYMvglOpxt",
        "outputId": "ccc9e2d1-ec15-4854-bdd7-2c9d5ca53345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "s78suFyXOhDH",
        "outputId": "7107e7a9-8007-436f-95af-63229be6126c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f42564746d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"356pt\" height=\"527pt\"\n viewBox=\"0.00 0.00 356.00 527.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 523)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-523 352,-523 352,4 -4,4\"/>\n<!-- 139922885606000 -->\n<g id=\"node1\" class=\"node\">\n<title>139922885606000</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"273,-31 190,-31 190,0 273,0 273,-31\"/>\n<text text-anchor=\"middle\" x=\"231.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 29794)</text>\n</g>\n<!-- 139922892035088 -->\n<g id=\"node2\" class=\"node\">\n<title>139922892035088</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"273,-86 190,-86 190,-67 273,-67 273,-86\"/>\n<text text-anchor=\"middle\" x=\"231.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 139922892035088&#45;&gt;139922885606000 -->\n<g id=\"edge16\" class=\"edge\">\n<title>139922892035088&#45;&gt;139922885606000</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M231.5,-66.9688C231.5,-60.1289 231.5,-50.5621 231.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"235.0001,-41.3678 231.5,-31.3678 228.0001,-41.3678 235.0001,-41.3678\"/>\n</g>\n<!-- 139922892081680 -->\n<g id=\"node3\" class=\"node\">\n<title>139922892081680</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"241,-141 92,-141 92,-122 241,-122 241,-141\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">NativeDropoutBackward0</text>\n</g>\n<!-- 139922892081680&#45;&gt;139922892035088 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139922892081680&#45;&gt;139922892035088</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M177.8221,-121.9197C187.2132,-113.9735 200.7827,-102.4916 211.9285,-93.0605\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.4969,-95.4721 219.8699,-86.3408 209.9752,-90.1284 214.4969,-95.4721\"/>\n</g>\n<!-- 139922892027280 -->\n<g id=\"node4\" class=\"node\">\n<title>139922892027280</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-196 119,-196 119,-177 214,-177 214,-196\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ReluBackward0</text>\n</g>\n<!-- 139922892027280&#45;&gt;139922892081680 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139922892027280&#45;&gt;139922892081680</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M166.5,-176.9197C166.5,-169.9083 166.5,-160.1442 166.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.0001,-151.3408 166.5,-141.3408 163.0001,-151.3409 170.0001,-151.3408\"/>\n</g>\n<!-- 139922892025936 -->\n<g id=\"node5\" class=\"node\">\n<title>139922892025936</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"217,-257 116,-257 116,-238 217,-238 217,-257\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-245\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddmmBackward0</text>\n</g>\n<!-- 139922892025936&#45;&gt;139922892027280 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139922892025936&#45;&gt;139922892027280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M166.5,-237.9688C166.5,-229.5131 166.5,-216.8901 166.5,-206.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.0001,-206.1656 166.5,-196.1656 163.0001,-206.1657 170.0001,-206.1656\"/>\n</g>\n<!-- 139924676388624 -->\n<g id=\"node6\" class=\"node\">\n<title>139924676388624</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-318 0,-318 0,-299 101,-299 101,-318\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139924676388624&#45;&gt;139922892025936 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139924676388624&#45;&gt;139922892025936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M68.625,-298.9688C87.4973,-289.0445 117.2864,-273.3795 139.0475,-261.9362\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.8976,-264.9178 148.1195,-257.1656 137.6396,-258.7222 140.8976,-264.9178\"/>\n</g>\n<!-- 139922885817648 -->\n<g id=\"node7\" class=\"node\">\n<title>139922885817648</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-385 23.5,-385 23.5,-354 77.5,-354 77.5,-385\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-361\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (256)</text>\n</g>\n<!-- 139922885817648&#45;&gt;139924676388624 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139922885817648&#45;&gt;139924676388624</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-353.791C50.5,-346.0249 50.5,-336.5706 50.5,-328.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-328.0647 50.5,-318.0648 47.0001,-328.0648 54.0001,-328.0647\"/>\n</g>\n<!-- 139924676388048 -->\n<g id=\"node8\" class=\"node\">\n<title>139924676388048</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-318 119,-318 119,-299 214,-299 214,-318\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ViewBackward0</text>\n</g>\n<!-- 139924676388048&#45;&gt;139922892025936 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139924676388048&#45;&gt;139922892025936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M166.5,-298.9688C166.5,-290.5131 166.5,-277.8901 166.5,-267.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.0001,-267.1656 166.5,-257.1656 163.0001,-267.1657 170.0001,-267.1656\"/>\n</g>\n<!-- 139924676389392 -->\n<g id=\"node9\" class=\"node\">\n<title>139924676389392</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"225,-379 100,-379 100,-360 225,-360 225,-379\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-367\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">EmbeddingBackward0</text>\n</g>\n<!-- 139924676389392&#45;&gt;139924676388048 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139924676389392&#45;&gt;139924676388048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.125,-359.9688C163.6795,-351.5131 164.5072,-338.8901 165.2042,-328.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.7043,-328.3732 165.8662,-318.1656 161.7193,-327.9152 168.7043,-328.3732\"/>\n</g>\n<!-- 139924676387728 -->\n<g id=\"node10\" class=\"node\">\n<title>139924676387728</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"213,-446 112,-446 112,-427 213,-427 213,-446\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-434\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139924676387728&#45;&gt;139924676389392 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139924676387728&#45;&gt;139924676389392</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.5,-426.9005C162.5,-417.149 162.5,-401.7597 162.5,-389.3695\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.0001,-389.0816 162.5,-379.0817 159.0001,-389.0817 166.0001,-389.0816\"/>\n</g>\n<!-- 139922885603504 -->\n<g id=\"node11\" class=\"node\">\n<title>139922885603504</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"210,-519 115,-519 115,-488 210,-488 210,-519\"/>\n<text text-anchor=\"middle\" x=\"162.5\" y=\"-495\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (29794, 128)</text>\n</g>\n<!-- 139922885603504&#45;&gt;139924676387728 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139922885603504&#45;&gt;139924676387728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.5,-487.9604C162.5,-478.6356 162.5,-466.6748 162.5,-456.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.0001,-456.35 162.5,-446.3501 159.0001,-456.3501 166.0001,-456.35\"/>\n</g>\n<!-- 139924676389712 -->\n<g id=\"node12\" class=\"node\">\n<title>139924676389712</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"320,-318 243,-318 243,-299 320,-299 320,-318\"/>\n<text text-anchor=\"middle\" x=\"281.5\" y=\"-306\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139924676389712&#45;&gt;139922892025936 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139924676389712&#45;&gt;139922892025936</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M263.5313,-298.9688C244.8217,-289.0445 215.2893,-273.3795 193.7158,-261.9362\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"195.1963,-258.7597 184.7221,-257.1656 191.9161,-264.9436 195.1963,-258.7597\"/>\n</g>\n<!-- 139924676387792 -->\n<g id=\"node13\" class=\"node\">\n<title>139924676387792</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"344,-379 243,-379 243,-360 344,-360 344,-379\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-367\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139924676387792&#45;&gt;139924676389712 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139924676387792&#45;&gt;139924676389712</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M291.625,-359.9688C289.9616,-351.5131 287.4784,-338.8901 285.3875,-328.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"288.7659,-327.302 283.4014,-318.1656 281.8976,-328.6532 288.7659,-327.302\"/>\n</g>\n<!-- 139922885818320 -->\n<g id=\"node14\" class=\"node\">\n<title>139922885818320</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"338,-452 249,-452 249,-421 338,-421 338,-452\"/>\n<text text-anchor=\"middle\" x=\"293.5\" y=\"-428\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (256, 1152)</text>\n</g>\n<!-- 139922885818320&#45;&gt;139924676387792 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139922885818320&#45;&gt;139924676387792</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M293.5,-420.9604C293.5,-411.6356 293.5,-399.6748 293.5,-389.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.0001,-389.35 293.5,-379.3501 290.0001,-389.3501 297.0001,-389.35\"/>\n</g>\n<!-- 139922892080272 -->\n<g id=\"node15\" class=\"node\">\n<title>139922892080272</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"336,-141 259,-141 259,-122 336,-122 336,-141\"/>\n<text text-anchor=\"middle\" x=\"297.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">TBackward0</text>\n</g>\n<!-- 139922892080272&#45;&gt;139922892035088 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139922892080272&#45;&gt;139922892035088</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M286.0037,-121.9197C276.3747,-113.8956 262.4193,-102.2661 251.0405,-92.7837\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"253.2319,-90.0539 243.309,-86.3408 248.7506,-95.4315 253.2319,-90.0539\"/>\n</g>\n<!-- 139922892028112 -->\n<g id=\"node16\" class=\"node\">\n<title>139922892028112</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"348,-196 247,-196 247,-177 348,-177 348,-196\"/>\n<text text-anchor=\"middle\" x=\"297.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139922892028112&#45;&gt;139922892080272 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139922892028112&#45;&gt;139922892080272</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M297.5,-176.9197C297.5,-169.9083 297.5,-160.1442 297.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"301.0001,-151.3408 297.5,-141.3408 294.0001,-151.3409 301.0001,-151.3408\"/>\n</g>\n<!-- 139922885607056 -->\n<g id=\"node17\" class=\"node\">\n<title>139922885607056</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"345,-263 250,-263 250,-232 345,-232 345,-263\"/>\n<text text-anchor=\"middle\" x=\"297.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (29794, 256)</text>\n</g>\n<!-- 139922885607056&#45;&gt;139922892028112 -->\n<g id=\"edge15\" class=\"edge\">\n<title>139922885607056&#45;&gt;139922892028112</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M297.5,-231.791C297.5,-224.0249 297.5,-214.5706 297.5,-206.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"301.0001,-206.0647 297.5,-196.0648 294.0001,-206.0648 301.0001,-206.0647\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "import torchviz\n",
        "\n",
        "y = model(sample_train_gpu)\n",
        "\n",
        "out = torchviz.make_dot(y)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Vh6B-VkA01",
        "outputId": "c1ba902f-bdaf-4410-9880-3ded71908e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 11736064\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nhbUVsYnVAp"
      },
      "source": [
        "## Assert da Perplexidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbMP8VAUncfX",
        "outputId": "9d668d28-4eac-45ae-bb26-67951a287d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              31439\n",
            "correct initial perplexity: 29794\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    loss = F.cross_entropy(logits, target)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Avfd54La94D8"
      },
      "outputs": [],
      "source": [
        "class SaveBestModel:\n",
        "\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"Best validation loss: {self.best_valid_loss}\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, 'outputs/best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "AQJ1vqZ3EQmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97e03f0-8a41-43d7-9e93-ba18df95ea34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘outputs’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiJtrsqPnE_l"
      },
      "source": [
        "## Laço de Treinamento e Validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIMSaY-UUGUE",
        "outputId": "97473969-faf8-4816-9b44-2fe76d947f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 30350.18, valid ppl: 30308.54\n",
            "Best validation loss: 30308.536722044606\n",
            "5000 steps; 5120000 examples so far; train ppl: 1931.23, valid ppl: 1285.79\n",
            "Best validation loss: 1285.793127230622\n",
            "10000 steps; 10240000 examples so far; train ppl: 1128.96, valid ppl: 954.71\n",
            "Best validation loss: 954.7130947169651\n",
            "15000 steps; 15360000 examples so far; train ppl: 880.49, valid ppl: 772.84\n",
            "Best validation loss: 772.8367997076945\n",
            "20000 steps; 20480000 examples so far; train ppl: 735.41, valid ppl: 659.50\n",
            "Best validation loss: 659.496274964666\n",
            "25000 steps; 25600000 examples so far; train ppl: 641.89, valid ppl: 579.39\n",
            "Best validation loss: 579.3934090513076\n",
            "30000 steps; 30720000 examples so far; train ppl: 566.88, valid ppl: 518.47\n",
            "Best validation loss: 518.4669690055267\n",
            "35000 steps; 35840000 examples so far; train ppl: 509.69, valid ppl: 470.96\n",
            "Best validation loss: 470.96044912312414\n",
            "40000 steps; 40960000 examples so far; train ppl: 467.52, valid ppl: 431.98\n",
            "Best validation loss: 431.98464840961947\n",
            "45000 steps; 46080000 examples so far; train ppl: 433.13, valid ppl: 400.81\n",
            "Best validation loss: 400.80638751902046\n",
            "50000 steps; 51200000 examples so far; train ppl: 405.85, valid ppl: 375.16\n",
            "Best validation loss: 375.1639015452232\n",
            "55000 steps; 56320000 examples so far; train ppl: 381.00, valid ppl: 353.89\n",
            "Best validation loss: 353.88616443175675\n",
            "60000 steps; 61440000 examples so far; train ppl: 353.90, valid ppl: 336.39\n",
            "Best validation loss: 336.3931645785205\n",
            "65000 steps; 66560000 examples so far; train ppl: 339.05, valid ppl: 320.35\n",
            "Best validation loss: 320.350220686854\n",
            "70000 steps; 71680000 examples so far; train ppl: 325.13, valid ppl: 307.20\n",
            "Best validation loss: 307.1977488019779\n",
            "75000 steps; 76800000 examples so far; train ppl: 313.32, valid ppl: 295.67\n",
            "Best validation loss: 295.66875624922795\n",
            "80000 steps; 81920000 examples so far; train ppl: 302.91, valid ppl: 285.41\n",
            "Best validation loss: 285.407379063369\n",
            "85000 steps; 87040000 examples so far; train ppl: 287.47, valid ppl: 276.43\n",
            "Best validation loss: 276.43396782943177\n",
            "90000 steps; 92160000 examples so far; train ppl: 279.38, valid ppl: 268.65\n",
            "Best validation loss: 268.65165516622454\n",
            "95000 steps; 97280000 examples so far; train ppl: 272.89, valid ppl: 261.68\n",
            "Best validation loss: 261.6831455846814\n",
            "100000 steps; 102400000 examples so far; train ppl: 266.04, valid ppl: 255.27\n",
            "Best validation loss: 255.27109475470283\n",
            "105000 steps; 107520000 examples so far; train ppl: 261.07, valid ppl: 249.47\n",
            "Best validation loss: 249.47049324431427\n",
            "110000 steps; 112640000 examples so far; train ppl: 253.49, valid ppl: 244.45\n",
            "Best validation loss: 244.44839919131041\n",
            "115000 steps; 117760000 examples so far; train ppl: 245.59, valid ppl: 239.56\n",
            "Best validation loss: 239.5622150610126\n",
            "120000 steps; 122880000 examples so far; train ppl: 241.92, valid ppl: 235.28\n",
            "Best validation loss: 235.28139879570736\n",
            "125000 steps; 128000000 examples so far; train ppl: 238.39, valid ppl: 231.48\n",
            "Best validation loss: 231.47991540873647\n",
            "130000 steps; 133120000 examples so far; train ppl: 235.30, valid ppl: 227.75\n",
            "Best validation loss: 227.7524586571861\n",
            "135000 steps; 138240000 examples so far; train ppl: 232.28, valid ppl: 224.29\n",
            "Best validation loss: 224.28521705586255\n",
            "140000 steps; 143360000 examples so far; train ppl: 223.30, valid ppl: 221.36\n",
            "Best validation loss: 221.36369798147553\n",
            "145000 steps; 148480000 examples so far; train ppl: 221.58, valid ppl: 218.51\n",
            "Best validation loss: 218.51249507781156\n",
            "150000 steps; 153600000 examples so far; train ppl: 220.20, valid ppl: 215.29\n",
            "Best validation loss: 215.29385352212782\n",
            "155000 steps; 158720000 examples so far; train ppl: 217.06, valid ppl: 213.20\n",
            "Best validation loss: 213.20278192748643\n",
            "160000 steps; 163840000 examples so far; train ppl: 216.01, valid ppl: 210.56\n",
            "Best validation loss: 210.55889760852492\n",
            "165000 steps; 168960000 examples so far; train ppl: 210.66, valid ppl: 208.25\n",
            "Best validation loss: 208.25474369265828\n",
            "170000 steps; 174080000 examples so far; train ppl: 207.71, valid ppl: 206.39\n",
            "Best validation loss: 206.3944196886113\n",
            "175000 steps; 179200000 examples so far; train ppl: 206.27, valid ppl: 204.60\n",
            "Best validation loss: 204.59950759527234\n",
            "180000 steps; 184320000 examples so far; train ppl: 204.37, valid ppl: 202.48\n",
            "Best validation loss: 202.48209586783707\n",
            "185000 steps; 189440000 examples so far; train ppl: 203.11, valid ppl: 200.55\n",
            "Best validation loss: 200.55005988309662\n",
            "190000 steps; 194560000 examples so far; train ppl: 201.26, valid ppl: 198.83\n",
            "Best validation loss: 198.82613359286316\n",
            "195000 steps; 199680000 examples so far; train ppl: 195.55, valid ppl: 197.20\n",
            "Best validation loss: 197.20475945072985\n"
          ]
        }
      ],
      "source": [
        "max_examples = 200_000_000\n",
        "eval_every_steps = 5000\n",
        "lr = 3e-5\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=512,\n",
        ").to(device)\n",
        "\n",
        "save_best_model = SaveBestModel()\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=1024)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "last_loss = 30986.66 #Length vocab size adjust\n",
        "patience = 2\n",
        "trigger_times = 0\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    model.eval()\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "\n",
        "\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        current_loss = valid_ppl\n",
        "\n",
        "        train_loss.append(train_ppl)\n",
        "        valid_loss.append(valid_ppl)\n",
        "\n",
        "#----------------------Early stopping------------------------------------\n",
        "        if current_loss > last_loss:\n",
        "            trigger_times += 1\n",
        "            print('Trigger Times:', trigger_times)\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break\n",
        "\n",
        "        else:\n",
        "            trigger_times = 0\n",
        "\n",
        "        last_loss = current_loss\n",
        "\n",
        "        save_best_model(\n",
        "        last_loss, 0, model, optimizer, nn.functional.cross_entropy\n",
        "        )\n",
        "#------------------------------------------------------------------------        \n",
        "\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgdNymJdNPXP"
      },
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "dgmJebwaHtiG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "86ad9cc6-f42b-41fd-b619-7ae0112372c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f425647f0d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss)\n",
        "\n",
        "#plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6928567-86dd-435c-99e3-a86330bbd4a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 183.8072607120818\n"
          ]
        }
      ],
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHvEs8mPszy_"
      },
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "MA68TbFDHyj7"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de36587-3c06-4b76-b986-bd03dc79a3d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de comer pizza pois me faz um\n",
            "Eu gosto de comer pizza pois me faz um dia\n",
            "Eu gosto de comer pizza pois me faz um dia de\n",
            "Eu gosto de comer pizza pois me faz um dia de dia\n",
            "Eu gosto de comer pizza pois me faz um dia de dia,\n",
            "Eu gosto de comer pizza pois me faz um dia de dia, mas\n",
            "Eu gosto de comer pizza pois me faz um dia de dia, mas não\n",
            "Eu gosto de comer pizza pois me faz um dia de dia, mas não é\n",
            "Eu gosto de comer pizza pois me faz um dia de dia, mas não é o\n",
            "Eu gosto de comer pizza pois me faz um dia de dia, mas não é o que\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor(input_ids_truncated).to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "zXxwr8QUHzvR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "experiment_Mateus_Oliveira_Aula_7_Exercício.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}