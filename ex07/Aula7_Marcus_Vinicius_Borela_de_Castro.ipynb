{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 7 - Exercício - Marcus Borela",
      "provenance": [],
      "collapsed_sections": [
        "O488oyu5l8kC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex07/Aula7_Marcus_Vinicius_Borela_de_Castro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = \"Marcus Vinicius Borela de Castro\"\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "da14cce1-2c07-4e61-efd1-f72f50cf68d7"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Marcus Vinicius Borela de Castro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural simples para prever a proxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Língua\".\n",
        "\n",
        "Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "\n",
        "Alguns conselhos úteis:\n",
        "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação do ambiente"
      ],
      "metadata": {
        "id": "R_19GeXbBGsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3twP0YJC4jmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424e2520-2d45-4d1c-b2bc-ab7ba606899b"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048093c8-2ff9-4503-ca6e-5d58597ae56f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 14 20:33:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41a967c-79c7-4cc7-9bf0-819408084546"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixando as seeds"
      ],
      "metadata": {
        "id": "achvQ78sa3p3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkETIyWGkbOf"
      },
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "  garantimos que os pesos vao ser sempre os mesmos.\n",
        "  fontes de apoio: \n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True "
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViMcw_kVkbOf"
      },
      "source": [
        "inicializa_seed(123)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo Hiperparâmetros iniciais"
      ],
      "metadata": {
        "id": "hABkY5GRtH9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inicia_hparam()->dict:\n",
        "  # Inicialização dos parâmetros\n",
        "  hparam = {}\n",
        "  hparam['max_epochs'] = 10\n",
        "  hparam[\"num_classes\"]= 2\n",
        "  hparam[\"num_workers_dataloader\"] = 2\n",
        "  hparam['batch_size']= 50\n",
        "  hparam['early_stop']= 4\n",
        "  hparam[\"learning_rate\"] = 0.001\n",
        "  hparam['criterion'] = torch.nn.CrossEntropyLoss()  \n",
        "  hparam[\"device\"] = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  if torch.cuda.is_available(): print(torch. cuda. get_device_name(hparam[\"device\"]))    \n",
        "  return hparam"
      ],
      "metadata": {
        "id": "6OOO6ogjtG3Y"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparam=inicia_hparam()\n",
        "hparam['batch_size']= 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkTH4FkstTFn",
        "outputId": "47984219-a21a-4bba-c86d-866f4cb34bab"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparam['num_sentenca_train'] = 50 # 500\n",
        "hparam['num_sentenca_valid'] = 10 # 100\n",
        "hparam['num_sentenca_test'] = 10 # 100"
      ],
      "metadata": {
        "id": "zLz2KpBDuYU3"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando para debug"
      ],
      "metadata": {
        "id": "8v2gtkEPhA0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://zohaib.me/debugging-in-google-collab-notebook/"
      ],
      "metadata": {
        "id": "4ZEqQ7mKg5fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "# %pdb off # desativa debug em exceção\n",
        "# %pdb on  # ativa debug em exceção\n",
        "# ipdb.set_trace(context=8)  para execução nesse ponto"
      ],
      "metadata": {
        "id": "LjrlXHq1hC8n"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %xmode Verbose \n",
        "# %xmode Plain"
      ],
      "metadata": {
        "id": "qQkwh-z_D3V5"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wEkY_g7kgp-"
      },
      "source": [
        "## Rastro (neptune.ai)\n",
        "\n",
        "Gerado rastro da execução no Neptune (detalhes no artigo [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anygzoSagczS"
      },
      "source": [
        "### Importação de libraries para Rastro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsylYpdWdmsZ",
        "outputId": "29f45194-6429-4eb5-fef0-0122055ce7fb"
      },
      "source": [
        "try:\n",
        "  from torchviz import make_dot \n",
        "except ImportError:\n",
        "  !pip install torchviz\n",
        "  from torchviz import make_dot \n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=a20a2b81680f30272e70189950f4fa3fff99cb70b6b237bce648e9aa017aa793\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DotwpZ4JdmsZ"
      },
      "source": [
        "import getpass\n",
        "import copy\n",
        "import time\n",
        "import re\n",
        "import tempfile\n",
        "import os\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2glxk01UfDk"
      },
      "source": [
        "### Código Rastro\n",
        "\n",
        "Busca implementar o rastro proposto em [Rastro-DM: Mineração de Dados com Rastro](https://revista.tcu.gov.br/ojs/index.php/RTCU/article/view/1664), autores Marcus Vinícius Borela de Castro e Remis Balaniuk, com o apoio da [solução Neptune](https://app.neptune.ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-iQwMPiLXO3"
      },
      "source": [
        "def converte_optimizer_state_dict(parm_optimizer)-> dict:\n",
        "  # return str(hparam['optimizer'])\n",
        "  return parm_optimizer.state_dict()['param_groups'][0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwnX3SrGfq4Z"
      },
      "source": [
        "class NeptuneRastroRun():\n",
        "    se_geracao_rastro = True \n",
        "    neptune_project = \"\"\n",
        "    tag_contexto_rastro = \"\"\n",
        "    neptune_api_token = \"\"\n",
        "\n",
        "    def __init__(self, parm_params:dict,  parm_lista_tag:list = None):\n",
        "      # print(f\"NeptuneRastroRun.init: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_params `{parm_params} \")\n",
        "      if self.__class__.se_geracao_rastro:      \n",
        "        self.run_neptune = neptune.init(project=self.__class__.neptune_project, api_token=self.__class__.neptune_api_token, capture_hardware_metrics=True)\n",
        "        self.run_neptune['sys/name'] = self.__class__.tag_contexto_rastro\n",
        "        vparams = copy.deepcopy(parm_params)\n",
        "        if \"optimizer\" in vparams:\n",
        "          vparams[\"optimizer\"] = converte_optimizer_state_dict(vparams[\"optimizer\"])\n",
        "        if 'criterion'  in vparams:\n",
        "          vparams[\"criterion\"] = str(vparams[\"criterion\"])\n",
        "        if 'device' in vparams:\n",
        "          vparams['device'] = str(vparams[\"device\"])\n",
        "        self.device = vparams[\"device\"]\n",
        "        for tag in parm_lista_tag:\n",
        "          self.run_neptune['sys/tags'].add(tag)\n",
        "        self.run_neptune['parameters'] = vparams\n",
        "        self.tmpDir = tempfile.mkdtemp()\n",
        "\n",
        "    @property\n",
        "    def run():\n",
        "      return self.run_neptune\n",
        "\n",
        "    @classmethod\n",
        "    def ativa_geracao_rastro(cls):\n",
        "     cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def def_contexto(cls):\n",
        "     cls.se_geracao_rastro = True      \n",
        "\n",
        "    @classmethod\n",
        "    def desativa_geracao_rastro(cls):\n",
        "     cls.se_geracao_rastro = False      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_status_geracao_rastro(cls):\n",
        "     return cls.se_geracao_rastro      \n",
        "\n",
        "    @classmethod\n",
        "    def retorna_tag_contexto_rastro(cls):\n",
        "     return cls.tag_contexto_rastro \n",
        "\n",
        "    @classmethod\n",
        "    def inicia_contexto(cls, neptune_project, tag_contexto_rastro, neptune_api_token):\n",
        "      assert '.' not in tag_contexto_rastro, \"NeptuneRastroRun.init(): tag_contexto_rastro não pode possuir ponto, pois será usado para gravar nome de arquivo\"      \n",
        "      cls.neptune_api_token = neptune_api_token\n",
        "      cls.tag_contexto_rastro = tag_contexto_rastro\n",
        "      cls.neptune_project = neptune_project\n",
        "\n",
        "    def salva_metrica(self, parm_metricas={}):\n",
        "      #print(f\"NeptuneRastroRun.salva_metrica: se_geracao_rastro {self.__class__.se_geracao_rastro} parm_metricas:{parm_metricas} \")\n",
        "      if self.__class__.se_geracao_rastro:\n",
        "        for metrica, valor in parm_metricas.items(): \n",
        "          self.run_neptune[metrica].log(valor)\n",
        " \n",
        "    def gera_grafico_modelo(self, loader_train, model):\n",
        "      if self.__class__.se_geracao_rastro: \n",
        "        # efetuar um forward \n",
        "        \"\"\"\n",
        "        se dataloader devolver x e y:\n",
        "        \"\"\"\n",
        "        x_, y_ = next(iter(loader_train))\n",
        "        x_ = x_.to(self.device)\n",
        "        outputs = model(x_)\n",
        "        \"\"\"\n",
        "        # se dataloader devolver dict:\n",
        "        dados_ = next(iter(loader_train))\n",
        "        outputs = model(dados_['x'].to(self.device))\n",
        "        #outputs = model(x_['input_ids'].to(self.device), x_['attention_mask'].to(self.device))\n",
        "        \"\"\"\n",
        "        nome_arquivo = os.path.join(self.tmpDir, \"modelo \"+ self.__class__.tag_contexto_rastro + time.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
        "        make_dot(outputs, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(nome_arquivo, format=\"png\")\n",
        "        self.run_neptune[\"parameters/model_graph\"].upload(nome_arquivo+'.png')\n",
        "        self.run_neptune['parameters/model'] = re.sub('<bound method Module.state_dict of ', '',str(model.state_dict))      \n",
        "\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "      if self.__class__.se_geracao_rastro:         \n",
        "        self.run_neptune.stop()\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O488oyu5l8kC"
      },
      "source": [
        "### Definindo parâmetros para o rastro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY4Hjvp1wR9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "e180939e-3714-458b-b9f9-9b9d9017693a"
      },
      "source": [
        "NeptuneRastroRun.inicia_contexto('marcusborela/IA025', 'Aula 7 - Modelo de linguagem',   getpass.getpass('Informe NEPTUNE_API_TOKEN'))\n",
        "#NeptuneRastroRun.desativa_geracao_rastro()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-e139cca0cafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNeptuneRastroRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minicia_contexto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'marcusborela/IA025'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Aula 6 - Resnet'\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Informe NEPTUNE_API_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#NeptuneRastroRun.desativa_geracao_rastro()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str, tokenizer):\n",
        "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n"
      ],
      "metadata": {
        "id": "wjJskE24A1DM"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
        "      # Escreva seu código aqui\n",
        "      assert isinstance(texts,list), 'texts deve ser do tipo list'\n",
        "      assert isinstance(texts[0],str), 'texts deve ser do tipo iterator of iterator of strings'\n",
        "      assert isinstance(context_size,int), 'context_size deve ser do tipo int'\n",
        "      self.tokenizer = tokenizer\n",
        "      self.context_size = context_size\n",
        "      ind_primeira_vez = True\n",
        "      for texto in texts: \n",
        "        texto_numericalizado = torch.LongTensor(tokenize(texto, self.tokenizer))\n",
        "        num_palavras_texto = len(texto_numericalizado)\n",
        "        for num_amostra_texto in range(0,num_palavras_texto-self.context_size):\n",
        "          # ipdb.set_trace(context=6)\n",
        "          if ind_primeira_vez:\n",
        "            ind_primeira_vez = False\n",
        "            self.x_data = texto_numericalizado[num_amostra_texto:num_amostra_texto + self.context_size].unsqueeze(0)\n",
        "            self.y_data = texto_numericalizado[num_amostra_texto + self.context_size].unsqueeze(0)  # nesse ponto unsqueeze transforma tensor(x) em tensor([x]) \n",
        "          else:\n",
        "            self.x_data = torch.cat((self.x_data,texto_numericalizado[num_amostra_texto:num_amostra_texto + self.context_size].unsqueeze(0)))\n",
        "            self.y_data = torch.cat((self.y_data, texto_numericalizado[num_amostra_texto + self.context_size].unsqueeze(0)))  # nesse ponto unsqueeze transforma tensor(x) em tensor([x]) \n",
        "        # ipdb.set_trace(context=6)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva seu código aqui\n",
        "        return len(self.y_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva seu código aqui\n",
        "        return self.x_data[idx], self.y_data[idx]"
      ],
      "metadata": {
        "id": "cg-7js9PA3V8"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste se sua implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']"
      ],
      "metadata": {
        "id": "SKwP0mWxA_c8"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEL0PKVjUaGC",
        "outputId": "e429571c-4b9d-41d6-e153-21833d053f05"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_texto_tokenizado = tokenize(dummy_texts,tokenizer)\n",
        "type(dummy_texto_tokenizado), dummy_texto_tokenizado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zlErQeRDSUp",
        "outputId": "ba48509e-6c8e-4e1a-b87f-9d4d388b5c67"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, [[3396, 10303, 125, 13239], [1660, 5971, 785, 125, 1847, 13779, 15616]])"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125],\n",
        "     [ 1660,  5971,   785],\n",
        "     [ 5971,   785,   125],\n",
        "     [  785,   125,  1847],\n",
        "     [  125,  1847, 13779]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor([13239,   125,  1847, 13779, 15616])\n",
        "\n",
        "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "print('Passou no assert de input')\n",
        "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "print('Passou no assert de target')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5IbbfpDNfAo",
        "outputId": "eff6c6fc-1f8c-4d0c-84bb-e39e6703b5e6"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de input\n",
            "Passou no assert de target\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
      ],
      "metadata": {
        "id": "vGlN1WqrXPA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565188b0-bf82-43c6-d9b7-93090c17ad15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-14 19:19:09--  https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.145.128, 74.125.128.128, 74.125.143.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.145.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123983611 (118M) [text/plain]\n",
            "Saving to: ‘sample_brwac.txt’\n",
            "\n",
            "sample_brwac.txt    100%[===================>] 118.24M   110MB/s    in 1.1s    \n",
            "\n",
            "2022-05-14 19:19:11 (110 MB/s) - ‘sample_brwac.txt’ saved [123983611/123983611]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "context_size = 9\n",
        "\n",
        "texts = open('sample_brwac.txt').readlines()\n",
        "\n",
        "print('Truncating for debugging purposes.')\n",
        "texts = texts[:hparam['num_sentenca_train']]  \n",
        "\n",
        "training_texts = texts[:-(hparam['num_sentenca_valid'] + hparam['num_sentenca_test'])]\n",
        "valid_texts = texts[-(hparam['num_sentenca_valid'] + hparam['num_sentenca_test']):-hparam['num_sentenca_test']]\n",
        "test_texts = texts[-hparam['num_sentenca_test']:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c446b64c-deb9-4a61-c0fe-50601e05fc57"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncating for debugging purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hparam['train_size'] = len(training_dataset) \n",
        "hparam['valid_size'] = len(valid_dataset) \n",
        "hparam['test_size'] = len(test_dataset) \n",
        "\n",
        "\n",
        "print(f\"training examples: {hparam['train_size']}\")\n",
        "print(f\"valid examples: {hparam['valid_size']}\")\n",
        "print(f\"test examples: {hparam['test_size']}\")"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a769f9-8e51-4973-a054-ea886f0a1fac"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 32301\n",
            "valid examples: 25156\n",
            "test examples: 4297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_size):\n",
        "        \"\"\"\n",
        "        Implements the Neural Language Model proposed by Bengio et al.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            context_size (int): Size of the sequence to consider as context for prediction.\n",
        "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, context_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=64,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "\n",
        "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "model(sample_train_gpu).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size,)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity.\n",
        "    \"\"\"\n",
        "    # Escreva seu código aqui.\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "sample_train_gpu = sample_train.to(device)\n",
        "target_token_ids = target_token_ids.to(device)\n",
        "logits = model(sample_train_gpu)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=2000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE"
      },
      "source": [
        "max_examples = 100_000_000\n",
        "eval_every_steps = 5000\n",
        "lr = 3e-5\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    context_size=context_size,\n",
        "    embedding_dim=128,\n",
        "    hidden_size=256,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train_step(input, target):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logits = model(input.to(device))\n",
        "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input, target):\n",
        "    logits = model(input)\n",
        "    loss = nn.functional.cross_entropy(logits, target)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for input, target in train_loader:\n",
        "        loss = train_step(input.to(device), target.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(input.to(device), target.to(device))\n",
        "                    for input, target in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "\n",
        "        n_examples += len(input)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(input.to(device), target.to(device))\n",
        "        for input, target in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ?  # Ex: 'Eu gosto de comer pizza pois me faz'\n",
        "max_output_tokens = 10\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
        "    logits = model(input_ids_truncated.to(device))\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rascunho"
      ],
      "metadata": {
        "id": "n4LktPU-RWtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raise Exception(\"Abaixo só rascunho - não cabe execuções fora de contexto \")"
      ],
      "metadata": {
        "id": "c76LLkBiRYBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "UahI_BCKWG-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_sentenca_numericalizada =  tokenize(dummy_texts, tokenizer)\n",
        "type(lista_sentenca_numericalizada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyjnrGR9Unax",
        "outputId": "e9a08fe4-19d6-46dc-b8d6-279ca8ad0c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista_sentenca_numericalizada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX1L8IWYVJGa",
        "outputId": "e959f3e1-44ab-4711-dadd-0c61984e7e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3396, 10303, 125, 13239], [1660, 5971, 785, 125, 1847, 13779, 15616]]"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lista_sentenca_numericalizada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGryWwOFVGPx",
        "outputId": "d31e3493-09d7-40b3-b87f-aa9ce1a9c8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.Tensor(lista_sentenca_numericalizada)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "U5RXM2DXU_Ph",
        "outputId": "0b52aad3-ad4a-4088-d091-a96e9cb21289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-170-a645c1ed2f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_sentenca_numericalizada\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 7)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " tensor_dtt=torch.LongTensor(dummy_texto_tokenizado[0])"
      ],
      "metadata": {
        "id": "_ivfg8mpHrSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_dtt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rhZF2x3H1cs",
        "outputId": "b06947cd-83df-451c-fdef-e88eab9d23e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3396, 10303,   125, 13239])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_dtt[1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqXy-R_lH3Ed",
        "outputId": "d52252c8-1ff4-408a-ee8e-f0d24408c7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([10303,   125])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)"
      ],
      "metadata": {
        "id": "IbBnbhOYBPYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "assert len(dummy_dataset) == 5\n",
        "print('passou no assert de tamanho do dataset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJFkDQ6XHLBG",
        "outputId": "e77ff581-cb88-4ae4-b3e4-b1f774a5f870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "passou no assert de tamanho do dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n"
      ],
      "metadata": {
        "id": "_mEmcunhNAcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch_target\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxCAivvqNg2J",
        "outputId": "80151346-11e4-44fe-eca4-028ea2e6210b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13239,   125,  1847, 13779, 15616])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VCMqYe0NmSh",
        "outputId": "cb0187d7-111d-46ef-f8f0-be1e50acbb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3396, 10303,   125],\n",
              "        [ 1660,  5971,   785],\n",
              "        [ 5971,   785,   125],\n",
              "        [  785,   125,  1847],\n",
              "        [  125,  1847, 13779]])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.LongTensor([]) \n",
        "t1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDq_oAp6PVRn",
        "outputId": "ea2c9743-c0d0-4206-e782-89dc8662afc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([], size=(1, 0), dtype=torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = torch.LongTensor([ 3396, 10303,   125])\n",
        "t2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PRgOW7OPZOX",
        "outputId": "e751146b-22e6-4912-d03b-170a95ba0eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3396, 10303,   125])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2.view(1, -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh1UqWoCP_ou",
        "outputId": "58deb5d7-d1e7-4550-f5ab-cbef014cffdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3396, 10303,   125]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2.reshape(1,-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVdSjiBUQmpO",
        "outputId": "2b801051-cb2d-4f23-af4b-7f8219b5b391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3396, 10303,   125]])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2.unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD6bIgY8QtCd",
        "outputId": "744d7606-a4ad-45db-bd97-b2601a8146d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3396, 10303,   125]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.LongTensor([t2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "OnS-eeImP4B-",
        "outputId": "985ed1ac-c9b2-4086-c45c-d959de06c006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-383441611b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((t1, t2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O__Q3N4cPgM-",
        "outputId": "719d2b35-0319-40ef-c6fe-363799f41935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3396, 10303,   125])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    }
  ]
}