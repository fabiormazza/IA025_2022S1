{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Aula 5 - Exercício - Marcus Borela",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex05/Marcus_Vinicius_Borela_de_Castro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Marcus Vinícius Borela de CAstro'\n",
        "\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "CdORg7oe68oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2abd1da-d8fd-48bf-d9f0-fcf8f53300e2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Marcus Vinícius Borela de CAstro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "Este exercicío consiste em treinar no MNIST um modelo de duas camadas, sendo a primeira uma camada convolucional e a segunda uma camada linear de classificação.\n",
        "\n",
        "Não podemos usar as funções torch.nn.Conv{1,2,3}d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:14.033692Z",
          "start_time": "2018-08-21T14:08:11.179981Z"
        },
        "id": "-fLUSHaCQT1x"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixando as seeds"
      ],
      "metadata": {
        "id": "achvQ78sa3p3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkETIyWGkbOf"
      },
      "source": [
        "def inicializa_seed(num_semente:int=123):\n",
        "  \"\"\"\n",
        "  É recomendado reiniciar as seeds antes de inicializar o modelo, pois assim\n",
        "  garantimos que os pesos vao ser sempre os mesmos.\n",
        "  fontes de apoio: \n",
        "      http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "      https://github.com/CyberZHG/torch-multi-head-attention/blob/master/torch_multi_head_attention/multi_head_attention.py#L15\n",
        "  \"\"\"\n",
        "  random.seed(num_semente)\n",
        "  np.random.seed(num_semente)\n",
        "  torch.manual_seed(num_semente)\n",
        "  #torch.cuda.manual_seed(num_semente)\n",
        "  #Cuda algorithms\n",
        "  #torch.backends.cudnn.deterministic = True "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViMcw_kVkbOf"
      },
      "source": [
        "inicializa_seed(123)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define pesos iniciais"
      ],
      "metadata": {
        "id": "fzurMVpHxcNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "out_channels = 2\n",
        "kernel_size = 5\n",
        "stride = 3\n",
        "\n",
        "# Input image size\n",
        "height_in = 28  \n",
        "width_in = 28\n",
        "\n",
        "# Image size after the first convolutional layer.\n",
        "height_out = (height_in - kernel_size - 1) // stride + 1\n",
        "width_out = (width_in - kernel_size - 1) // stride + 1\n",
        "\n",
        "\n",
        "initial_conv_weight = torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01)\n",
        "initial_conv_bias = torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01)\n",
        "\n",
        "initial_classification_weight = torch.FloatTensor(10, out_channels * height_out * width_out).uniform_(-0.01, 0.01)\n",
        "initial_classification_bias = torch.FloatTensor(10,).uniform_(-0.01, 0.01)"
      ],
      "metadata": {
        "id": "9a6jQJLLlfF3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" height_out {height_out}, width_out {width_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y2-ILXqbE1d",
        "outputId": "93d458e8-f331-4e65-b5a2-ca956aab2873"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " height_out 8, width_out 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoQjDs_QT12"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:08:20.282474Z",
          "start_time": "2018-08-21T14:08:20.275450Z"
        },
        "id": "tEQYUr4TQT13"
      },
      "source": [
        "batch_size = 50"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7Rv_2BQT16"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T14:10:45.430605Z",
          "start_time": "2018-08-21T14:10:04.953051Z"
        },
        "id": "G0dEKCn-QT17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa3f3802-4b72-448f-eebc-9252853726e3"
      },
      "source": [
        "dataset_dir = '../data/'\n",
        "\n",
        "dataset_train_full = MNIST(dataset_dir, train=True, download=True,\n",
        "                           transform=torchvision.transforms.ToTensor())\n",
        "print(dataset_train_full.data.shape)\n",
        "print(dataset_train_full.targets.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rOy9ntrQT2D"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exercício utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNF2XjLBWWe7"
      },
      "source": [
        "indices = torch.randperm(len(dataset_train_full))[:1000]\n",
        "dataset_train = torch.utils.data.Subset(dataset_train_full, indices)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define os pesos iniciais"
      ],
      "metadata": {
        "id": "wYqj_oeSliYj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSNLD2JyA2e-"
      },
      "source": [
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-21T13:30:35.209157Z",
          "start_time": "2018-08-21T13:30:34.757103Z"
        },
        "id": "w52KGYlIQT2A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0695558-0944-46e4-8d98-982fc8e3e44d"
      },
      "source": [
        "print('Número de minibatches de trenamento:', len(loader_train))\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7vPdWoIbwht",
        "outputId": "6dd4828d-72fd-4aee-c7af-980f1616c348"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([50, 1, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYw9dR89b3Is",
        "outputId": "905c5825-c9da-463e-8eb5-6733ff8f7a75"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([50, 1, 28, 28]), torch.Size([50]))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0, 0, 0, ] # 1a linha da 1a amostra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6g_7GhLcJak",
        "outputId": "6759fee7-e0bb-400f-cd4f-f0a61de17dd4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0, 0,] # 28 linhas da 1a amostra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J64G0oDcYed",
        "outputId": "307e14c4-5db2-4ccb-bbdd-5e24e949273e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2980, 0.9922, 0.9922, 0.6235,\n",
              "         0.3373, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0667, 0.0196, 0.0353, 0.3608, 0.4824, 0.8745,\n",
              "         0.9882, 0.7569, 0.1490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.3255, 0.8196, 0.4196, 0.0000, 0.0000, 0.0000, 0.0980,\n",
              "         0.6784, 0.9922, 0.9412, 0.3216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0667, 0.8196, 0.6902, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.4588, 0.9882, 0.9412, 0.1451, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.2902, 0.9176, 0.9882, 0.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0118, 0.4588, 0.9882, 0.7529, 0.0431, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4980,\n",
              "         1.0000, 0.9765, 0.4510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.1686, 0.9686, 0.9922, 0.3373, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9569,\n",
              "         0.9765, 0.4588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.4353, 0.9882, 0.5373, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3255, 0.9412, 0.9882,\n",
              "         0.4510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0745, 0.8588, 0.8667, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9412, 0.9882, 0.6157,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9882, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922, 0.9882, 0.9882, 0.1255,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9882, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3412, 0.9922, 0.9922, 0.1804, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.2745, 0.9922, 0.9529, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5412, 0.9882, 0.8157, 0.0667, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.3569, 0.9882, 0.5373, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5412, 0.9882, 0.5922, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.1333, 0.9176, 0.9882, 0.4157, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5412, 0.9882, 0.2706, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0353, 0.4784, 0.9882, 0.8549, 0.0549, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9529, 0.8235, 0.0235, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.5020, 0.9882, 0.9882, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8275, 0.0275, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
              "         0.5020, 1.0000, 0.9765, 0.4510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7059, 0.9882, 0.6039, 0.0353, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2588, 0.7608,\n",
              "         0.9882, 0.8941, 0.4588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3961, 0.8902, 0.9882, 0.6039, 0.2745,\n",
              "         0.2510, 0.1255, 0.2000, 0.2745, 0.2745, 0.5176, 0.7216, 0.9176, 0.9882,\n",
              "         0.7412, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961, 0.6275, 0.9255, 0.9882,\n",
              "         0.9765, 0.8941, 0.9412, 0.9882, 0.9882, 0.9922, 0.9216, 0.6275, 0.2588,\n",
              "         0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.0863,\n",
              "         0.5373, 0.5373, 0.6588, 0.8235, 0.5373, 0.2941, 0.0706, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape[1] # canais"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yo8ov5QdUNk",
        "outputId": "503dc776-9006-47e7-9ae8-4b3afd3affd7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camada Convolucional"
      ],
      "metadata": {
        "id": "dfU_v7aPfq40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((4,1,4,5),  dtype=torch.float, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWnT39vv2UFk",
        "outputId": "cbeb2434-3c56-4f2c-ba89-24284e39402b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0.]]]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saida = torch.empty((4,1,4,5),  dtype=torch.float, requires_grad=True)"
      ],
      "metadata": {
        "id": "uR3td8Fn7KZZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saida.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BwNMw6q7Q6I",
        "outputId": "81cf4a62-5dbf-4dcf-c3dd-3dbca0e53812"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saida[0,0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8PB0cWe92X-",
        "outputId": "2b8eedd7-b4c3-4ba4-b97f-11a795bfc601"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3753e-35, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cat((saida[0,0,0], torch.tensor([[12]])), dim=-1)"
      ],
      "metadata": {
        "id": "-FebDt7c__d1"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saida"
      ],
      "metadata": {
        "id": "1Qi-eoW97Sp_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConv2d(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Versão com for sobre o kernel \n",
        "  \"\"\"\n",
        "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, verbose:bool = False):\n",
        "    super(MyConv2d, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size  # The same for height and width.\n",
        "    self.stride = stride  # The same for height and width.\n",
        "    self.weight = torch.nn.Parameter(torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01))\n",
        "    self.bias = torch.nn.Parameter(torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01))\n",
        "    self.verbose = verbose\n",
        "    if self.verbose:\n",
        "      print(f\"Inicializado MyConv2d\")\n",
        "      print(f\"in_channels: {self.in_channels} \")\n",
        "      print(f\"out_channels: {self.out_channels} \")\n",
        "      print(f\"kernel_size: {self.kernel_size} \")\n",
        "      print(f\"stride: {self.stride} \")\n",
        "      print(f\"weight.shape: {self.weight.shape} \")\n",
        "      print(f\"weight: {self.weight} \")\n",
        "      print(f\"bias.shape: {self.bias.shape} \")\n",
        "      print(f\"bias: {self.bias} \")\n",
        "\n",
        "  def forward(self, x):\n",
        "    assert x.dim() == 4, f'x must have 4 dimensions, not {x.shape}'\n",
        "    assert x.shape[1] == 1, f'x must have only 1 channel, not {x.shape[1]}' # Num_canais sempre 1 (mnist, preto/branco)\n",
        "\n",
        "    # print(f\"kernel.shape: {self.weight.shape}, kernel: {self.weight}\")\n",
        "    # Escreva seu código aqui.\n",
        "    # versão com for nas dimensões de X\n",
        "    num_amostras = x.shape[0]\n",
        "    num_linhas_entrada = x.shape[2]\n",
        "    num_colunas_entrada = x.shape[3]\n",
        "    num_linhas_saida = (num_linhas_entrada - self.kernel_size) // self.stride + 1\n",
        "    num_colunas_saida = (num_colunas_entrada - self.kernel_size) // self.stride + 1\n",
        "    saida = torch.zeros((num_amostras,self.out_channels,num_linhas_saida,num_colunas_saida), dtype=torch.float, requires_grad=False)        \n",
        "    if self.verbose:\n",
        "      print(f\" num_amostras: {num_amostras}, self.out_channels: {self.out_channels}, num_linhas_entrada: {num_linhas_entrada}, num_colunas_entrada: {num_colunas_entrada}, num_linhas_saida: {num_linhas_saida}, num_colunas_saida: {num_colunas_saida}\")\n",
        "      print(f\"saida.shape: {saida.shape}\")\n",
        "    for ndx_amostra in range(num_amostras):\n",
        "      if self.verbose:\n",
        "        print(f\"\\nndx_amostra: {ndx_amostra}\")\n",
        "\n",
        "      for ndx_linhas_kernel in range(self.kernel_size):\n",
        "        for ndx_colunas_kernel in range(self.kernel_size):\n",
        "          if self.verbose:\n",
        "            print(f\"\\nndx_linhas_kernel, ndx_colunas_kernel: {ndx_linhas_kernel}, {ndx_colunas_kernel}\")\n",
        "          # dica para visao_com_stride obtida em https://stackoverflow.com/questions/48097941/strided-convolution-of-2d-in-numpy\n",
        "          visao_com_stride=x[ndx_amostra, 0][ndx_linhas_kernel:num_linhas_entrada-self.kernel_size+ndx_linhas_kernel+1:self.stride,\n",
        "                             ndx_colunas_kernel: num_colunas_entrada - self.kernel_size + ndx_colunas_kernel +1:self.stride]\n",
        "          if self.verbose:\n",
        "            print(f\"\\nvisao_com_stride.shape: {visao_com_stride.shape}\")\n",
        "            print(f\"\\nvisao_com_stride: {visao_com_stride}\")\n",
        "          produto = self.weight.data[ndx_amostra,:,ndx_linhas_kernel,ndx_colunas_kernel]*visao_com_stride\n",
        "          if self.verbose:\n",
        "            print(f\"\\nproduto: {produto}\")\n",
        "          if self.verbose:\n",
        "            print(f\"\\nsaida[ndx_amostra,0].shape: {saida[ndx_amostra,0].shape}\")\n",
        "            print(f\"\\nsaida[ndx_amostra,0]: {saida[ndx_amostra,0]}\")\n",
        "          saida[ndx_amostra,0] += produto\n",
        "          \"\"\"\n",
        "          soma = produto.sum(axis=(-1, -2))\n",
        "          if self.verbose:\n",
        "            print(f\"\\nsoma: {soma}\")\n",
        "          \"\"\"\n",
        "    if self.verbose:\n",
        "      print(f\" saida: {saida}\")\n",
        "    # somando bias\n",
        "    for ndx_amostra in range(num_amostras):\n",
        "      if self.verbose:\n",
        "        print(f\"\\nndx_amostra: {ndx_amostra}\")\n",
        "      for ndx_out_channels in range(self.out_channels):\n",
        "        saida[ndx_amostra, ndx_out_channels] += self.bias[ndx_out_channels]\n",
        "    if self.verbose:\n",
        "      print(f\" saida apos somar bias: {saida}\")\n",
        "    # versão com for no kernel\n",
        "\n",
        "    return saida"
      ],
      "metadata": {
        "id": "4DgFIMilhP9j"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConv2d(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Versão com for sobre as dimensões do filtro de saída\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, verbose:bool = False):\n",
        "    super(MyConv2d, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size  # The same for height and width.\n",
        "    self.stride = stride  # The same for height and width.\n",
        "    self.weight = torch.nn.Parameter(torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01))\n",
        "    self.bias = torch.nn.Parameter(torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01))\n",
        "    self.verbose = verbose\n",
        "    if self.verbose:\n",
        "      print(f\"Inicializado MyConv2d\")\n",
        "      print(f\"in_channels: {self.in_channels} \")\n",
        "      print(f\"out_channels: {self.out_channels} \")\n",
        "      print(f\"kernel_size: {self.kernel_size} \")\n",
        "      print(f\"stride: {self.stride} \")\n",
        "      print(f\"weight.shape: {self.weight.shape} \")\n",
        "      print(f\"weight: {self.weight} \")\n",
        "      print(f\"bias.shape: {self.bias.shape} \")\n",
        "      print(f\"bias: {self.bias} \")\n",
        "\n",
        "  def forward(self, x):\n",
        "    assert x.dim() == 4, f'x must have 4 dimensions, not {x.shape}'\n",
        "    assert x.shape[1] == 1, f'x must have only 1 channel, not {x.shape[1]}' # Num_canais sempre 1 (mnist, preto/branco)\n",
        "\n",
        "    # print(f\"kernel.shape: {self.weight.shape}, kernel: {self.weight}\")\n",
        "    # Escreva seu código aqui.\n",
        "    # versão com for nas dimensões de X\n",
        "    num_amostras = x.shape[0]\n",
        "    num_linhas_entrada = x.shape[2]\n",
        "    num_colunas_entrada = x.shape[3]\n",
        "    num_linhas_saida = (num_linhas_entrada - self.kernel_size) // self.stride + 1\n",
        "    num_colunas_saida = (num_colunas_entrada - self.kernel_size) // self.stride + 1\n",
        "    saida = torch.zeros((num_amostras,self.out_channels,num_linhas_saida,num_colunas_saida), dtype=torch.float, requires_grad=False)        \n",
        "    if self.verbose:\n",
        "      print(f\" num_amostras: {num_amostras}, self.out_channels: {self.out_channels}, num_linhas_entrada: {num_linhas_entrada}, num_colunas_entrada: {num_colunas_entrada}, num_linhas_saida: {num_linhas_saida}, num_colunas_saida: {num_colunas_saida}\")\n",
        "      print(f\"saida.shape: {saida.shape}\")\n",
        "    for ndx_amostra in range(num_amostras):\n",
        "      if self.verbose:\n",
        "        print(f\"\\nndx_amostra: {ndx_amostra}\")\n",
        "      # for ndx_out_channels in range(self.out_channels):\n",
        "      #   print(f\"\\nndx_out_channels: {ndx_out_channels}\")\n",
        "      for ndx_in_channels in range(self.in_channels):\n",
        "        if self.verbose:\n",
        "          print(f\"\\nndx_in_channels: {ndx_in_channels}\")\n",
        "        ndx_linhas_entrada = 0\n",
        "        for ndx_linhas_saida in range(num_linhas_saida):\n",
        "          ndx_colunas_entrada = 0\n",
        "          for ndx_colunas_saida in range(num_colunas_saida):\n",
        "            produto = torch.mul(x[ndx_amostra, ndx_in_channels, ndx_linhas_entrada:ndx_linhas_entrada+self.kernel_size, ndx_colunas_entrada:ndx_colunas_entrada+self.kernel_size], self.weight)\n",
        "            soma = torch.sum(produto, dim=(2,3), keepdim=True )\n",
        "            valor_soma = soma.squeeze()\n",
        "            if self.verbose:\n",
        "              print(f\"\\nndx_linhas_saida, ndx_colunas_saida: {ndx_linhas_saida}, {ndx_colunas_saida}\")\n",
        "              print(f\" alvo do kernel em x: x[{ndx_amostra},{ndx_in_channels},{ndx_linhas_entrada}:{ndx_linhas_entrada+self.kernel_size}, {ndx_colunas_entrada}:{ndx_colunas_entrada+self.kernel_size}]\")\n",
        "              print(f\" \\n {x[ndx_amostra, ndx_in_channels, ndx_linhas_entrada:ndx_linhas_entrada+self.kernel_size, ndx_colunas_entrada:ndx_colunas_entrada+self.kernel_size]}\")\n",
        "              print(f\" produto: {produto}\")\n",
        "              print(f\" soma: {soma}\")\n",
        "              print(f\" valor_soma: {valor_soma}\")\n",
        "            # saida = torch.cat((saida, soma))\n",
        "            if self.out_channels > 1:  # soma é um com dimensões, como em torch.tensor([[[[ 46.]], [[134.]]]])\n",
        "              for ndx_out_channels in range(self.out_channels):\n",
        "                saida[ndx_amostra, ndx_out_channels, ndx_linhas_saida, ndx_colunas_saida] += valor_soma[ndx_out_channels]\n",
        "                if self.verbose:\n",
        "                  print(f\" somado na saída em [{ndx_amostra}, {ndx_out_channels}, {ndx_linhas_saida}, {ndx_colunas_saida}] = {saida[ndx_amostra, ndx_out_channels, ndx_linhas_saida, ndx_colunas_saida]}\")\n",
        "            else: # soma é um tensor escalar, como em tensor(34.)\n",
        "                saida[ndx_amostra, 0, ndx_linhas_saida, ndx_colunas_saida] += valor_soma\n",
        "                if self.verbose:\n",
        "                  print(f\" somado na saída em [{ndx_amostra}, 0, {ndx_linhas_saida}, {ndx_colunas_saida}] = {saida[ndx_amostra, 0, ndx_linhas_saida, ndx_colunas_saida]}\")\n",
        "            ndx_colunas_entrada += self.stride\n",
        "          ndx_linhas_entrada += self.stride\n",
        "    if self.verbose:\n",
        "      print(f\" saida: {saida}\")\n",
        "    # somando bias\n",
        "    for ndx_amostra in range(num_amostras):\n",
        "      if self.verbose:\n",
        "        print(f\"\\nndx_amostra: {ndx_amostra}\")\n",
        "      for ndx_out_channels in range(self.out_channels):\n",
        "        saida[ndx_amostra, ndx_out_channels] += self.bias[ndx_out_channels]\n",
        "    if self.verbose:\n",
        "      print(f\" saida apos somar bias: {saida}\")\n",
        "    # versão com for no kernel\n",
        "\n",
        "    return saida"
      ],
      "metadata": {
        "id": "urzHK0QB_hyY"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo simples"
      ],
      "metadata": {
        "id": "ROizI33sqE79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels_dummy = 1\n",
        "out_channels_dummy = 1\n",
        "kernel_size_dummy = 2\n",
        "stride_dummy = 1\n",
        "x = torch.arange(30).float().reshape(1, 1, 5, 6)"
      ],
      "metadata": {
        "id": "i1TuxWbkqMJc"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-FSsEC9icLX",
        "outputId": "6466509c-5f26-4a80-8561-29b83f8c1c0b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
            "          [12., 13., 14., 15., 16., 17.],\n",
            "          [18., 19., 20., 21., 22., 23.],\n",
            "          [24., 25., 26., 27., 28., 29.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko7dGDl66A8k",
        "outputId": "194e97aa-6a5a-458a-e430-d2713742217d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
              "        [ 6.,  7.,  8.,  9., 10., 11.],\n",
              "        [12., 13., 14., 15., 16., 17.],\n",
              "        [18., 19., 20., 21., 22., 23.],\n",
              "        [24., 25., 26., 27., 28., 29.]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpjEOuG5XFE",
        "outputId": "35d9fdc1-5e15-4f87-8c9e-3cae4bbe1b15"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 1.],\n",
              "          [2., 3.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_layer = MyConv2d( in_channels=in_channels_dummy,out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, verbose=True)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "initial_weights_dummy = torch.arange(in_channels_dummy * out_channels_dummy * kernel_size_dummy * kernel_size_dummy).float()\n",
        "initial_weights_dummy = initial_weights_dummy.reshape(in_channels_dummy, out_channels_dummy, kernel_size_dummy, kernel_size_dummy)\n",
        "initial_bias_dummy = torch.arange(out_channels_dummy,).float()\n",
        "\n",
        "conv_layer.weight.data = initial_weights_dummy\n",
        "conv_layer.bias.data = initial_bias_dummy\n",
        "\n",
        "out = conv_layer(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASbzrpDHiaxm",
        "outputId": "309dbbe7-6c41-4c6e-83fe-d4b9d806d647"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializado MyConv2d\n",
            "in_channels: 1 \n",
            "out_channels: 1 \n",
            "kernel_size: 2 \n",
            "stride: 1 \n",
            "weight.shape: torch.Size([1, 1, 2, 2]) \n",
            "weight: Parameter containing:\n",
            "tensor([[[[ 0.0096,  0.0095],\n",
            "          [-0.0079, -0.0069]]]], requires_grad=True) \n",
            "bias.shape: torch.Size([1]) \n",
            "bias: Parameter containing:\n",
            "tensor([-0.0007], requires_grad=True) \n",
            " num_amostras: 1, self.out_channels: 1, num_linhas_entrada: 5, num_colunas_entrada: 6, num_linhas_saida: 4, num_colunas_saida: 5\n",
            "saida.shape: torch.Size([1, 1, 4, 5])\n",
            "\n",
            "ndx_amostra: 0\n",
            "\n",
            "ndx_linhas_kernel, ndx_colunas_kernel: 0, 0\n",
            "\n",
            "visao_com_stride.shape: torch.Size([4, 5])\n",
            "\n",
            "visao_com_stride: tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
            "        [ 6.,  7.,  8.,  9., 10.],\n",
            "        [12., 13., 14., 15., 16.],\n",
            "        [18., 19., 20., 21., 22.]])\n",
            "\n",
            "produto: tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "\n",
            "saida[ndx_amostra,0].shape: torch.Size([4, 5])\n",
            "\n",
            "saida[ndx_amostra,0]: tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "\n",
            "ndx_linhas_kernel, ndx_colunas_kernel: 0, 1\n",
            "\n",
            "visao_com_stride.shape: torch.Size([4, 5])\n",
            "\n",
            "visao_com_stride: tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 7.,  8.,  9., 10., 11.],\n",
            "        [13., 14., 15., 16., 17.],\n",
            "        [19., 20., 21., 22., 23.]])\n",
            "\n",
            "produto: tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 7.,  8.,  9., 10., 11.],\n",
            "        [13., 14., 15., 16., 17.],\n",
            "        [19., 20., 21., 22., 23.]])\n",
            "\n",
            "saida[ndx_amostra,0].shape: torch.Size([4, 5])\n",
            "\n",
            "saida[ndx_amostra,0]: tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "\n",
            "ndx_linhas_kernel, ndx_colunas_kernel: 1, 0\n",
            "\n",
            "visao_com_stride.shape: torch.Size([4, 5])\n",
            "\n",
            "visao_com_stride: tensor([[ 6.,  7.,  8.,  9., 10.],\n",
            "        [12., 13., 14., 15., 16.],\n",
            "        [18., 19., 20., 21., 22.],\n",
            "        [24., 25., 26., 27., 28.]])\n",
            "\n",
            "produto: tensor([[12., 14., 16., 18., 20.],\n",
            "        [24., 26., 28., 30., 32.],\n",
            "        [36., 38., 40., 42., 44.],\n",
            "        [48., 50., 52., 54., 56.]])\n",
            "\n",
            "saida[ndx_amostra,0].shape: torch.Size([4, 5])\n",
            "\n",
            "saida[ndx_amostra,0]: tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
            "        [ 7.,  8.,  9., 10., 11.],\n",
            "        [13., 14., 15., 16., 17.],\n",
            "        [19., 20., 21., 22., 23.]])\n",
            "\n",
            "ndx_linhas_kernel, ndx_colunas_kernel: 1, 1\n",
            "\n",
            "visao_com_stride.shape: torch.Size([4, 5])\n",
            "\n",
            "visao_com_stride: tensor([[ 7.,  8.,  9., 10., 11.],\n",
            "        [13., 14., 15., 16., 17.],\n",
            "        [19., 20., 21., 22., 23.],\n",
            "        [25., 26., 27., 28., 29.]])\n",
            "\n",
            "produto: tensor([[21., 24., 27., 30., 33.],\n",
            "        [39., 42., 45., 48., 51.],\n",
            "        [57., 60., 63., 66., 69.],\n",
            "        [75., 78., 81., 84., 87.]])\n",
            "\n",
            "saida[ndx_amostra,0].shape: torch.Size([4, 5])\n",
            "\n",
            "saida[ndx_amostra,0]: tensor([[13., 16., 19., 22., 25.],\n",
            "        [31., 34., 37., 40., 43.],\n",
            "        [49., 52., 55., 58., 61.],\n",
            "        [67., 70., 73., 76., 79.]])\n",
            " saida: tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]])\n",
            "\n",
            "ndx_amostra: 0\n",
            " saida apos somar bias: tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_conv_layer = torch.nn.Conv2d(out_channels=out_channels_dummy, in_channels=in_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, padding=0)\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_weights_dummy, bias=initial_bias_dummy))\n",
        "target_out = pytorch_conv_layer(x)"
      ],
      "metadata": {
        "id": "xN--jid1fn-p"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blRMn_9hgIGZ",
        "outputId": "0a4546cf-4ab1-42d2-cc89-7986ccf47998"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "FUgUwtXPgGaD"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo aleatório"
      ],
      "metadata": {
        "id": "_75UnRhdd_MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2, in_channels, height_in, width_in)\n",
        "print(f\"x.shape: {x.shape}, x:{x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7X_A2jeDs4f",
        "outputId": "e2854033-fd29-4f33-dc02-43081964bbde"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: torch.Size([2, 1, 28, 28]), x:tensor([[[[0.5444, 0.8069, 0.4608,  ..., 0.0925, 0.1576, 0.1809],\n",
            "          [0.5935, 0.9664, 0.1442,  ..., 0.0447, 0.2092, 0.6473],\n",
            "          [0.8107, 0.8820, 0.0295,  ..., 0.8798, 0.3925, 0.7311],\n",
            "          ...,\n",
            "          [0.4897, 0.1014, 0.9633,  ..., 0.0432, 0.0477, 0.4564],\n",
            "          [0.2797, 0.0936, 0.2211,  ..., 0.0542, 0.8944, 0.0662],\n",
            "          [0.0045, 0.1403, 0.0030,  ..., 0.4913, 0.2372, 0.6304]]],\n",
            "\n",
            "\n",
            "        [[[0.7377, 0.1925, 0.7885,  ..., 0.7508, 0.7382, 0.2218],\n",
            "          [0.7311, 0.8451, 0.5695,  ..., 0.3861, 0.1616, 0.8981],\n",
            "          [0.8360, 0.3942, 0.3983,  ..., 0.4764, 0.1275, 0.5789],\n",
            "          ...,\n",
            "          [0.6245, 0.3001, 0.6014,  ..., 0.0493, 0.9965, 0.3818],\n",
            "          [0.4617, 0.3987, 0.2691,  ..., 0.9618, 0.1460, 0.4024],\n",
            "          [0.5799, 0.5014, 0.4983,  ..., 0.4949, 0.1673, 0.8714]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = MyConv2d(out_channels=out_channels, in_channels=in_channels, kernel_size=kernel_size, stride=stride, verbose=True)\n",
        "conv_layer.weight.data = initial_conv_weight\n",
        "conv_layer.bias.data = initial_conv_bias\n",
        "print(f\"conv_layer.weight.data.shape: {conv_layer.weight.data.shape}, conv_layer.bias.data.shape: {conv_layer.bias.data.shape}\")\n",
        "print(f\"conv_layer.weight.data: {conv_layer.weight.data}, conv_layer.bias.data: {conv_layer.bias.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPL18MC-Ed9x",
        "outputId": "79ab0f77-45a5-49a0-e59c-916b2efbb316"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializado MyConv2d\n",
            "in_channels: 1 \n",
            "out_channels: 2 \n",
            "kernel_size: 5 \n",
            "stride: 3 \n",
            "weight.shape: torch.Size([2, 1, 5, 5]) \n",
            "weight: Parameter containing:\n",
            "tensor([[[[ 6.8721e-03,  1.0488e-03,  8.2140e-03, -2.6509e-03,  9.7976e-03],\n",
            "          [-9.7086e-03,  5.0833e-03, -9.3138e-04, -8.3314e-03,  2.4552e-03],\n",
            "          [ 6.2312e-03, -8.1002e-04,  9.8112e-03,  6.8703e-03, -4.4498e-04],\n",
            "          [-3.2657e-03,  6.5942e-03, -9.0181e-03,  6.3381e-03,  1.0239e-04],\n",
            "          [-8.9177e-03,  8.7320e-04,  9.4075e-03,  8.0192e-03,  9.1731e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1533e-03, -9.2626e-03, -8.8386e-03, -4.3859e-03,  2.9526e-04],\n",
            "          [ 8.8358e-03, -9.1525e-03, -5.9753e-03,  4.2808e-03,  9.9938e-05],\n",
            "          [ 5.9728e-03,  9.5903e-03, -3.9046e-03,  5.4655e-03,  5.1411e-03],\n",
            "          [-2.1312e-04,  4.8437e-03, -6.9068e-03,  3.1873e-03,  8.5543e-03],\n",
            "          [-4.0623e-03, -5.9359e-03, -3.2313e-05, -5.0214e-03,  7.9310e-03]]]],\n",
            "       requires_grad=True) \n",
            "bias.shape: torch.Size([2]) \n",
            "bias: Parameter containing:\n",
            "tensor([ 0.0044, -0.0062], requires_grad=True) \n",
            "conv_layer.weight.data.shape: torch.Size([2, 1, 5, 5]), conv_layer.bias.data.shape: torch.Size([2])\n",
            "conv_layer.weight.data: tensor([[[[-8.6270e-03, -1.4278e-04, -6.8349e-04,  3.8259e-03, -1.8863e-02],\n",
            "          [ 1.3487e-02,  1.0601e-02,  1.8155e-02,  2.6996e-03, -6.2414e-03],\n",
            "          [ 2.0762e-02,  4.2654e-02,  3.2380e-02,  1.0703e-02, -7.7719e-03],\n",
            "          [ 6.1842e-02,  7.3545e-02,  8.6209e-02,  4.3069e-02,  1.9671e-02],\n",
            "          [ 9.1870e-02,  1.1321e-01,  1.0658e-01,  6.0958e-02,  1.2061e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4021e-01, -2.6000e-02,  1.2446e-01,  1.6889e-01,  6.3819e-02],\n",
            "          [ 1.4768e-02,  2.8285e-01,  5.5958e-01,  5.3341e-01,  4.2006e-01],\n",
            "          [ 1.4713e-01,  5.2281e-01,  8.1167e-01,  7.4344e-01,  5.8766e-01],\n",
            "          [ 2.7532e-01,  6.5197e-01,  8.6848e-01,  7.7362e-01,  5.8130e-01],\n",
            "          [ 1.9373e-01,  5.3128e-01,  6.5517e-01,  4.9403e-01,  3.5819e-01]]]]), conv_layer.bias.data: tensor([-0.0005, -0.0055])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = conv_layer(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fs0iR9kb0Vy",
        "outputId": "f0cc26ec-2f8b-4ae4-cde2-61820b943d69"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " num_amostras: 2, self.out_channels: 2, num_linhas_entrada: 28, num_colunas_entrada: 28, num_linhas_saida: 8, num_colunas_saida: 8\n",
            "saida.shape: torch.Size([2, 2, 8, 8])\n",
            "\n",
            "ndx_amostra: 0\n",
            "\n",
            "ndx_in_channels: 0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 0\n",
            " alvo do kernel em x: x[0,0,0:5, 0:5]\n",
            " \n",
            " tensor([[0.5444, 0.8069, 0.4608, 0.4559, 0.8586],\n",
            "        [0.5935, 0.9664, 0.1442, 0.8255, 0.5865],\n",
            "        [0.8107, 0.8820, 0.0295, 0.1263, 0.6360],\n",
            "        [0.9847, 0.5138, 0.3078, 0.7688, 0.3493],\n",
            "        [0.8739, 0.6939, 0.3555, 0.4899, 0.8051]])\n",
            " produto: tensor([[[[-4.6969e-03, -1.1521e-04, -3.1497e-04,  1.7441e-03, -1.6196e-02],\n",
            "          [ 8.0043e-03,  1.0244e-02,  2.6175e-03,  2.2284e-03, -3.6608e-03],\n",
            "          [ 1.6831e-02,  3.7621e-02,  9.5578e-04,  1.3516e-03, -4.9431e-03],\n",
            "          [ 6.0894e-02,  3.7791e-02,  2.6539e-02,  3.3112e-02,  6.8719e-03],\n",
            "          [ 8.0281e-02,  7.8548e-02,  3.7893e-02,  2.9863e-02,  9.7108e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.6337e-02, -2.0979e-02,  5.7355e-02,  7.6994e-02,  5.4793e-02],\n",
            "          [ 8.7642e-03,  2.7334e-01,  8.0679e-02,  4.4031e-01,  2.4638e-01],\n",
            "          [ 1.1928e-01,  4.6112e-01,  2.3959e-02,  9.3878e-02,  3.7376e-01],\n",
            "          [ 2.7110e-01,  3.3501e-01,  2.6736e-01,  5.9478e-01,  2.0307e-01],\n",
            "          [ 1.6929e-01,  3.6863e-01,  2.3293e-01,  2.4202e-01,  2.8839e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4532]]],\n",
            "\n",
            "\n",
            "        [[[5.1859]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4532, 5.1859], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 0] = 0.4531751871109009\n",
            " somado na saída em [0, 1, 0, 0] = 5.185864448547363\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 1\n",
            " alvo do kernel em x: x[0,0,0:5, 3:8]\n",
            " \n",
            " tensor([[0.4559, 0.8586, 0.0417, 0.3841, 0.6811],\n",
            "        [0.8255, 0.5865, 0.8746, 0.2304, 0.2282],\n",
            "        [0.1263, 0.6360, 0.9178, 0.0733, 0.5015],\n",
            "        [0.7688, 0.3493, 0.1520, 0.6655, 0.0695],\n",
            "        [0.4899, 0.8051, 0.8477, 0.1478, 0.1820]])\n",
            " produto: tensor([[[[-3.9328e-03, -1.2258e-04, -2.8506e-05,  1.4694e-03, -1.2848e-02],\n",
            "          [ 1.1133e-02,  6.2177e-03,  1.5878e-02,  6.2199e-04, -1.4245e-03],\n",
            "          [ 2.6217e-03,  2.7129e-02,  2.9717e-02,  7.8428e-04, -3.8978e-03],\n",
            "          [ 4.7546e-02,  2.5692e-02,  1.3102e-02,  2.8662e-02,  1.3675e-03],\n",
            "          [ 4.5006e-02,  9.1145e-02,  9.0346e-02,  9.0077e-03,  2.1957e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.3918e-02, -2.2323e-02,  5.1909e-03,  6.4868e-02,  4.3466e-02],\n",
            "          [ 1.2190e-02,  1.6590e-01,  4.8942e-01,  1.2290e-01,  9.5874e-02],\n",
            "          [ 1.8579e-02,  3.3252e-01,  7.4491e-01,  5.4475e-02,  2.9473e-01],\n",
            "          [ 2.1167e-01,  2.2775e-01,  1.3199e-01,  5.1484e-01,  4.0411e-02],\n",
            "          [ 9.4904e-02,  4.2775e-01,  5.5536e-01,  7.3002e-02,  6.5206e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4274]]],\n",
            "\n",
            "\n",
            "        [[[4.7017]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4274, 4.7017], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 1] = 0.42738819122314453\n",
            " somado na saída em [0, 1, 0, 1] = 4.701666355133057\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 2\n",
            " alvo do kernel em x: x[0,0,0:5, 6:11]\n",
            " \n",
            " tensor([[0.3841, 0.6811, 0.4741, 0.7135, 0.8706],\n",
            "        [0.2304, 0.2282, 0.6119, 0.8426, 0.2049],\n",
            "        [0.0733, 0.5015, 0.5677, 0.9056, 0.4965],\n",
            "        [0.6655, 0.0695, 0.9716, 0.2725, 0.9571],\n",
            "        [0.1478, 0.1820, 0.1750, 0.7932, 0.8820]])\n",
            " produto: tensor([[[[-3.3134e-03, -9.7244e-05, -3.2402e-04,  2.7297e-03, -1.6423e-02],\n",
            "          [ 3.1075e-03,  2.4195e-03,  1.1109e-02,  2.2748e-03, -1.2791e-03],\n",
            "          [ 1.5213e-03,  2.1392e-02,  1.8382e-02,  9.6934e-03, -3.8588e-03],\n",
            "          [ 4.1156e-02,  5.1127e-03,  8.3759e-02,  1.1736e-02,  1.8828e-02],\n",
            "          [ 1.3575e-02,  2.0608e-02,  1.8647e-02,  4.8354e-02,  1.0638e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.3852e-02, -1.7708e-02,  5.9004e-02,  1.2050e-01,  5.5562e-02],\n",
            "          [ 3.4025e-03,  6.4557e-02,  3.4241e-01,  4.4947e-01,  8.6089e-02],\n",
            "          [ 1.0781e-02,  2.6221e-01,  4.6079e-01,  6.7329e-01,  2.9178e-01],\n",
            "          [ 1.8322e-01,  4.5323e-02,  8.4380e-01,  2.1081e-01,  5.5638e-01],\n",
            "          [ 2.8627e-02,  9.6716e-02,  1.1463e-01,  3.9187e-01,  3.1594e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3197]]],\n",
            "\n",
            "\n",
            "        [[[5.5956]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3197, 5.5956], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 2] = 0.3197489380836487\n",
            " somado na saída em [0, 1, 0, 2] = 5.595590591430664\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 3\n",
            " alvo do kernel em x: x[0,0,0:5, 9:14]\n",
            " \n",
            " tensor([[0.7135, 0.8706, 0.6737, 0.6623, 0.3379],\n",
            "        [0.8426, 0.2049, 0.7000, 0.7572, 0.9754],\n",
            "        [0.9056, 0.4965, 0.5812, 0.4520, 0.8654],\n",
            "        [0.2725, 0.9571, 0.1986, 0.0765, 0.5045],\n",
            "        [0.7932, 0.8820, 0.2869, 0.7848, 0.2599]])\n",
            " produto: tensor([[[[-6.1551e-03, -1.2430e-04, -4.6044e-04,  2.5340e-03, -6.3744e-03],\n",
            "          [ 1.1365e-02,  2.1726e-03,  1.2708e-02,  2.0440e-03, -6.0881e-03],\n",
            "          [ 1.8803e-02,  2.1178e-02,  1.8820e-02,  4.8384e-03, -6.7257e-03],\n",
            "          [ 1.6852e-02,  7.0392e-02,  1.7125e-02,  3.2957e-03,  9.9240e-03],\n",
            "          [ 7.2873e-02,  9.9851e-02,  3.0578e-02,  4.7840e-02,  3.1352e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0004e-01, -2.2636e-02,  8.3846e-02,  1.1186e-01,  2.1566e-02],\n",
            "          [ 1.2444e-02,  5.7968e-02,  3.9170e-01,  4.0387e-01,  4.0975e-01],\n",
            "          [ 1.3325e-01,  2.5958e-01,  4.7177e-01,  3.3607e-01,  5.0855e-01],\n",
            "          [ 7.5022e-02,  6.2402e-01,  1.7252e-01,  5.9198e-02,  2.9326e-01],\n",
            "          [ 1.5367e-01,  4.6861e-01,  1.8796e-01,  3.8772e-01,  9.3108e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4404]]],\n",
            "\n",
            "\n",
            "        [[[5.5946]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4404, 5.5946], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 3] = 0.44040167331695557\n",
            " somado na saída em [0, 1, 0, 3] = 5.594639778137207\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 4\n",
            " alvo do kernel em x: x[0,0,0:5, 12:17]\n",
            " \n",
            " tensor([[6.6233e-01, 3.3792e-01, 5.4206e-01, 8.0480e-01, 2.2099e-01],\n",
            "        [7.5716e-01, 9.7544e-01, 4.1438e-01, 8.9669e-04, 9.2427e-01],\n",
            "        [4.5205e-01, 8.6539e-01, 5.2787e-02, 3.7608e-01, 3.8904e-01],\n",
            "        [7.6521e-02, 5.0449e-01, 9.1816e-02, 5.6808e-01, 5.7069e-02],\n",
            "        [7.8481e-01, 2.5994e-01, 5.5020e-01, 9.6584e-01, 3.7116e-01]])\n",
            " produto: tensor([[[[-5.7139e-03, -4.8248e-05, -3.7050e-04,  3.0791e-03, -4.1686e-03],\n",
            "          [ 1.0212e-02,  1.0340e-02,  7.5229e-03,  2.4207e-06, -5.7688e-03],\n",
            "          [ 9.3854e-03,  3.6912e-02,  1.7093e-03,  4.0252e-03, -3.0236e-03],\n",
            "          [ 4.7322e-03,  3.7103e-02,  7.9154e-03,  2.4466e-02,  1.1226e-03],\n",
            "          [ 7.2100e-02,  2.9427e-02,  5.8642e-02,  5.8876e-02,  4.4766e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.2867e-02, -8.7859e-03,  6.7467e-02,  1.3593e-01,  1.4103e-02],\n",
            "          [ 1.1182e-02,  2.7590e-01,  2.3188e-01,  4.7830e-04,  3.8825e-01],\n",
            "          [ 6.6512e-02,  4.5244e-01,  4.2846e-02,  2.7959e-01,  2.2862e-01],\n",
            "          [ 2.1068e-02,  3.2891e-01,  7.9741e-02,  4.3948e-01,  3.3174e-02],\n",
            "          [ 1.5204e-01,  1.3810e-01,  3.6047e-01,  4.7715e-01,  1.3295e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3630]]],\n",
            "\n",
            "\n",
            "        [[[4.2566]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3630, 4.2566], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 4] = 0.36295565962791443\n",
            " somado na saída em [0, 1, 0, 4] = 4.256614685058594\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 5\n",
            " alvo do kernel em x: x[0,0,0:5, 15:20]\n",
            " \n",
            " tensor([[8.0480e-01, 2.2099e-01, 9.1605e-01, 7.7513e-01, 1.4379e-01],\n",
            "        [8.9669e-04, 9.2427e-01, 3.1048e-01, 2.8257e-01, 6.3205e-01],\n",
            "        [3.7608e-01, 3.8904e-01, 3.4412e-01, 9.7020e-01, 5.1305e-01],\n",
            "        [5.6808e-01, 5.7069e-02, 3.7558e-01, 6.3647e-01, 2.0885e-01],\n",
            "        [9.6584e-01, 3.7116e-01, 6.8759e-01, 1.8011e-01, 1.1930e-01]])\n",
            " produto: tensor([[[[-6.9431e-03, -3.1552e-05, -6.2611e-04,  2.9656e-03, -2.7124e-03],\n",
            "          [ 1.2094e-05,  9.7980e-03,  5.6367e-03,  7.6284e-04, -3.9449e-03],\n",
            "          [ 7.8081e-03,  1.6594e-02,  1.1143e-02,  1.0384e-02, -3.9874e-03],\n",
            "          [ 3.5131e-02,  4.1971e-03,  3.2379e-02,  2.7412e-02,  4.1084e-03],\n",
            "          [ 8.8731e-02,  4.2017e-02,  7.3286e-02,  1.0979e-02,  1.4389e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1284e-01, -5.7456e-03,  1.1401e-01,  1.3092e-01,  9.1764e-03],\n",
            "          [ 1.3242e-05,  2.6143e-01,  1.7374e-01,  1.5073e-01,  2.6550e-01],\n",
            "          [ 5.5334e-02,  2.0339e-01,  2.7931e-01,  7.2129e-01,  3.0150e-01],\n",
            "          [ 1.5640e-01,  3.7207e-02,  3.2619e-01,  4.9239e-01,  1.2141e-01],\n",
            "          [ 1.8711e-01,  1.9719e-01,  4.5049e-01,  8.8980e-02,  4.2731e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3665]]],\n",
            "\n",
            "\n",
            "        [[[4.6478]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3665, 4.6478], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 5] = 0.36653876304626465\n",
            " somado na saída em [0, 1, 0, 5] = 4.647846221923828\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 6\n",
            " alvo do kernel em x: x[0,0,0:5, 18:23]\n",
            " \n",
            " tensor([[0.7751, 0.1438, 0.0989, 0.8933, 0.3573],\n",
            "        [0.2826, 0.6321, 0.9380, 0.3903, 0.1758],\n",
            "        [0.9702, 0.5131, 0.6100, 0.2873, 0.7170],\n",
            "        [0.6365, 0.2089, 0.2934, 0.8697, 0.4545],\n",
            "        [0.1801, 0.1193, 0.8315, 0.1945, 0.2627]])\n",
            " produto: tensor([[[[-6.6871e-03, -2.0530e-05, -6.7611e-05,  3.4176e-03, -6.7390e-03],\n",
            "          [ 3.8112e-03,  6.7003e-03,  1.7028e-02,  1.0536e-03, -1.0975e-03],\n",
            "          [ 2.0143e-02,  2.1884e-02,  1.9753e-02,  3.0753e-03, -5.5725e-03],\n",
            "          [ 3.9361e-02,  1.5360e-02,  2.5293e-02,  3.7456e-02,  8.9398e-03],\n",
            "          [ 1.6547e-02,  1.3505e-02,  8.8626e-02,  1.1854e-02,  3.1687e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0868e-01, -3.7385e-03,  1.2312e-02,  1.5087e-01,  2.2799e-02],\n",
            "          [ 4.1730e-03,  1.7878e-01,  5.2486e-01,  2.0818e-01,  7.3868e-02],\n",
            "          [ 1.4275e-01,  2.6823e-01,  4.9514e-01,  2.1361e-01,  4.2136e-01],\n",
            "          [ 1.7523e-01,  1.3617e-01,  2.5481e-01,  6.7281e-01,  2.6418e-01],\n",
            "          [ 3.4892e-02,  6.3381e-02,  5.4479e-01,  9.6069e-02,  9.4103e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3368]]],\n",
            "\n",
            "\n",
            "        [[[4.9409]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3368, 4.9409], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 6] = 0.3367930054664612\n",
            " somado na saída em [0, 1, 0, 6] = 4.940924167633057\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 7\n",
            " alvo do kernel em x: x[0,0,0:5, 21:26]\n",
            " \n",
            " tensor([[0.8933, 0.3573, 0.5969, 0.0264, 0.0925],\n",
            "        [0.3903, 0.1758, 0.2558, 0.2239, 0.0447],\n",
            "        [0.2873, 0.7170, 0.7123, 0.2849, 0.8798],\n",
            "        [0.8697, 0.4545, 0.5547, 0.0938, 0.0179],\n",
            "        [0.1945, 0.2627, 0.8937, 0.8583, 0.7419]])\n",
            " produto: tensor([[[[-7.7064e-03, -5.1008e-05, -4.0798e-04,  1.0118e-04, -1.7455e-03],\n",
            "          [ 5.2639e-03,  1.8642e-03,  4.6445e-03,  6.0436e-04, -2.7908e-04],\n",
            "          [ 5.9654e-03,  3.0583e-02,  2.3066e-02,  3.0491e-03, -6.8376e-03],\n",
            "          [ 5.3784e-02,  3.3423e-02,  4.7818e-02,  4.0403e-03,  3.5265e-04],\n",
            "          [ 1.7865e-02,  2.9741e-02,  9.5254e-02,  5.2322e-02,  8.9487e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2525e-01, -9.2885e-03,  7.4293e-02,  4.4667e-03,  5.9054e-03],\n",
            "          [ 5.7636e-03,  4.9739e-02,  1.4316e-01,  1.1941e-01,  1.8783e-02],\n",
            "          [ 4.2275e-02,  3.7486e-01,  5.7819e-01,  2.1179e-01,  5.1701e-01],\n",
            "          [ 2.3944e-01,  2.9629e-01,  4.8172e-01,  7.2574e-02,  1.0421e-02],\n",
            "          [ 3.7672e-02,  1.3958e-01,  5.8553e-01,  4.2403e-01,  2.6576e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4017]]],\n",
            "\n",
            "\n",
            "        [[[4.5641]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4017, 4.5641], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 7] = 0.4016609787940979\n",
            " somado na saída em [0, 1, 0, 7] = 4.564123630523682\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 0\n",
            " alvo do kernel em x: x[0,0,3:8, 0:5]\n",
            " \n",
            " tensor([[0.9847, 0.5138, 0.3078, 0.7688, 0.3493],\n",
            "        [0.8739, 0.6939, 0.3555, 0.4899, 0.8051],\n",
            "        [0.4922, 0.9312, 0.6702, 0.8348, 0.8135],\n",
            "        [0.7966, 0.1033, 0.1201, 0.2556, 0.8644],\n",
            "        [0.8722, 0.2762, 0.5123, 0.1451, 0.8481]])\n",
            " produto: tensor([[[[-8.4947e-03, -7.3366e-05, -2.1041e-04,  2.9415e-03, -6.5897e-03],\n",
            "          [ 1.1786e-02,  7.3554e-03,  6.4545e-03,  1.3225e-03, -5.0251e-03],\n",
            "          [ 1.0219e-02,  3.9719e-02,  2.1701e-02,  8.9351e-03, -6.3223e-03],\n",
            "          [ 4.9263e-02,  7.5968e-03,  1.0351e-02,  1.1010e-02,  1.7003e-02],\n",
            "          [ 8.0129e-02,  3.1262e-02,  5.4599e-02,  8.8474e-03,  1.0229e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.3806e-01, -1.3360e-02,  3.8315e-02,  1.2985e-01,  2.2294e-02],\n",
            "          [ 1.2905e-02,  1.9626e-01,  1.9895e-01,  2.6131e-01,  3.3820e-01],\n",
            "          [ 7.2421e-02,  4.8684e-01,  5.4397e-01,  6.2062e-01,  4.7805e-01],\n",
            "          [ 2.1932e-01,  6.7345e-02,  1.0427e-01,  1.9777e-01,  5.0245e-01],\n",
            "          [ 1.6897e-01,  1.4671e-01,  3.3562e-01,  7.1702e-02,  3.0377e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3640]]],\n",
            "\n",
            "\n",
            "        [[[5.3665]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3640, 5.3665], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 0] = 0.36400797963142395\n",
            " somado na saída em [0, 1, 1, 0] = 5.366488456726074\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 1\n",
            " alvo do kernel em x: x[0,0,3:8, 3:8]\n",
            " \n",
            " tensor([[0.7688, 0.3493, 0.1520, 0.6655, 0.0695],\n",
            "        [0.4899, 0.8051, 0.8477, 0.1478, 0.1820],\n",
            "        [0.8348, 0.8135, 0.2612, 0.8773, 0.2141],\n",
            "        [0.2556, 0.8644, 0.8939, 0.2059, 0.7635],\n",
            "        [0.1451, 0.8481, 0.1040, 0.4666, 0.5438]])\n",
            " produto: tensor([[[[-6.6327e-03, -4.9877e-05, -1.0387e-04,  2.5461e-03, -1.3113e-03],\n",
            "          [ 6.6074e-03,  8.5350e-03,  1.5389e-02,  3.9892e-04, -1.1362e-03],\n",
            "          [ 1.7332e-02,  3.4698e-02,  8.4565e-03,  9.3905e-03, -1.6643e-03],\n",
            "          [ 1.5810e-02,  6.3569e-02,  7.7059e-02,  8.8660e-03,  1.5020e-02],\n",
            "          [ 1.3334e-02,  9.6004e-02,  1.1083e-02,  2.8444e-02,  6.5593e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0780e-01, -9.0826e-03,  1.8915e-02,  1.1240e-01,  4.4365e-03],\n",
            "          [ 7.2347e-03,  2.2773e-01,  4.7433e-01,  7.8821e-02,  7.6470e-02],\n",
            "          [ 1.2283e-01,  4.2530e-01,  2.1198e-01,  6.5225e-01,  1.2584e-01],\n",
            "          [ 7.0385e-02,  5.6353e-01,  7.7630e-01,  1.5926e-01,  4.4384e-01],\n",
            "          [ 2.8117e-02,  4.5056e-01,  6.8128e-02,  2.3052e-01,  1.9480e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4282]]],\n",
            "\n",
            "\n",
            "        [[[5.4071]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4282, 5.4071], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 1] = 0.4282034635543823\n",
            " somado na saída em [0, 1, 1, 1] = 5.407087326049805\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 2\n",
            " alvo do kernel em x: x[0,0,3:8, 6:11]\n",
            " \n",
            " tensor([[0.6655, 0.0695, 0.9716, 0.2725, 0.9571],\n",
            "        [0.1478, 0.1820, 0.1750, 0.7932, 0.8820],\n",
            "        [0.8773, 0.2141, 0.0156, 0.7124, 0.0082],\n",
            "        [0.2059, 0.7635, 0.8109, 0.6228, 0.1714],\n",
            "        [0.4666, 0.5438, 0.3334, 0.9216, 0.1280]])\n",
            " produto: tensor([[[[-5.7412e-03, -9.9256e-06, -6.6407e-04,  1.0425e-03, -1.8055e-02],\n",
            "          [ 1.9930e-03,  1.9298e-03,  3.1763e-03,  2.1414e-03, -5.5051e-03],\n",
            "          [ 1.8215e-02,  9.1340e-03,  5.0469e-04,  7.6246e-03, -6.3869e-05],\n",
            "          [ 1.2731e-02,  5.6154e-02,  6.9904e-02,  2.6825e-02,  3.3718e-03],\n",
            "          [ 4.2868e-02,  6.1565e-02,  3.5530e-02,  5.6178e-02,  1.5441e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.3310e-02, -1.8074e-03,  1.2093e-01,  4.6022e-02,  6.1083e-02],\n",
            "          [ 2.1822e-03,  5.1491e-02,  9.7902e-02,  4.2311e-01,  3.7051e-01],\n",
            "          [ 1.2909e-01,  1.1196e-01,  1.2651e-02,  5.2960e-01,  4.8294e-03],\n",
            "          [ 5.6677e-02,  4.9780e-01,  7.0422e-01,  4.8184e-01,  9.9640e-02],\n",
            "          [ 9.0396e-02,  2.8893e-01,  2.1840e-01,  4.5529e-01,  4.5858e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3824]]],\n",
            "\n",
            "\n",
            "        [[[4.8053]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3824, 4.8053], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 2] = 0.3823935091495514\n",
            " somado na saída em [0, 1, 1, 2] = 4.80528450012207\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 3\n",
            " alvo do kernel em x: x[0,0,3:8, 9:14]\n",
            " \n",
            " tensor([[0.2725, 0.9571, 0.1986, 0.0765, 0.5045],\n",
            "        [0.7932, 0.8820, 0.2869, 0.7848, 0.2599],\n",
            "        [0.7124, 0.0082, 0.6638, 0.5381, 0.2658],\n",
            "        [0.6228, 0.1714, 0.3207, 0.6018, 0.7032],\n",
            "        [0.9216, 0.1280, 0.8467, 0.6675, 0.5928]])\n",
            " produto: tensor([[[[-2.3508e-03, -1.3666e-04, -1.3577e-04,  2.9276e-04, -9.5164e-03],\n",
            "          [ 1.0699e-02,  9.3503e-03,  5.2085e-03,  2.1187e-03, -1.6224e-03],\n",
            "          [ 1.4790e-02,  3.5053e-04,  2.1495e-02,  5.7589e-03, -2.0654e-03],\n",
            "          [ 3.8518e-02,  1.2606e-02,  2.7644e-02,  2.5918e-02,  1.3832e-02],\n",
            "          [ 8.4665e-02,  1.4493e-02,  9.0244e-02,  4.0692e-02,  7.1500e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.8207e-02, -2.4885e-02,  2.4724e-02,  1.2924e-02,  3.2196e-02],\n",
            "          [ 1.1714e-02,  2.4948e-01,  1.6054e-01,  4.1862e-01,  1.0919e-01],\n",
            "          [ 1.0481e-01,  4.2965e-03,  5.3883e-01,  4.0001e-01,  1.5617e-01],\n",
            "          [ 1.7148e-01,  1.1175e-01,  2.7849e-01,  4.6555e-01,  4.0876e-01],\n",
            "          [ 1.7853e-01,  6.8018e-02,  5.5473e-01,  3.2978e-01,  2.1234e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4100]]],\n",
            "\n",
            "\n",
            "        [[[4.9399]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4100, 4.9399], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 3] = 0.40999892354011536\n",
            " somado na saída em [0, 1, 1, 3] = 4.939853668212891\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 4\n",
            " alvo do kernel em x: x[0,0,3:8, 12:17]\n",
            " \n",
            " tensor([[0.0765, 0.5045, 0.0918, 0.5681, 0.0571],\n",
            "        [0.7848, 0.2599, 0.5502, 0.9658, 0.3712],\n",
            "        [0.5381, 0.2658, 0.5184, 0.3479, 0.9851],\n",
            "        [0.6018, 0.7032, 0.9989, 0.4274, 0.5112],\n",
            "        [0.6675, 0.5928, 0.6232, 0.5211, 0.4997]])\n",
            " produto: tensor([[[[-6.6015e-04, -7.2030e-05, -6.2756e-05,  2.1734e-03, -1.0765e-03],\n",
            "          [ 1.0585e-02,  2.7556e-03,  9.9887e-03,  2.6074e-03, -2.3166e-03],\n",
            "          [ 1.1171e-02,  1.1336e-02,  1.6785e-02,  3.7236e-03, -7.6564e-03],\n",
            "          [ 3.7215e-02,  5.1715e-02,  8.6114e-02,  1.8408e-02,  1.0057e-02],\n",
            "          [ 6.1326e-02,  6.7110e-02,  6.6428e-02,  3.1763e-02,  6.0270e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0729e-02, -1.3117e-02,  1.1428e-02,  9.5945e-02,  3.6421e-03],\n",
            "          [ 1.1590e-02,  7.3524e-02,  3.0788e-01,  5.1518e-01,  1.5591e-01],\n",
            "          [ 7.9166e-02,  1.3894e-01,  4.2075e-01,  2.5864e-01,  5.7892e-01],\n",
            "          [ 1.6568e-01,  4.5845e-01,  8.6752e-01,  3.3066e-01,  2.9718e-01],\n",
            "          [ 1.2932e-01,  3.1495e-01,  4.0833e-01,  2.5742e-01,  1.7899e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4954]]],\n",
            "\n",
            "\n",
            "        [[[6.0362]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4954, 6.0362], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 4] = 0.49544358253479004\n",
            " somado na saída em [0, 1, 1, 4] = 6.036169528961182\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 5\n",
            " alvo do kernel em x: x[0,0,3:8, 15:20]\n",
            " \n",
            " tensor([[0.5681, 0.0571, 0.3756, 0.6365, 0.2089],\n",
            "        [0.9658, 0.3712, 0.6876, 0.1801, 0.1193],\n",
            "        [0.3479, 0.9851, 0.2056, 0.2787, 0.0114],\n",
            "        [0.4274, 0.5112, 0.3364, 0.8574, 0.0128],\n",
            "        [0.5211, 0.4997, 0.1157, 0.1025, 0.8061]])\n",
            " produto: tensor([[[[-4.9008e-03, -8.1481e-06, -2.5671e-04,  2.4351e-03, -3.9397e-03],\n",
            "          [ 1.3027e-02,  3.9346e-03,  1.2483e-02,  4.8623e-04, -7.4459e-04],\n",
            "          [ 7.2230e-03,  4.2020e-02,  6.6570e-03,  2.9827e-03, -8.8771e-05],\n",
            "          [ 2.6433e-02,  3.7598e-02,  2.9004e-02,  3.6926e-02,  2.5127e-04],\n",
            "          [ 4.7869e-02,  5.6569e-02,  1.2330e-02,  6.2497e-03,  9.7223e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.9651e-02, -1.4838e-03,  4.6746e-02,  1.0750e-01,  1.3329e-02],\n",
            "          [ 1.4263e-02,  1.0498e-01,  3.8476e-01,  9.6073e-02,  5.0112e-02],\n",
            "          [ 5.1187e-02,  5.1504e-01,  1.6687e-01,  2.0718e-01,  6.7123e-03],\n",
            "          [ 1.1768e-01,  3.3330e-01,  2.9219e-01,  6.6328e-01,  7.4251e-03],\n",
            "          [ 1.0094e-01,  2.6548e-01,  7.5795e-02,  5.0650e-02,  2.8873e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3443]]],\n",
            "\n",
            "\n",
            "        [[[3.8791]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3443, 3.8791], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 5] = 0.3442624509334564\n",
            " somado na saída em [0, 1, 1, 5] = 3.8790977001190186\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 6\n",
            " alvo do kernel em x: x[0,0,3:8, 18:23]\n",
            " \n",
            " tensor([[0.6365, 0.2089, 0.2934, 0.8697, 0.4545],\n",
            "        [0.1801, 0.1193, 0.8315, 0.1945, 0.2627],\n",
            "        [0.2787, 0.0114, 0.2991, 0.0275, 0.8117],\n",
            "        [0.8574, 0.0128, 0.8265, 0.2077, 0.4808],\n",
            "        [0.1025, 0.8061, 0.6084, 0.6593, 0.2038]])\n",
            " produto: tensor([[[[-5.4909e-03, -2.9820e-05, -2.0053e-04,  3.3273e-03, -8.5726e-03],\n",
            "          [ 2.4293e-03,  1.2647e-03,  1.5096e-02,  5.2497e-04, -1.6397e-03],\n",
            "          [ 5.7859e-03,  4.8720e-04,  9.6859e-03,  2.9400e-04, -6.3081e-03],\n",
            "          [ 5.3022e-02,  9.3940e-04,  7.1252e-02,  8.9473e-03,  9.4577e-03],\n",
            "          [ 9.4190e-03,  9.1253e-02,  6.4850e-02,  4.0191e-02,  2.4585e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.9241e-02, -5.4301e-03,  3.6516e-02,  1.4689e-01,  2.9003e-02],\n",
            "          [ 2.6599e-03,  3.3743e-02,  4.6530e-01,  1.0373e-01,  1.1036e-01],\n",
            "          [ 4.1003e-02,  5.9716e-03,  2.4280e-01,  2.0421e-02,  4.7698e-01],\n",
            "          [ 2.3605e-01,  8.3277e-03,  7.1780e-01,  1.6072e-01,  2.7948e-01],\n",
            "          [ 1.9862e-02,  4.2826e-01,  3.9863e-01,  3.2572e-01,  7.3011e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3684]]],\n",
            "\n",
            "\n",
            "        [[[4.2685]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3684, 4.2685], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 6] = 0.36844268441200256\n",
            " somado na saída em [0, 1, 1, 6] = 4.268548965454102\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 7\n",
            " alvo do kernel em x: x[0,0,3:8, 21:26]\n",
            " \n",
            " tensor([[0.8697, 0.4545, 0.5547, 0.0938, 0.0179],\n",
            "        [0.1945, 0.2627, 0.8937, 0.8583, 0.7419],\n",
            "        [0.0275, 0.8117, 0.7771, 0.9301, 0.0492],\n",
            "        [0.2077, 0.4808, 0.5968, 0.8936, 0.3434],\n",
            "        [0.6593, 0.2038, 0.4914, 0.4948, 0.1476]])\n",
            " produto: tensor([[[[-7.5028e-03, -6.4886e-05, -3.7911e-04,  3.5891e-04, -3.3817e-04],\n",
            "          [ 2.6228e-03,  2.7850e-03,  1.6225e-02,  2.3171e-03, -4.6308e-03],\n",
            "          [ 5.7029e-04,  3.4620e-02,  2.5163e-02,  9.9552e-03, -3.8222e-04],\n",
            "          [ 1.2847e-02,  3.5359e-02,  5.1449e-02,  3.8488e-02,  6.7551e-03],\n",
            "          [ 6.0572e-02,  2.3075e-02,  5.2380e-02,  3.0160e-02,  1.7802e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2194e-01, -1.1816e-02,  6.9036e-02,  1.5844e-02,  1.1441e-03],\n",
            "          [ 2.8718e-03,  7.4309e-02,  5.0010e-01,  4.5784e-01,  3.1166e-01],\n",
            "          [ 4.0415e-03,  4.2435e-01,  6.3077e-01,  6.9148e-01,  2.8901e-02],\n",
            "          [ 5.7196e-02,  3.1345e-01,  5.1831e-01,  6.9134e-01,  1.9962e-01],\n",
            "          [ 1.2773e-01,  1.0829e-01,  3.2198e-01,  2.4443e-01,  5.2869e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3942]]],\n",
            "\n",
            "\n",
            "        [[[5.7138]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3942, 5.7138], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 7] = 0.39418691396713257\n",
            " somado na saída em [0, 1, 1, 7] = 5.7137956619262695\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 0\n",
            " alvo do kernel em x: x[0,0,6:11, 0:5]\n",
            " \n",
            " tensor([[0.7966, 0.1033, 0.1201, 0.2556, 0.8644],\n",
            "        [0.8722, 0.2762, 0.5123, 0.1451, 0.8481],\n",
            "        [0.1478, 0.6999, 0.1151, 0.0479, 0.0795],\n",
            "        [0.9550, 0.5871, 0.3118, 0.9105, 0.2747],\n",
            "        [0.5441, 0.0409, 0.8966, 0.1534, 0.6440]])\n",
            " produto: tensor([[[[-6.8721e-03, -1.4748e-05, -8.2063e-05,  9.7808e-04, -1.6305e-02],\n",
            "          [ 1.1764e-02,  2.9274e-03,  9.3001e-03,  3.9182e-04, -5.2931e-03],\n",
            "          [ 3.0691e-03,  2.9854e-02,  3.7254e-03,  5.1295e-04, -6.1786e-04],\n",
            "          [ 5.9059e-02,  4.3178e-02,  2.6880e-02,  3.9214e-02,  5.4032e-03],\n",
            "          [ 4.9985e-02,  4.6305e-03,  9.5561e-02,  9.3522e-03,  7.7674e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1169e-01, -2.6856e-03,  1.4943e-02,  4.3177e-02,  5.5162e-02],\n",
            "          [ 1.2881e-02,  7.8109e-02,  2.8665e-01,  7.7418e-02,  3.5624e-01],\n",
            "          [ 2.1750e-02,  3.6592e-01,  9.3384e-02,  3.5629e-02,  4.6718e-02],\n",
            "          [ 2.6293e-01,  3.8277e-01,  2.7079e-01,  7.0438e-01,  1.5967e-01],\n",
            "          [ 1.0540e-01,  2.1731e-02,  5.8742e-01,  7.5794e-02,  2.3068e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3744]]],\n",
            "\n",
            "\n",
            "        [[[4.1752]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3744, 4.1752], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 0] = 0.374367892742157\n",
            " somado na saída em [0, 1, 2, 0] = 4.1751580238342285\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 1\n",
            " alvo do kernel em x: x[0,0,6:11, 3:8]\n",
            " \n",
            " tensor([[0.2556, 0.8644, 0.8939, 0.2059, 0.7635],\n",
            "        [0.1451, 0.8481, 0.1040, 0.4666, 0.5438],\n",
            "        [0.0479, 0.0795, 0.5487, 0.3643, 0.4437],\n",
            "        [0.9105, 0.2747, 0.4546, 0.5344, 0.5650],\n",
            "        [0.1534, 0.6440, 0.9341, 0.3017, 0.8421]])\n",
            " produto: tensor([[[[-2.2055e-03, -1.2341e-04, -6.1095e-04,  7.8759e-04, -1.4403e-02],\n",
            "          [ 1.9576e-03,  8.9901e-03,  1.8878e-03,  1.2597e-03, -3.3943e-03],\n",
            "          [ 9.9501e-04,  3.3910e-03,  1.7767e-02,  3.8993e-03, -3.4483e-03],\n",
            "          [ 5.6307e-02,  2.0201e-02,  3.9192e-02,  2.3018e-02,  1.1114e-02],\n",
            "          [ 1.4095e-02,  7.2905e-02,  9.9562e-02,  1.8388e-02,  1.0156e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.5845e-02, -2.2473e-02,  1.1125e-01,  3.4768e-02,  4.8728e-02],\n",
            "          [ 2.1434e-03,  2.3987e-01,  5.8188e-02,  2.4890e-01,  2.2845e-01],\n",
            "          [ 7.0513e-03,  4.1563e-02,  4.4536e-01,  2.7084e-01,  2.6073e-01],\n",
            "          [ 2.5068e-01,  1.7908e-01,  3.9482e-01,  4.1346e-01,  3.2844e-01],\n",
            "          [ 2.9722e-02,  3.4215e-01,  6.1201e-01,  1.4902e-01,  3.0162e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3817]]],\n",
            "\n",
            "\n",
            "        [[[4.9405]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3817, 4.9405], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 1] = 0.38168758153915405\n",
            " somado na saída em [0, 1, 2, 1] = 4.940529823303223\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 2\n",
            " alvo do kernel em x: x[0,0,6:11, 6:11]\n",
            " \n",
            " tensor([[0.2059, 0.7635, 0.8109, 0.6228, 0.1714],\n",
            "        [0.4666, 0.5438, 0.3334, 0.9216, 0.1280],\n",
            "        [0.3643, 0.4437, 0.8622, 0.8054, 0.2351],\n",
            "        [0.5344, 0.5650, 0.6080, 0.8687, 0.4432],\n",
            "        [0.3017, 0.8421, 0.3224, 0.0311, 0.0024]])\n",
            " produto: tensor([[[[-1.7759e-03, -1.0902e-04, -5.5422e-04,  2.3829e-03, -3.2333e-03],\n",
            "          [ 6.2935e-03,  5.7651e-03,  6.0520e-03,  2.4879e-03, -7.9906e-04],\n",
            "          [ 7.5638e-03,  1.8925e-02,  2.7919e-02,  8.6200e-03, -1.8275e-03],\n",
            "          [ 3.3051e-02,  4.1553e-02,  5.2419e-02,  3.7415e-02,  8.7177e-03],\n",
            "          [ 2.7713e-02,  9.5326e-02,  3.4361e-02,  1.8976e-03,  2.9237e-05]]],\n",
            "\n",
            "\n",
            "        [[[-2.8864e-02, -1.9852e-02,  1.0092e-01,  1.0519e-01,  1.0939e-02],\n",
            "          [ 6.8910e-03,  1.5382e-01,  1.8654e-01,  4.9158e-01,  5.3779e-02],\n",
            "          [ 5.3603e-02,  2.3196e-01,  6.9985e-01,  5.9874e-01,  1.3819e-01],\n",
            "          [ 1.4714e-01,  3.6837e-01,  5.2808e-01,  6.7207e-01,  2.5761e-01],\n",
            "          [ 5.8438e-02,  4.4737e-01,  2.1122e-01,  1.5379e-02,  8.6828e-04]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4102]]],\n",
            "\n",
            "\n",
            "        [[[5.4898]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4102, 5.4898], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 2] = 0.41019320487976074\n",
            " somado na saída em [0, 1, 2, 2] = 5.489834308624268\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 3\n",
            " alvo do kernel em x: x[0,0,6:11, 9:14]\n",
            " \n",
            " tensor([[0.6228, 0.1714, 0.3207, 0.6018, 0.7032],\n",
            "        [0.9216, 0.1280, 0.8467, 0.6675, 0.5928],\n",
            "        [0.8054, 0.2351, 0.4916, 0.1120, 0.1679],\n",
            "        [0.8687, 0.4432, 0.3169, 0.6795, 0.0219],\n",
            "        [0.0311, 0.0024, 0.1972, 0.5300, 0.2081]])\n",
            " produto: tensor([[[[-5.3732e-03, -2.4473e-05, -2.1917e-04,  2.3023e-03, -1.3264e-02],\n",
            "          [ 1.2430e-02,  1.3572e-03,  1.5372e-02,  1.8021e-03, -3.7000e-03],\n",
            "          [ 1.6721e-02,  1.0030e-02,  1.5917e-02,  1.1985e-03, -1.3052e-03],\n",
            "          [ 5.3724e-02,  3.2593e-02,  2.7317e-02,  2.9267e-02,  4.3110e-04],\n",
            "          [ 2.8599e-03,  2.7442e-04,  2.1018e-02,  3.2307e-02,  2.5102e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.7330e-02, -4.4566e-03,  3.9910e-02,  1.0164e-01,  4.4876e-02],\n",
            "          [ 1.3610e-02,  3.6212e-02,  4.7379e-01,  3.5607e-01,  2.4902e-01],\n",
            "          [ 1.1850e-01,  1.2294e-01,  3.9898e-01,  8.3247e-02,  9.8693e-02],\n",
            "          [ 2.3918e-01,  2.8893e-01,  2.7520e-01,  5.2571e-01,  1.2739e-02],\n",
            "          [ 6.0307e-03,  1.2879e-03,  1.2920e-01,  2.6183e-01,  7.4547e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2555]]],\n",
            "\n",
            "\n",
            "        [[[3.8603]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2555, 3.8603], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 3] = 0.25554490089416504\n",
            " somado na saída em [0, 1, 2, 3] = 3.860344409942627\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 4\n",
            " alvo do kernel em x: x[0,0,6:11, 12:17]\n",
            " \n",
            " tensor([[0.6018, 0.7032, 0.9989, 0.4274, 0.5112],\n",
            "        [0.6675, 0.5928, 0.6232, 0.5211, 0.4997],\n",
            "        [0.1120, 0.1679, 0.0519, 0.6558, 0.1278],\n",
            "        [0.6795, 0.0219, 0.4782, 0.4995, 0.2134],\n",
            "        [0.5300, 0.2081, 0.4175, 0.1786, 0.7889]])\n",
            " produto: tensor([[[[-5.1915e-03, -1.0040e-04, -6.8273e-04,  1.6353e-03, -9.6435e-03],\n",
            "          [ 9.0034e-03,  6.2843e-03,  1.1315e-02,  1.4066e-03, -3.1189e-03],\n",
            "          [ 2.3248e-03,  7.1635e-03,  1.6791e-03,  7.0188e-03, -9.9332e-04],\n",
            "          [ 4.2024e-02,  1.6117e-03,  4.1229e-02,  2.1511e-02,  4.1977e-03],\n",
            "          [ 4.8690e-02,  2.3560e-02,  4.4496e-02,  1.0887e-02,  9.5156e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.4376e-02, -1.8283e-02,  1.2432e-01,  7.2189e-02,  3.2626e-02],\n",
            "          [ 9.8582e-03,  1.6768e-01,  3.4876e-01,  2.7794e-01,  2.0991e-01],\n",
            "          [ 1.6475e-02,  8.7803e-02,  4.2091e-02,  4.8752e-01,  7.5108e-02],\n",
            "          [ 1.8709e-01,  1.4288e-02,  4.1534e-01,  3.8639e-01,  1.2404e-01],\n",
            "          [ 1.0267e-01,  1.1057e-01,  2.7352e-01,  8.8229e-02,  2.8259e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2758]]],\n",
            "\n",
            "\n",
            "        [[[3.8344]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2758, 3.8344], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 4] = 0.27582281827926636\n",
            " somado na saída em [0, 1, 2, 4] = 3.834355354309082\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 5\n",
            " alvo do kernel em x: x[0,0,6:11, 15:20]\n",
            " \n",
            " tensor([[0.4274, 0.5112, 0.3364, 0.8574, 0.0128],\n",
            "        [0.5211, 0.4997, 0.1157, 0.1025, 0.8061],\n",
            "        [0.6558, 0.1278, 0.8664, 0.4483, 0.0887],\n",
            "        [0.4995, 0.2134, 0.5761, 0.7579, 0.6325],\n",
            "        [0.1786, 0.7889, 0.0215, 0.1110, 0.8070]])\n",
            " produto: tensor([[[[-3.6874e-03, -7.2992e-05, -2.2995e-04,  3.2802e-03, -2.4095e-04],\n",
            "          [ 7.0278e-03,  5.2973e-03,  2.1003e-03,  2.7678e-04, -5.0311e-03],\n",
            "          [ 1.3615e-02,  5.4515e-03,  2.8056e-02,  4.7983e-03, -6.8965e-04],\n",
            "          [ 3.0888e-02,  1.5694e-02,  4.9666e-02,  3.2642e-02,  1.2443e-02],\n",
            "          [ 1.6407e-02,  8.9313e-02,  2.2876e-03,  6.7654e-03,  9.7335e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.9930e-02, -1.3292e-02,  4.1874e-02,  1.4481e-01,  8.1517e-04],\n",
            "          [ 7.6949e-03,  1.4134e-01,  6.4736e-02,  5.4688e-02,  3.3860e-01],\n",
            "          [ 9.6486e-02,  6.6820e-02,  7.0327e-01,  3.3329e-01,  5.2147e-02],\n",
            "          [ 1.3751e-01,  1.3912e-01,  5.0034e-01,  5.8632e-01,  3.6770e-01],\n",
            "          [ 3.4598e-02,  4.1915e-01,  1.4062e-02,  5.4830e-02,  2.8906e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3258]]],\n",
            "\n",
            "\n",
            "        [[[4.5160]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3258, 4.5160], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 5] = 0.325789213180542\n",
            " somado na saída em [0, 1, 2, 5] = 4.516048431396484\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 6\n",
            " alvo do kernel em x: x[0,0,6:11, 18:23]\n",
            " \n",
            " tensor([[0.8574, 0.0128, 0.8265, 0.2077, 0.4808],\n",
            "        [0.1025, 0.8061, 0.6084, 0.6593, 0.2038],\n",
            "        [0.4483, 0.0887, 0.2720, 0.9001, 0.4307],\n",
            "        [0.7579, 0.6325, 0.1265, 0.3069, 0.5941],\n",
            "        [0.1110, 0.8070, 0.2636, 0.8188, 0.6381]])\n",
            " produto: tensor([[[[-7.3966e-03, -1.8237e-06, -5.6490e-04,  7.9481e-04, -9.0692e-03],\n",
            "          [ 1.3828e-03,  8.5451e-03,  1.1046e-02,  1.7799e-03, -1.2722e-03],\n",
            "          [ 9.3077e-03,  3.7850e-03,  8.8071e-03,  9.6338e-03, -3.3473e-03],\n",
            "          [ 4.6870e-02,  4.6521e-02,  1.0908e-02,  1.3217e-02,  1.1687e-02],\n",
            "          [ 1.0196e-02,  9.1358e-02,  2.8091e-02,  4.9914e-02,  7.6957e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2021e-01, -3.3210e-04,  1.0287e-01,  3.5087e-02,  3.0683e-02],\n",
            "          [ 1.5141e-03,  2.2800e-01,  3.4047e-01,  3.5169e-01,  8.5623e-02],\n",
            "          [ 6.5961e-02,  4.6393e-02,  2.2077e-01,  6.6916e-01,  2.5310e-01],\n",
            "          [ 2.0866e-01,  4.1240e-01,  1.0988e-01,  2.3742e-01,  3.4537e-01],\n",
            "          [ 2.1501e-02,  4.2875e-01,  1.7267e-01,  4.0452e-01,  2.2854e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3499]]],\n",
            "\n",
            "\n",
            "        [[[4.8805]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3499, 4.8805], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 6] = 0.3498873710632324\n",
            " somado na saída em [0, 1, 2, 6] = 4.880490303039551\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 7\n",
            " alvo do kernel em x: x[0,0,6:11, 21:26]\n",
            " \n",
            " tensor([[0.2077, 0.4808, 0.5968, 0.8936, 0.3434],\n",
            "        [0.6593, 0.2038, 0.4914, 0.4948, 0.1476],\n",
            "        [0.9001, 0.4307, 0.1647, 0.2302, 0.7914],\n",
            "        [0.3069, 0.5941, 0.0512, 0.6173, 0.8512],\n",
            "        [0.8188, 0.6381, 0.0239, 0.4454, 0.8700]])\n",
            " produto: tensor([[[[-1.7922e-03, -6.8645e-05, -4.0791e-04,  3.4190e-03, -6.4777e-03],\n",
            "          [ 8.8927e-03,  2.1608e-03,  8.9221e-03,  1.3357e-03, -9.2123e-04],\n",
            "          [ 1.8688e-02,  1.8371e-02,  5.3326e-03,  2.4642e-03, -6.1510e-03],\n",
            "          [ 1.8979e-02,  4.3695e-02,  4.4159e-03,  2.6586e-02,  1.6745e-02],\n",
            "          [ 7.5225e-02,  7.2231e-02,  2.5472e-03,  2.7151e-02,  1.0494e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.9128e-02, -1.2500e-02,  7.4279e-02,  1.5093e-01,  2.1915e-02],\n",
            "          [ 9.7369e-03,  5.7654e-02,  2.7500e-01,  2.6391e-01,  6.2001e-02],\n",
            "          [ 1.3243e-01,  2.2517e-01,  1.3367e-01,  1.7116e-01,  4.6510e-01],\n",
            "          [ 8.4493e-02,  3.8735e-01,  4.4486e-02,  4.7754e-01,  4.9483e-01],\n",
            "          [ 1.5863e-01,  3.3899e-01,  1.5658e-02,  2.2005e-01,  3.1163e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3518]]],\n",
            "\n",
            "\n",
            "        [[[4.5350]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3518, 4.5350], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 2, 7] = 0.3518359363079071\n",
            " somado na saída em [0, 1, 2, 7] = 4.535004138946533\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 0\n",
            " alvo do kernel em x: x[0,0,9:14, 0:5]\n",
            " \n",
            " tensor([[0.9550, 0.5871, 0.3118, 0.9105, 0.2747],\n",
            "        [0.5441, 0.0409, 0.8966, 0.1534, 0.6440],\n",
            "        [0.2672, 0.9027, 0.7153, 0.6619, 0.6170],\n",
            "        [0.0739, 0.6834, 0.4851, 0.7610, 0.0710],\n",
            "        [0.3097, 0.5670, 0.1736, 0.8919, 0.7100]])\n",
            " produto: tensor([[[[-8.2387e-03, -8.3825e-05, -2.1311e-04,  3.4834e-03, -5.1813e-03],\n",
            "          [ 7.3384e-03,  4.3361e-04,  1.6277e-02,  4.1418e-04, -4.0195e-03],\n",
            "          [ 5.5474e-03,  3.8506e-02,  2.3162e-02,  7.0847e-03, -4.7953e-03],\n",
            "          [ 4.5728e-03,  5.0261e-02,  4.1816e-02,  3.2776e-02,  1.3958e-03],\n",
            "          [ 2.8455e-02,  6.4184e-02,  1.8503e-02,  5.4367e-02,  8.5640e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3390e-01, -1.5265e-02,  3.8807e-02,  1.5378e-01,  1.7529e-02],\n",
            "          [ 8.0351e-03,  1.1569e-02,  5.0171e-01,  8.1836e-02,  2.7052e-01],\n",
            "          [ 3.9312e-02,  4.7197e-01,  5.8061e-01,  4.9209e-01,  3.6259e-01],\n",
            "          [ 2.0358e-02,  4.4556e-01,  4.2126e-01,  5.8875e-01,  4.1246e-02],\n",
            "          [ 6.0004e-02,  3.0122e-01,  1.1374e-01,  4.4061e-01,  2.5433e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3846]]],\n",
            "\n",
            "\n",
            "        [[[5.5683]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3846, 5.5683], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 0] = 0.38461145758628845\n",
            " somado na saída em [0, 1, 3, 0] = 5.568274974822998\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 1\n",
            " alvo do kernel em x: x[0,0,9:14, 3:8]\n",
            " \n",
            " tensor([[0.9105, 0.2747, 0.4546, 0.5344, 0.5650],\n",
            "        [0.1534, 0.6440, 0.9341, 0.3017, 0.8421],\n",
            "        [0.6619, 0.6170, 0.7900, 0.6138, 0.2550],\n",
            "        [0.7610, 0.0710, 0.2205, 0.8380, 0.2511],\n",
            "        [0.8919, 0.7100, 0.8548, 0.0563, 0.7446]])\n",
            " produto: tensor([[[[-7.8548e-03, -3.9217e-05, -3.1073e-04,  2.0447e-03, -1.0658e-02],\n",
            "          [ 2.0693e-03,  6.8270e-03,  1.6959e-02,  8.1434e-04, -5.2557e-03],\n",
            "          [ 1.3743e-02,  2.6318e-02,  2.5581e-02,  6.5702e-03, -1.9819e-03],\n",
            "          [ 4.7064e-02,  5.2184e-03,  1.9006e-02,  3.6094e-02,  4.9389e-03],\n",
            "          [ 8.1936e-02,  8.0381e-02,  9.1111e-02,  3.4301e-03,  8.9802e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2766e-01, -7.1415e-03,  5.6583e-02,  9.0264e-02,  3.6058e-02],\n",
            "          [ 2.2657e-03,  1.8216e-01,  5.2272e-01,  1.6090e-01,  3.5372e-01],\n",
            "          [ 9.7390e-02,  3.2258e-01,  6.4123e-01,  4.5636e-01,  1.4986e-01],\n",
            "          [ 2.0953e-01,  4.6260e-02,  1.9147e-01,  6.4833e-01,  1.4595e-01],\n",
            "          [ 1.7278e-01,  3.7723e-01,  5.6006e-01,  2.7799e-02,  2.6669e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4530]]],\n",
            "\n",
            "\n",
            "        [[[5.5834]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4530, 5.5834], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 1] = 0.4529838263988495\n",
            " somado na saída em [0, 1, 3, 1] = 5.583376884460449\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 2\n",
            " alvo do kernel em x: x[0,0,9:14, 6:11]\n",
            " \n",
            " tensor([[0.5344, 0.5650, 0.6080, 0.8687, 0.4432],\n",
            "        [0.3017, 0.8421, 0.3224, 0.0311, 0.0024],\n",
            "        [0.6138, 0.2550, 0.1298, 0.6676, 0.0151],\n",
            "        [0.8380, 0.2511, 0.0179, 0.3581, 0.0824],\n",
            "        [0.0563, 0.7446, 0.4099, 0.9986, 0.4192]])\n",
            " produto: tensor([[[[-4.6106e-03, -8.0670e-05, -4.1560e-04,  3.3237e-03, -8.3597e-03],\n",
            "          [ 4.0685e-03,  8.9266e-03,  5.8528e-03,  8.4039e-05, -1.5130e-05],\n",
            "          [ 1.2745e-02,  1.0877e-02,  4.2036e-03,  7.1457e-03, -1.1719e-04],\n",
            "          [ 5.1827e-02,  1.8465e-02,  1.5435e-03,  1.5425e-02,  1.6205e-03],\n",
            "          [ 5.1695e-03,  8.4288e-02,  4.3691e-02,  6.0871e-02,  5.0560e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.4935e-02, -1.4690e-02,  7.5679e-02,  1.4672e-01,  2.8282e-02],\n",
            "          [ 4.4548e-03,  2.3818e-01,  1.8040e-01,  1.6605e-02,  1.0183e-03],\n",
            "          [ 9.0318e-02,  1.3332e-01,  1.0537e-01,  4.9633e-01,  8.8610e-03],\n",
            "          [ 2.3073e-01,  1.6369e-01,  1.5550e-02,  2.7706e-01,  4.7886e-02],\n",
            "          [ 1.0901e-02,  3.9557e-01,  2.6857e-01,  4.9332e-01,  1.5015e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3316]]],\n",
            "\n",
            "\n",
            "        [[[3.4894]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3316, 3.4894], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 2] = 0.33158397674560547\n",
            " somado na saída em [0, 1, 3, 2] = 3.4893534183502197\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 3\n",
            " alvo do kernel em x: x[0,0,9:14, 9:14]\n",
            " \n",
            " tensor([[0.8687, 0.4432, 0.3169, 0.6795, 0.0219],\n",
            "        [0.0311, 0.0024, 0.1972, 0.5300, 0.2081],\n",
            "        [0.6676, 0.0151, 0.8216, 0.9763, 0.2183],\n",
            "        [0.3581, 0.0824, 0.1050, 0.6690, 0.0888],\n",
            "        [0.9986, 0.4192, 0.4891, 0.3429, 0.2931]])\n",
            " produto: tensor([[[[-7.4945e-03, -6.3274e-05, -2.1658e-04,  2.5998e-03, -4.1339e-04],\n",
            "          [ 4.1987e-04,  2.5697e-05,  3.5801e-03,  1.4308e-03, -1.2990e-03],\n",
            "          [ 1.3861e-02,  6.4316e-04,  2.6605e-02,  1.0449e-02, -1.6969e-03],\n",
            "          [ 2.2148e-02,  6.0584e-03,  9.0537e-03,  2.8811e-02,  1.7459e-03],\n",
            "          [ 9.1739e-02,  4.7456e-02,  5.2135e-02,  2.0902e-02,  3.5348e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2181e-01, -1.1522e-02,  3.9439e-02,  1.1477e-01,  1.3986e-03],\n",
            "          [ 4.5973e-04,  6.8564e-04,  1.1035e-01,  2.8270e-01,  8.7424e-02],\n",
            "          [ 9.8229e-02,  7.8832e-03,  6.6690e-01,  7.2580e-01,  1.2831e-01],\n",
            "          [ 9.8602e-02,  5.3707e-02,  9.1208e-02,  5.1752e-01,  5.1594e-02],\n",
            "          [ 1.9345e-01,  2.2271e-01,  3.2047e-01,  1.6940e-01,  1.0498e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3320]]],\n",
            "\n",
            "\n",
            "        [[[3.9547]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3320, 3.9547], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 3] = 0.332014262676239\n",
            " somado na saída em [0, 1, 3, 3] = 3.9546642303466797\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 4\n",
            " alvo do kernel em x: x[0,0,9:14, 12:17]\n",
            " \n",
            " tensor([[0.6795, 0.0219, 0.4782, 0.4995, 0.2134],\n",
            "        [0.5300, 0.2081, 0.4175, 0.1786, 0.7889],\n",
            "        [0.9763, 0.2183, 0.7854, 0.1455, 0.5126],\n",
            "        [0.6690, 0.0888, 0.8351, 0.8831, 0.6363],\n",
            "        [0.3429, 0.2931, 0.6008, 0.3828, 0.8155]])\n",
            " produto: tensor([[[[-5.8624e-03, -3.1290e-06, -3.2687e-04,  1.9109e-03, -4.0253e-03],\n",
            "          [ 7.1482e-03,  2.2063e-03,  7.5792e-03,  4.8213e-04, -4.9241e-03],\n",
            "          [ 2.0270e-02,  9.3129e-03,  2.5430e-02,  1.5576e-03, -3.9840e-03],\n",
            "          [ 4.1370e-02,  6.5275e-03,  7.1992e-02,  3.8036e-02,  1.2516e-02],\n",
            "          [ 3.1502e-02,  3.3178e-02,  6.4039e-02,  2.3336e-02,  9.8363e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.5280e-02, -5.6979e-04,  5.9523e-02,  8.4356e-02,  1.3618e-02],\n",
            "          [ 7.8269e-03,  5.8867e-02,  2.3361e-01,  9.5263e-02,  3.3141e-01],\n",
            "          [ 1.4364e-01,  1.1415e-01,  6.3745e-01,  1.0819e-01,  3.0124e-01],\n",
            "          [ 1.8418e-01,  5.7866e-02,  7.2525e-01,  6.8322e-01,  3.6986e-01],\n",
            "          [ 6.6428e-02,  1.5571e-01,  3.9365e-01,  1.8912e-01,  2.9212e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3891]]],\n",
            "\n",
            "\n",
            "        [[[5.2107]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3891, 5.2107], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 4] = 0.3891027867794037\n",
            " somado na saída em [0, 1, 3, 4] = 5.210690021514893\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 5\n",
            " alvo do kernel em x: x[0,0,9:14, 15:20]\n",
            " \n",
            " tensor([[0.4995, 0.2134, 0.5761, 0.7579, 0.6325],\n",
            "        [0.1786, 0.7889, 0.0215, 0.1110, 0.8070],\n",
            "        [0.1455, 0.5126, 0.4858, 0.7786, 0.7620],\n",
            "        [0.8831, 0.6363, 0.2496, 0.7034, 0.4076],\n",
            "        [0.3828, 0.8155, 0.9144, 0.4274, 0.5408]])\n",
            " produto: tensor([[[[-4.3088e-03, -3.0467e-05, -3.9377e-04,  2.8996e-03, -1.1932e-02],\n",
            "          [ 2.4088e-03,  8.3635e-03,  3.8966e-04,  2.9962e-04, -5.0369e-03],\n",
            "          [ 3.0214e-03,  2.1865e-02,  1.5731e-02,  8.3340e-03, -5.9226e-03],\n",
            "          [ 5.4616e-02,  4.6793e-02,  2.1516e-02,  3.0293e-02,  8.0188e-03],\n",
            "          [ 3.5170e-02,  9.2322e-02,  9.7460e-02,  2.6052e-02,  6.5226e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.0030e-02, -5.5481e-03,  7.1704e-02,  1.2800e-01,  4.0369e-02],\n",
            "          [ 2.6374e-03,  2.2315e-01,  1.2010e-02,  5.9200e-02,  3.3899e-01],\n",
            "          [ 2.1411e-02,  2.6800e-01,  3.9432e-01,  5.7887e-01,  4.4782e-01],\n",
            "          [ 2.4315e-01,  4.1482e-01,  2.1676e-01,  5.4414e-01,  2.3696e-01],\n",
            "          [ 7.4163e-02,  4.3328e-01,  5.9909e-01,  2.1113e-01,  1.9371e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4545]]],\n",
            "\n",
            "\n",
            "        [[[5.6781]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4545, 5.6781], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 5] = 0.4544509947299957\n",
            " somado na saída em [0, 1, 3, 5] = 5.678108215332031\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 6\n",
            " alvo do kernel em x: x[0,0,9:14, 18:23]\n",
            " \n",
            " tensor([[0.7579, 0.6325, 0.1265, 0.3069, 0.5941],\n",
            "        [0.1110, 0.8070, 0.2636, 0.8188, 0.6381],\n",
            "        [0.7786, 0.7620, 0.4424, 0.5859, 0.0589],\n",
            "        [0.7034, 0.4076, 0.8871, 0.1050, 0.3756],\n",
            "        [0.4274, 0.5408, 0.9517, 0.6683, 0.8119]])\n",
            " produto: tensor([[[[-6.5384e-03, -9.0314e-05, -8.6478e-05,  1.1741e-03, -1.1207e-02],\n",
            "          [ 1.4969e-03,  8.5550e-03,  4.7848e-03,  2.2105e-03, -3.9824e-03],\n",
            "          [ 1.6166e-02,  3.2504e-02,  1.4325e-02,  6.2714e-03, -4.5751e-04],\n",
            "          [ 4.3498e-02,  2.9980e-02,  7.6475e-02,  4.5230e-03,  7.3889e-03],\n",
            "          [ 3.9262e-02,  6.1221e-02,  1.0144e-01,  4.0736e-02,  9.7921e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0627e-01, -1.6446e-02,  1.5748e-02,  5.1832e-02,  3.7917e-02],\n",
            "          [ 1.6390e-03,  2.2826e-01,  1.4748e-01,  4.3677e-01,  2.6802e-01],\n",
            "          [ 1.1456e-01,  3.9841e-01,  3.5907e-01,  4.3560e-01,  3.4594e-02],\n",
            "          [ 1.9365e-01,  2.6577e-01,  7.7041e-01,  8.1244e-02,  2.1835e-01],\n",
            "          [ 8.2792e-02,  2.8732e-01,  6.2356e-01,  3.3014e-01,  2.9080e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4794]]],\n",
            "\n",
            "\n",
            "        [[[5.5512]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4794, 5.5512], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 6] = 0.47944051027297974\n",
            " somado na saída em [0, 1, 3, 6] = 5.551226615905762\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 7\n",
            " alvo do kernel em x: x[0,0,9:14, 21:26]\n",
            " \n",
            " tensor([[0.3069, 0.5941, 0.0512, 0.6173, 0.8512],\n",
            "        [0.8188, 0.6381, 0.0239, 0.4454, 0.8700],\n",
            "        [0.5859, 0.0589, 0.3614, 0.5573, 0.5615],\n",
            "        [0.1050, 0.3756, 0.4787, 0.1583, 0.8352],\n",
            "        [0.6683, 0.8119, 0.1126, 0.7692, 0.2817]])\n",
            " produto: tensor([[[[-2.6476e-03, -8.4829e-05, -3.5010e-05,  2.3617e-03, -1.6057e-02],\n",
            "          [ 1.1044e-02,  6.7639e-03,  4.3387e-04,  1.2024e-03, -5.4302e-03],\n",
            "          [ 1.2165e-02,  2.5109e-03,  1.1701e-02,  5.9647e-03, -4.3642e-03],\n",
            "          [ 6.4946e-03,  2.7625e-02,  4.1272e-02,  6.8159e-03,  1.6430e-02],\n",
            "          [ 6.1393e-02,  9.1908e-02,  1.2005e-02,  4.6887e-02,  3.3977e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.3030e-02, -1.5447e-02,  6.3753e-03,  1.0426e-01,  5.4326e-02],\n",
            "          [ 1.2092e-02,  1.8047e-01,  1.3373e-02,  2.3759e-01,  3.6547e-01],\n",
            "          [ 8.6211e-02,  3.0777e-02,  2.9331e-01,  4.1430e-01,  3.2999e-01],\n",
            "          [ 2.8914e-02,  2.4489e-01,  4.1578e-01,  1.2243e-01,  4.8552e-01],\n",
            "          [ 1.2946e-01,  4.3133e-01,  7.3795e-02,  3.7999e-01,  1.0090e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3398]]],\n",
            "\n",
            "\n",
            "        [[[4.4831]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3398, 4.4831], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 3, 7] = 0.33975517749786377\n",
            " somado na saída em [0, 1, 3, 7] = 4.483064651489258\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 0\n",
            " alvo do kernel em x: x[0,0,12:17, 0:5]\n",
            " \n",
            " tensor([[0.0739, 0.6834, 0.4851, 0.7610, 0.0710],\n",
            "        [0.3097, 0.5670, 0.1736, 0.8919, 0.7100],\n",
            "        [0.6253, 0.6837, 0.1047, 0.0874, 0.6166],\n",
            "        [0.6112, 0.2342, 0.5989, 0.0454, 0.2555],\n",
            "        [0.3638, 0.7152, 0.0928, 0.4748, 0.2241]])\n",
            " produto: tensor([[[[-6.3790e-04, -9.7576e-05, -3.3153e-04,  2.9116e-03, -1.3385e-03],\n",
            "          [ 4.1776e-03,  6.0103e-03,  3.1518e-03,  2.4077e-03, -4.4317e-03],\n",
            "          [ 1.2982e-02,  2.9164e-02,  3.3900e-03,  9.3549e-04, -4.7924e-03],\n",
            "          [ 3.7801e-02,  1.7226e-02,  5.1627e-02,  1.9564e-03,  5.0251e-03],\n",
            "          [ 3.3419e-02,  8.0961e-02,  9.8888e-03,  2.8944e-02,  2.7029e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0368e-02, -1.7769e-02,  6.0371e-02,  1.2853e-01,  4.5283e-03],\n",
            "          [ 4.5742e-03,  1.6037e-01,  9.7146e-02,  4.7573e-01,  2.9826e-01],\n",
            "          [ 9.1998e-02,  3.5747e-01,  8.4977e-02,  6.4978e-02,  3.6237e-01],\n",
            "          [ 1.6829e-01,  1.5270e-01,  5.2009e-01,  3.5143e-02,  1.4850e-01],\n",
            "          [ 7.0471e-02,  3.7996e-01,  6.0787e-02,  2.3457e-01,  8.0271e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3231]]],\n",
            "\n",
            "\n",
            "        [[[4.0139]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3231, 4.0139], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 0] = 0.3230513036251068\n",
            " somado na saída em [0, 1, 4, 0] = 4.013948917388916\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 1\n",
            " alvo do kernel em x: x[0,0,12:17, 3:8]\n",
            " \n",
            " tensor([[0.7610, 0.0710, 0.2205, 0.8380, 0.2511],\n",
            "        [0.8919, 0.7100, 0.8548, 0.0563, 0.7446],\n",
            "        [0.0874, 0.6166, 0.3788, 0.9029, 0.9887],\n",
            "        [0.0454, 0.2555, 0.1407, 0.0555, 0.8892],\n",
            "        [0.4748, 0.2241, 0.4974, 0.6475, 0.3617]])\n",
            " produto: tensor([[[[-6.5654e-03, -1.0131e-05, -1.5068e-04,  3.2063e-03, -4.7361e-03],\n",
            "          [ 1.2029e-02,  7.5271e-03,  1.5519e-02,  1.5191e-04, -4.6471e-03],\n",
            "          [ 1.8147e-03,  2.6302e-02,  1.2264e-02,  9.6640e-03, -7.6842e-03],\n",
            "          [ 2.8093e-03,  1.8787e-02,  1.2127e-02,  2.3882e-03,  1.7492e-02],\n",
            "          [ 4.3621e-02,  2.5369e-02,  5.3015e-02,  3.9468e-02,  4.3630e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0671e-01, -1.8448e-03,  2.7439e-02,  1.4154e-01,  1.6023e-02],\n",
            "          [ 1.3171e-02,  2.0084e-01,  4.7835e-01,  3.0015e-02,  3.1276e-01],\n",
            "          [ 1.2860e-02,  3.2239e-01,  3.0743e-01,  6.7125e-01,  5.8103e-01],\n",
            "          [ 1.2507e-02,  1.6655e-01,  1.2217e-01,  4.2899e-02,  5.1689e-01],\n",
            "          [ 9.1984e-02,  1.1906e-01,  3.2589e-01,  3.1986e-01,  1.2957e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2841]]],\n",
            "\n",
            "\n",
            "        [[[4.8539]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2841, 4.8539], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 1] = 0.28412559628486633\n",
            " somado na saída em [0, 1, 4, 1] = 4.853912353515625\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 2\n",
            " alvo do kernel em x: x[0,0,12:17, 6:11]\n",
            " \n",
            " tensor([[0.8380, 0.2511, 0.0179, 0.3581, 0.0824],\n",
            "        [0.0563, 0.7446, 0.4099, 0.9986, 0.4192],\n",
            "        [0.9029, 0.9887, 0.4718, 0.7160, 0.5587],\n",
            "        [0.0555, 0.8892, 0.8935, 0.2579, 0.2561],\n",
            "        [0.6475, 0.3617, 0.7963, 0.5100, 0.2081]])\n",
            " produto: tensor([[[[-7.2299e-03, -3.5848e-05, -1.2238e-05,  1.3702e-03, -1.5539e-03],\n",
            "          [ 7.5893e-04,  7.8929e-03,  7.4421e-03,  2.6958e-03, -2.6164e-03],\n",
            "          [ 1.8746e-02,  4.2173e-02,  1.5275e-02,  7.6637e-03, -4.3424e-03],\n",
            "          [ 3.4293e-03,  6.5396e-02,  7.7028e-02,  1.1107e-02,  5.0386e-03],\n",
            "          [ 5.9482e-02,  4.0951e-02,  8.4875e-02,  3.1088e-02,  2.5100e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1750e-01, -6.5278e-03,  2.2284e-03,  6.0487e-02,  5.2572e-03],\n",
            "          [ 8.3099e-04,  2.1060e-01,  2.2939e-01,  5.3265e-01,  1.7609e-01],\n",
            "          [ 1.3285e-01,  5.1691e-01,  3.8291e-01,  5.3231e-01,  3.2834e-01],\n",
            "          [ 1.5267e-02,  5.7973e-01,  7.7599e-01,  1.9950e-01,  1.4889e-01],\n",
            "          [ 1.2543e-01,  1.9219e-01,  5.2173e-01,  2.5195e-01,  7.4542e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4691]]],\n",
            "\n",
            "\n",
            "        [[[5.8720]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4691, 5.8720], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 2] = 0.4691314399242401\n",
            " somado na saída em [0, 1, 4, 2] = 5.8720269203186035\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 3\n",
            " alvo do kernel em x: x[0,0,12:17, 9:14]\n",
            " \n",
            " tensor([[0.3581, 0.0824, 0.1050, 0.6690, 0.0888],\n",
            "        [0.9986, 0.4192, 0.4891, 0.3429, 0.2931],\n",
            "        [0.7160, 0.5587, 0.2320, 0.3562, 0.6693],\n",
            "        [0.2579, 0.2561, 0.0464, 0.6760, 0.7866],\n",
            "        [0.5100, 0.2081, 0.6160, 0.2149, 0.5791]])\n",
            " produto: tensor([[[[-3.0897e-03, -1.1762e-05, -7.1780e-05,  2.5593e-03, -1.6742e-03],\n",
            "          [ 1.3468e-02,  4.4439e-03,  8.8803e-03,  9.2569e-04, -1.8292e-03],\n",
            "          [ 1.4866e-02,  2.3832e-02,  7.5136e-03,  3.8122e-03, -5.2014e-03],\n",
            "          [ 1.5948e-02,  1.8838e-02,  4.0034e-03,  2.9115e-02,  1.5474e-02],\n",
            "          [ 4.6853e-02,  2.3559e-02,  6.5651e-02,  1.3102e-02,  6.9850e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.0215e-02, -2.1418e-03,  1.3071e-02,  1.1298e-01,  5.6643e-03],\n",
            "          [ 1.4747e-02,  1.1857e-01,  2.7371e-01,  1.8290e-01,  1.2311e-01],\n",
            "          [ 1.0535e-01,  2.9211e-01,  1.8834e-01,  2.6479e-01,  3.9329e-01],\n",
            "          [ 7.1000e-02,  1.6699e-01,  4.0331e-02,  5.2298e-01,  4.5727e-01],\n",
            "          [ 9.8799e-02,  1.1056e-01,  4.0356e-01,  1.0618e-01,  2.0744e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3080]]],\n",
            "\n",
            "\n",
            "        [[[4.2214]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3080, 4.2214], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 3] = 0.30795031785964966\n",
            " somado na saída em [0, 1, 4, 3] = 4.2213969230651855\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 4\n",
            " alvo do kernel em x: x[0,0,12:17, 12:17]\n",
            " \n",
            " tensor([[0.6690, 0.0888, 0.8351, 0.8831, 0.6363],\n",
            "        [0.3429, 0.2931, 0.6008, 0.3828, 0.8155],\n",
            "        [0.3562, 0.6693, 0.9959, 0.7231, 0.2063],\n",
            "        [0.6760, 0.7866, 0.2424, 0.8872, 0.8570],\n",
            "        [0.2149, 0.5791, 0.6233, 0.2747, 0.8587]])\n",
            " produto: tensor([[[[-5.7711e-03, -1.2672e-05, -5.7077e-04,  3.3788e-03, -1.2002e-02],\n",
            "          [ 4.6248e-03,  3.1068e-03,  1.0908e-02,  1.0335e-03, -5.0901e-03],\n",
            "          [ 7.3948e-03,  2.8546e-02,  3.2247e-02,  7.7398e-03, -1.6033e-03],\n",
            "          [ 4.1806e-02,  5.7852e-02,  2.0896e-02,  3.8211e-02,  1.6858e-02],\n",
            "          [ 1.9745e-02,  6.5561e-02,  6.6430e-02,  1.6746e-02,  1.0357e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.3795e-02, -2.3076e-03,  1.0394e-01,  1.4916e-01,  4.0605e-02],\n",
            "          [ 5.0639e-03,  8.2896e-02,  3.3621e-01,  2.0420e-01,  3.4257e-01],\n",
            "          [ 5.2404e-02,  3.4989e-01,  8.0832e-01,  5.3760e-01,  1.2123e-01],\n",
            "          [ 1.8612e-01,  5.1285e-01,  2.1050e-01,  6.8636e-01,  4.9816e-01],\n",
            "          [ 4.1637e-02,  3.0768e-01,  4.0835e-01,  1.3572e-01,  3.0757e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4284]]],\n",
            "\n",
            "\n",
            "        [[[6.3329]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4284, 6.3329], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 4] = 0.42839086055755615\n",
            " somado na saída em [0, 1, 4, 4] = 6.332939147949219\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 5\n",
            " alvo do kernel em x: x[0,0,12:17, 15:20]\n",
            " \n",
            " tensor([[0.8831, 0.6363, 0.2496, 0.7034, 0.4076],\n",
            "        [0.3828, 0.8155, 0.9144, 0.4274, 0.5408],\n",
            "        [0.7231, 0.2063, 0.8126, 0.7680, 0.0982],\n",
            "        [0.8872, 0.8570, 0.6919, 0.4315, 0.9336],\n",
            "        [0.2747, 0.8587, 0.2598, 0.6268, 0.6426]])\n",
            " produto: tensor([[[[-7.6189e-03, -9.0843e-05, -1.7059e-04,  2.6910e-03, -7.6894e-03],\n",
            "          [ 5.1633e-03,  8.6453e-03,  1.6601e-02,  1.1537e-03, -3.3753e-03],\n",
            "          [ 1.5014e-02,  8.7992e-03,  2.6312e-02,  8.2198e-03, -7.6288e-04],\n",
            "          [ 5.4867e-02,  6.3025e-02,  5.9645e-02,  1.8585e-02,  1.8366e-02],\n",
            "          [ 2.5238e-02,  9.7207e-02,  2.7695e-02,  3.8209e-02,  7.7508e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2383e-01, -1.6543e-02,  3.1064e-02,  1.1879e-01,  2.6015e-02],\n",
            "          [ 5.6535e-03,  2.3067e-01,  5.1168e-01,  2.2796e-01,  2.2717e-01],\n",
            "          [ 1.0640e-01,  1.0785e-01,  6.5956e-01,  5.7094e-01,  5.7684e-02],\n",
            "          [ 2.4426e-01,  5.5871e-01,  6.0087e-01,  3.3384e-01,  5.4273e-01],\n",
            "          [ 5.3219e-02,  4.5620e-01,  1.7024e-01,  3.0966e-01,  2.3018e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4835]]],\n",
            "\n",
            "\n",
            "        [[[6.2410]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4835, 6.2410], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 5] = 0.4834784269332886\n",
            " somado na saída em [0, 1, 4, 5] = 6.240989685058594\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 6\n",
            " alvo do kernel em x: x[0,0,12:17, 18:23]\n",
            " \n",
            " tensor([[0.7034, 0.4076, 0.8871, 0.1050, 0.3756],\n",
            "        [0.4274, 0.5408, 0.9517, 0.6683, 0.8119],\n",
            "        [0.7680, 0.0982, 0.4418, 0.8854, 0.4912],\n",
            "        [0.4315, 0.9336, 0.3189, 0.4381, 0.4662],\n",
            "        [0.6268, 0.6426, 0.5365, 0.8428, 0.2218]])\n",
            " produto: tensor([[[[-6.0679e-03, -5.8201e-05, -6.0631e-04,  4.0179e-04, -7.0854e-03],\n",
            "          [ 5.7641e-03,  5.7329e-03,  1.7279e-02,  1.8040e-03, -5.0672e-03],\n",
            "          [ 1.5945e-02,  4.1869e-03,  1.4306e-02,  9.4762e-03, -3.8175e-03],\n",
            "          [ 2.6687e-02,  6.8665e-02,  2.7492e-02,  1.8867e-02,  9.1704e-03],\n",
            "          [ 5.7585e-02,  7.2748e-02,  5.7183e-02,  5.1374e-02,  2.6747e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.8620e-02, -1.0598e-02,  1.1041e-01,  1.7737e-02,  2.3971e-02],\n",
            "          [ 6.3114e-03,  1.5296e-01,  5.3258e-01,  3.5645e-01,  3.4104e-01],\n",
            "          [ 1.1300e-01,  5.1319e-02,  3.5861e-01,  6.5820e-01,  2.8866e-01],\n",
            "          [ 1.1881e-01,  6.0871e-01,  2.7696e-01,  3.3889e-01,  2.7099e-01],\n",
            "          [ 1.2143e-01,  3.4141e-01,  3.5151e-01,  4.1636e-01,  7.9433e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4446]]],\n",
            "\n",
            "\n",
            "        [[[5.8265]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4446, 5.8265], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 6] = 0.44463828206062317\n",
            " somado na saída em [0, 1, 4, 6] = 5.826518535614014\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 7\n",
            " alvo do kernel em x: x[0,0,12:17, 21:26]\n",
            " \n",
            " tensor([[0.1050, 0.3756, 0.4787, 0.1583, 0.8352],\n",
            "        [0.6683, 0.8119, 0.1126, 0.7692, 0.2817],\n",
            "        [0.8854, 0.4912, 0.1191, 0.4834, 0.9142],\n",
            "        [0.4381, 0.4662, 0.1572, 0.7759, 0.9364],\n",
            "        [0.8428, 0.2218, 0.2639, 0.2095, 0.9840]])\n",
            " produto: tensor([[[[-9.0599e-04, -5.3630e-05, -3.2722e-04,  6.0547e-04, -1.5755e-02],\n",
            "          [ 9.0131e-03,  8.6065e-03,  2.0449e-03,  2.0764e-03, -1.7582e-03],\n",
            "          [ 1.8382e-02,  2.0952e-02,  3.8557e-03,  5.1739e-03, -7.1050e-03],\n",
            "          [ 2.7091e-02,  3.4285e-02,  1.3552e-02,  3.3417e-02,  1.8421e-02],\n",
            "          [ 7.7426e-02,  2.5104e-02,  2.8129e-02,  1.2769e-02,  1.1868e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4725e-02, -9.7660e-03,  5.9585e-02,  2.6729e-02,  5.3303e-02],\n",
            "          [ 9.8688e-03,  2.2964e-01,  6.3028e-02,  4.1028e-01,  1.1833e-01],\n",
            "          [ 1.3027e-01,  2.5680e-01,  9.6652e-02,  3.5937e-01,  5.3723e-01],\n",
            "          [ 1.2061e-01,  3.0393e-01,  1.3653e-01,  6.0026e-01,  5.4435e-01],\n",
            "          [ 1.6327e-01,  1.1782e-01,  1.7291e-01,  1.0348e-01,  3.5246e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3269]]],\n",
            "\n",
            "\n",
            "        [[[4.9422]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3269, 4.9422], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 4, 7] = 0.32686668634414673\n",
            " somado na saída em [0, 1, 4, 7] = 4.9422173500061035\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 0\n",
            " alvo do kernel em x: x[0,0,15:20, 0:5]\n",
            " \n",
            " tensor([[0.6112, 0.2342, 0.5989, 0.0454, 0.2555],\n",
            "        [0.3638, 0.7152, 0.0928, 0.4748, 0.2241],\n",
            "        [0.7071, 0.8840, 0.6534, 0.1972, 0.6501],\n",
            "        [0.9321, 0.9256, 0.7926, 0.8350, 0.9735],\n",
            "        [0.3373, 0.1107, 0.9173, 0.6569, 0.0609]])\n",
            " produto: tensor([[[[-5.2732e-03, -3.3441e-05, -4.0931e-04,  1.7380e-04, -4.8187e-03],\n",
            "          [ 4.9063e-03,  7.5814e-03,  1.6844e-03,  1.2818e-03, -1.3987e-03],\n",
            "          [ 1.4681e-02,  3.7704e-02,  2.1157e-02,  2.1107e-03, -5.0523e-03],\n",
            "          [ 5.7641e-02,  6.8073e-02,  6.8327e-02,  3.5962e-02,  1.9149e-02],\n",
            "          [ 3.0985e-02,  1.2535e-02,  9.7768e-02,  4.0043e-02,  7.3468e-04]]],\n",
            "\n",
            "\n",
            "        [[[-8.5703e-02, -6.0896e-03,  7.4535e-02,  7.6722e-03,  1.6303e-02],\n",
            "          [ 5.3721e-03,  2.0229e-01,  5.1918e-02,  2.5327e-01,  9.4136e-02],\n",
            "          [ 1.0404e-01,  4.6215e-01,  5.3035e-01,  1.4660e-01,  3.8202e-01],\n",
            "          [ 2.5661e-01,  6.0346e-01,  6.8834e-01,  6.4597e-01,  5.6587e-01],\n",
            "          [ 6.5339e-02,  5.8827e-02,  6.0098e-01,  3.2452e-01,  2.1819e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.5055]]],\n",
            "\n",
            "\n",
            "        [[[6.0706]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.5055, 6.0706], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 0] = 0.5055129528045654\n",
            " somado na saída em [0, 1, 5, 0] = 6.070598125457764\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 1\n",
            " alvo do kernel em x: x[0,0,15:20, 3:8]\n",
            " \n",
            " tensor([[0.0454, 0.2555, 0.1407, 0.0555, 0.8892],\n",
            "        [0.4748, 0.2241, 0.4974, 0.6475, 0.3617],\n",
            "        [0.1972, 0.6501, 0.9511, 0.7254, 0.2179],\n",
            "        [0.8350, 0.9735, 0.8340, 0.3794, 0.5933],\n",
            "        [0.6569, 0.0609, 0.1597, 0.5346, 0.4560]])\n",
            " produto: tensor([[[[-3.9189e-04, -3.6473e-05, -9.6148e-05,  2.1215e-04, -1.6773e-02],\n",
            "          [ 6.4041e-03,  2.3757e-03,  9.0303e-03,  1.7479e-03, -2.2578e-03],\n",
            "          [ 4.0942e-03,  2.7728e-02,  3.0797e-02,  7.7642e-03, -1.6932e-03],\n",
            "          [ 5.1638e-02,  7.1593e-02,  7.1897e-02,  1.6341e-02,  1.1670e-02],\n",
            "          [ 6.0348e-02,  6.8957e-03,  1.7020e-02,  3.2588e-02,  5.5004e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.3693e-03, -6.6417e-03,  1.7508e-02,  9.3655e-03,  5.6748e-02],\n",
            "          [ 7.0120e-03,  6.3387e-02,  2.7834e-01,  3.4536e-01,  1.5195e-01],\n",
            "          [ 2.9014e-02,  3.3987e-01,  7.7198e-01,  5.3929e-01,  1.2803e-01],\n",
            "          [ 2.2989e-01,  6.3466e-01,  7.2430e-01,  2.9353e-01,  3.4487e-01],\n",
            "          [ 1.2726e-01,  3.2362e-02,  1.0462e-01,  2.6411e-01,  1.6335e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4144]]],\n",
            "\n",
            "\n",
            "        [[[5.6438]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4144, 5.6438], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 1] = 0.4143970012664795\n",
            " somado na saída em [0, 1, 5, 1] = 5.643797397613525\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 2\n",
            " alvo do kernel em x: x[0,0,15:20, 6:11]\n",
            " \n",
            " tensor([[0.0555, 0.8892, 0.8935, 0.2579, 0.2561],\n",
            "        [0.6475, 0.3617, 0.7963, 0.5100, 0.2081],\n",
            "        [0.7254, 0.2179, 0.6329, 0.3805, 0.5143],\n",
            "        [0.3794, 0.5933, 0.6574, 0.4123, 0.3551],\n",
            "        [0.5346, 0.4560, 0.5284, 0.2282, 0.5805]])\n",
            " produto: tensor([[[[-4.7838e-04, -1.2696e-04, -6.1070e-04,  9.8663e-04, -4.8316e-03],\n",
            "          [ 8.7327e-03,  3.8347e-03,  1.4457e-02,  1.3768e-03, -1.2989e-03],\n",
            "          [ 1.5061e-02,  9.2927e-03,  2.0492e-02,  4.0728e-03, -3.9971e-03],\n",
            "          [ 2.3465e-02,  4.3632e-02,  5.6676e-02,  1.7758e-02,  6.9848e-03],\n",
            "          [ 4.9113e-02,  5.1626e-02,  5.6316e-02,  1.3909e-02,  7.0011e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.7750e-03, -2.3119e-02,  1.1121e-01,  4.3555e-02,  1.6346e-02],\n",
            "          [ 9.5617e-03,  1.0232e-01,  4.4561e-01,  2.7203e-01,  8.7418e-02],\n",
            "          [ 1.0673e-01,  1.1390e-01,  5.1367e-01,  2.8289e-01,  3.0223e-01],\n",
            "          [ 1.0446e-01,  3.8679e-01,  5.7096e-01,  3.1899e-01,  2.0641e-01],\n",
            "          [ 1.0357e-01,  2.4228e-01,  3.4617e-01,  1.1273e-01,  2.0792e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3934]]],\n",
            "\n",
            "\n",
            "        [[[4.9768]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3934, 4.9768], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 2] = 0.3934432566165924\n",
            " somado na saída em [0, 1, 5, 2] = 4.97684907913208\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 3\n",
            " alvo do kernel em x: x[0,0,15:20, 9:14]\n",
            " \n",
            " tensor([[0.2579, 0.2561, 0.0464, 0.6760, 0.7866],\n",
            "        [0.5100, 0.2081, 0.6160, 0.2149, 0.5791],\n",
            "        [0.3805, 0.5143, 0.4522, 0.5507, 0.0593],\n",
            "        [0.4123, 0.3551, 0.0537, 0.2058, 0.7846],\n",
            "        [0.2282, 0.5805, 0.1810, 0.1015, 0.4929]])\n",
            " produto: tensor([[[[-2.2247e-03, -3.6571e-05, -3.1740e-05,  2.5864e-03, -1.4838e-02],\n",
            "          [ 6.8785e-03,  2.2061e-03,  1.1183e-02,  5.8022e-04, -3.6146e-03],\n",
            "          [ 7.9004e-03,  2.1937e-02,  1.4642e-02,  5.8943e-03, -4.6116e-04],\n",
            "          [ 2.5499e-02,  2.6114e-02,  4.6277e-03,  8.8656e-03,  1.5433e-02],\n",
            "          [ 2.0963e-02,  6.5711e-02,  1.9287e-02,  6.1871e-03,  5.9449e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.6158e-02, -6.6595e-03,  5.7798e-03,  1.1417e-01,  5.0201e-02],\n",
            "          [ 7.5316e-03,  5.8863e-02,  3.4468e-01,  1.1464e-01,  2.4327e-01],\n",
            "          [ 5.5988e-02,  2.6888e-01,  3.6702e-01,  4.0941e-01,  3.4870e-02],\n",
            "          [ 1.1352e-01,  2.3150e-01,  4.6620e-02,  1.5925e-01,  4.5607e-01],\n",
            "          [ 4.4204e-02,  3.0839e-01,  1.1856e-01,  5.0143e-02,  1.7655e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2512]]],\n",
            "\n",
            "\n",
            "        [[[3.7373]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2512, 3.7373], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 3] = 0.25123330950737\n",
            " somado na saída em [0, 1, 5, 3] = 3.7373046875\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 4\n",
            " alvo do kernel em x: x[0,0,15:20, 12:17]\n",
            " \n",
            " tensor([[0.6760, 0.7866, 0.2424, 0.8872, 0.8570],\n",
            "        [0.2149, 0.5791, 0.6233, 0.2747, 0.8587],\n",
            "        [0.5507, 0.0593, 0.6973, 0.8050, 0.5280],\n",
            "        [0.2058, 0.7846, 0.8677, 0.5138, 0.9070],\n",
            "        [0.1015, 0.4929, 0.6992, 0.3420, 0.5117]])\n",
            " produto: tensor([[[[-5.8320e-03, -1.1231e-04, -1.6567e-04,  3.3943e-03, -1.6165e-02],\n",
            "          [ 2.8988e-03,  6.1393e-03,  1.1315e-02,  7.4162e-04, -5.3593e-03],\n",
            "          [ 1.1434e-02,  2.5309e-03,  2.2578e-02,  8.6157e-03, -4.1036e-03],\n",
            "          [ 1.2730e-02,  5.7700e-02,  7.4803e-02,  2.2130e-02,  1.7843e-02],\n",
            "          [ 9.3246e-03,  5.5799e-02,  7.4519e-02,  2.0846e-02,  6.1714e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.4785e-02, -2.0452e-02,  3.0167e-02,  1.4984e-01,  5.4691e-02],\n",
            "          [ 3.1741e-03,  1.6381e-01,  3.4877e-01,  1.4654e-01,  3.6070e-01],\n",
            "          [ 8.1027e-02,  3.1022e-02,  5.6596e-01,  5.9843e-01,  3.1029e-01],\n",
            "          [ 5.6674e-02,  5.1151e-01,  7.5357e-01,  3.9751e-01,  5.2727e-01],\n",
            "          [ 1.9663e-02,  2.6187e-01,  4.5807e-01,  1.6895e-01,  1.8328e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3898]]],\n",
            "\n",
            "\n",
            "        [[[6.0675]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3898, 6.0675], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 4] = 0.3897757828235626\n",
            " somado na saída em [0, 1, 5, 4] = 6.067535877227783\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 5\n",
            " alvo do kernel em x: x[0,0,15:20, 15:20]\n",
            " \n",
            " tensor([[0.8872, 0.8570, 0.6919, 0.4315, 0.9336],\n",
            "        [0.2747, 0.8587, 0.2598, 0.6268, 0.6426],\n",
            "        [0.8050, 0.5280, 0.9690, 0.3240, 0.3179],\n",
            "        [0.5138, 0.9070, 0.4118, 0.7436, 0.6936],\n",
            "        [0.3420, 0.5117, 0.6690, 0.6647, 0.8437]])\n",
            " produto: tensor([[[[-7.6539e-03, -1.2236e-04, -4.7288e-04,  1.6510e-03, -1.7612e-02],\n",
            "          [ 3.7052e-03,  9.1027e-03,  4.7174e-03,  1.6921e-03, -4.0109e-03],\n",
            "          [ 1.6713e-02,  2.2522e-02,  3.1377e-02,  3.4680e-03, -2.4704e-03],\n",
            "          [ 3.1776e-02,  6.6709e-02,  3.5500e-02,  3.2025e-02,  1.3644e-02],\n",
            "          [ 3.1417e-02,  5.7924e-02,  7.1307e-02,  4.0521e-02,  1.0176e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.2440e-01, -2.2281e-02,  8.6111e-02,  7.2882e-02,  5.9584e-02],\n",
            "          [ 4.0570e-03,  2.4288e-01,  1.4540e-01,  3.3435e-01,  2.6994e-01],\n",
            "          [ 1.1844e-01,  2.7605e-01,  7.8653e-01,  2.4088e-01,  1.8679e-01],\n",
            "          [ 1.4147e-01,  5.9136e-01,  3.5763e-01,  5.7525e-01,  4.0320e-01],\n",
            "          [ 6.6250e-02,  2.7184e-01,  4.3832e-01,  3.2840e-01,  3.0221e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4536]]],\n",
            "\n",
            "\n",
            "        [[[6.1532]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4536, 6.1532], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 5] = 0.45360565185546875\n",
            " somado na saída em [0, 1, 5, 5] = 6.153151512145996\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 6\n",
            " alvo do kernel em x: x[0,0,15:20, 18:23]\n",
            " \n",
            " tensor([[0.4315, 0.9336, 0.3189, 0.4381, 0.4662],\n",
            "        [0.6268, 0.6426, 0.5365, 0.8428, 0.2218],\n",
            "        [0.3240, 0.3179, 0.0149, 0.8289, 0.4548],\n",
            "        [0.7436, 0.6936, 0.7246, 0.6506, 0.1683],\n",
            "        [0.6647, 0.8437, 0.9484, 0.7611, 0.2054]])\n",
            " produto: tensor([[[[-3.7228e-03, -1.3330e-04, -2.1797e-04,  1.6760e-03, -8.7938e-03],\n",
            "          [ 8.4541e-03,  6.8123e-03,  9.7402e-03,  2.2752e-03, -1.3841e-03],\n",
            "          [ 6.7272e-03,  1.3558e-02,  4.8300e-04,  8.8717e-03, -3.5347e-03],\n",
            "          [ 4.5985e-02,  5.1012e-02,  6.2465e-02,  2.8021e-02,  3.3113e-03],\n",
            "          [ 6.1069e-02,  9.5512e-02,  1.0108e-01,  4.6397e-02,  2.4776e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.0505e-02, -2.4275e-02,  3.9691e-02,  7.3986e-02,  2.9751e-02],\n",
            "          [ 9.2567e-03,  1.8176e-01,  3.0022e-01,  4.4955e-01,  9.3153e-02],\n",
            "          [ 4.7673e-02,  1.6618e-01,  1.2107e-02,  6.1622e-01,  2.6727e-01],\n",
            "          [ 2.0472e-01,  4.5222e-01,  6.2928e-01,  5.0332e-01,  9.7852e-02],\n",
            "          [ 1.2878e-01,  4.4825e-01,  6.2135e-01,  3.7602e-01,  7.3579e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.5381]]],\n",
            "\n",
            "\n",
            "        [[[5.7374]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.5381, 5.7374], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 6] = 0.5381428003311157\n",
            " somado na saída em [0, 1, 5, 6] = 5.73740816116333\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 7\n",
            " alvo do kernel em x: x[0,0,15:20, 21:26]\n",
            " \n",
            " tensor([[0.4381, 0.4662, 0.1572, 0.7759, 0.9364],\n",
            "        [0.8428, 0.2218, 0.2639, 0.2095, 0.9840],\n",
            "        [0.8289, 0.4548, 0.3353, 0.2510, 0.9980],\n",
            "        [0.6506, 0.1683, 0.4719, 0.0071, 0.3828],\n",
            "        [0.7611, 0.2054, 0.3938, 0.3947, 0.2218]])\n",
            " produto: tensor([[[[-3.7791e-03, -6.6560e-05, -1.0745e-04,  2.9685e-03, -1.7664e-02],\n",
            "          [ 1.1367e-02,  2.3508e-03,  4.7913e-03,  5.6548e-04, -6.1416e-03],\n",
            "          [ 1.7209e-02,  1.9399e-02,  1.0856e-02,  2.6860e-03, -7.7566e-03],\n",
            "          [ 4.0235e-02,  1.2380e-02,  4.0682e-02,  3.0697e-04,  7.5304e-03],\n",
            "          [ 6.9925e-02,  2.3255e-02,  4.1974e-02,  2.4063e-02,  2.6750e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.1421e-02, -1.2121e-02,  1.9566e-02,  1.3105e-01,  5.9762e-02],\n",
            "          [ 1.2446e-02,  6.2725e-02,  1.4768e-01,  1.1173e-01,  4.1334e-01],\n",
            "          [ 1.2196e-01,  2.3778e-01,  2.7214e-01,  1.8657e-01,  5.8650e-01],\n",
            "          [ 1.7912e-01,  1.0975e-01,  4.0984e-01,  5.5139e-03,  2.2253e-01],\n",
            "          [ 1.4745e-01,  1.0914e-01,  2.5802e-01,  1.9502e-01,  7.9442e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2997]]],\n",
            "\n",
            "\n",
            "        [[[4.0055]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2997, 4.0055], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 5, 7] = 0.2997044622898102\n",
            " somado na saída em [0, 1, 5, 7] = 4.005506992340088\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 0\n",
            " alvo do kernel em x: x[0,0,18:23, 0:5]\n",
            " \n",
            " tensor([[0.9321, 0.9256, 0.7926, 0.8350, 0.9735],\n",
            "        [0.3373, 0.1107, 0.9173, 0.6569, 0.0609],\n",
            "        [0.6109, 0.6175, 0.5783, 0.7307, 0.5122],\n",
            "        [0.5955, 0.4283, 0.8619, 0.3221, 0.1653],\n",
            "        [0.5441, 0.9790, 0.7058, 0.4529, 0.1724]])\n",
            " produto: tensor([[[[-8.0409e-03, -1.3215e-04, -5.4172e-04,  3.1946e-03, -1.8363e-02],\n",
            "          [ 4.5490e-03,  1.1738e-03,  1.6653e-02,  1.7733e-03, -3.8018e-04],\n",
            "          [ 1.2683e-02,  2.6340e-02,  1.8725e-02,  7.8205e-03, -3.9808e-03],\n",
            "          [ 3.6830e-02,  3.1500e-02,  7.4307e-02,  1.3873e-02,  3.2521e-03],\n",
            "          [ 4.9989e-02,  1.1083e-01,  7.5226e-02,  2.7606e-02,  2.0791e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3069e-01, -2.4065e-02,  9.8646e-02,  1.4103e-01,  6.2125e-02],\n",
            "          [ 4.9808e-03,  3.1319e-02,  5.1329e-01,  3.5039e-01,  2.5587e-02],\n",
            "          [ 8.9882e-02,  3.2285e-01,  4.6937e-01,  5.4320e-01,  3.0100e-01],\n",
            "          [ 1.6396e-01,  2.7924e-01,  7.4858e-01,  2.4919e-01,  9.6101e-02],\n",
            "          [ 1.0541e-01,  5.2013e-01,  4.6242e-01,  2.2373e-01,  6.1746e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4870]]],\n",
            "\n",
            "\n",
            "        [[[5.7094]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4870, 5.7094], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 0] = 0.48696452379226685\n",
            " somado na saída em [0, 1, 6, 0] = 5.709432125091553\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 1\n",
            " alvo do kernel em x: x[0,0,18:23, 3:8]\n",
            " \n",
            " tensor([[0.8350, 0.9735, 0.8340, 0.3794, 0.5933],\n",
            "        [0.6569, 0.0609, 0.1597, 0.5346, 0.4560],\n",
            "        [0.7307, 0.5122, 0.0679, 0.9169, 0.1045],\n",
            "        [0.3221, 0.1653, 0.8142, 0.9603, 0.1189],\n",
            "        [0.4529, 0.1724, 0.9092, 0.8097, 0.2121]])\n",
            " produto: tensor([[[[-7.2036e-03, -1.3899e-04, -5.7002e-04,  1.4516e-03, -1.1191e-02],\n",
            "          [ 8.8598e-03,  6.4573e-04,  2.8991e-03,  1.4432e-03, -2.8463e-03],\n",
            "          [ 1.5170e-02,  2.1847e-02,  2.1981e-03,  9.8137e-03, -8.1205e-04],\n",
            "          [ 1.9920e-02,  1.2158e-02,  7.0191e-02,  4.1360e-02,  2.3380e-03],\n",
            "          [ 4.1605e-02,  1.9515e-02,  9.6905e-02,  4.9355e-02,  2.5583e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1708e-01, -2.5310e-02,  1.0380e-01,  6.4083e-02,  3.7862e-02],\n",
            "          [ 9.7009e-03,  1.7229e-02,  8.9358e-02,  2.8516e-01,  1.9156e-01],\n",
            "          [ 1.0751e-01,  2.6778e-01,  5.5099e-02,  6.8165e-01,  6.1402e-02],\n",
            "          [ 8.8684e-02,  1.0778e-01,  7.0711e-01,  7.4293e-01,  6.9089e-02],\n",
            "          [ 8.7732e-02,  9.1584e-02,  5.9568e-01,  3.9999e-01,  7.5976e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3975]]],\n",
            "\n",
            "\n",
            "        [[[4.7964]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3975, 4.7964], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 1] = 0.3974718451499939\n",
            " somado na saída em [0, 1, 6, 1] = 4.796365737915039\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 2\n",
            " alvo do kernel em x: x[0,0,18:23, 6:11]\n",
            " \n",
            " tensor([[0.3794, 0.5933, 0.6574, 0.4123, 0.3551],\n",
            "        [0.5346, 0.4560, 0.5284, 0.2282, 0.5805],\n",
            "        [0.9169, 0.1045, 0.2465, 0.3961, 0.3859],\n",
            "        [0.9603, 0.1189, 0.9378, 0.7991, 0.0997],\n",
            "        [0.8097, 0.2121, 0.5644, 0.0153, 0.1865]])\n",
            " produto: tensor([[[[-3.2733e-03, -8.4706e-05, -4.4934e-04,  1.5775e-03, -6.6979e-03],\n",
            "          [ 7.2104e-03,  4.8344e-03,  9.5924e-03,  6.1600e-04, -3.6229e-03],\n",
            "          [ 1.9036e-02,  4.4567e-03,  7.9807e-03,  4.2391e-03, -2.9995e-03],\n",
            "          [ 5.9389e-02,  8.7409e-03,  8.0845e-02,  3.4415e-02,  1.9615e-03],\n",
            "          [ 7.4383e-02,  2.4012e-02,  6.0158e-02,  9.3368e-04,  2.2493e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.3200e-02, -1.5425e-02,  8.1825e-02,  6.9640e-02,  2.2660e-02],\n",
            "          [ 7.8949e-03,  1.2899e-01,  2.9567e-01,  1.2171e-01,  2.4383e-01],\n",
            "          [ 1.3491e-01,  5.4626e-02,  2.0005e-01,  2.9444e-01,  2.2680e-01],\n",
            "          [ 2.6440e-01,  7.7487e-02,  8.1445e-01,  6.1818e-01,  5.7964e-02],\n",
            "          [ 1.5685e-01,  1.1269e-01,  3.6979e-01,  7.5669e-03,  6.6799e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3895]]],\n",
            "\n",
            "\n",
            "        [[[4.3606]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3895, 4.3606], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 2] = 0.3895035982131958\n",
            " somado na saída em [0, 1, 6, 2] = 4.360596656799316\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 3\n",
            " alvo do kernel em x: x[0,0,18:23, 9:14]\n",
            " \n",
            " tensor([[0.4123, 0.3551, 0.0537, 0.2058, 0.7846],\n",
            "        [0.2282, 0.5805, 0.1810, 0.1015, 0.4929],\n",
            "        [0.3961, 0.3859, 0.0413, 0.2849, 0.0669],\n",
            "        [0.7991, 0.0997, 0.2319, 0.5300, 0.9068],\n",
            "        [0.0153, 0.1865, 0.3269, 0.6611, 0.5052]])\n",
            " produto: tensor([[[[-3.5571e-03, -5.0697e-05, -3.6690e-05,  7.8756e-04, -1.4799e-02],\n",
            "          [ 3.0776e-03,  6.1534e-03,  3.2853e-03,  2.7400e-04, -3.0764e-03],\n",
            "          [ 8.2229e-03,  1.6462e-02,  1.3385e-03,  3.0499e-03, -5.1962e-04],\n",
            "          [ 4.9417e-02,  7.3335e-03,  1.9995e-02,  2.2826e-02,  1.7839e-02],\n",
            "          [ 1.4071e-03,  2.1112e-02,  3.4838e-02,  4.0299e-02,  6.0930e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.7813e-02, -9.2319e-03,  6.6812e-03,  3.4767e-02,  5.0070e-02],\n",
            "          [ 3.3698e-03,  1.6418e-01,  1.0126e-01,  5.4140e-02,  2.0705e-01],\n",
            "          [ 5.8273e-02,  2.0177e-01,  3.3553e-02,  2.1184e-01,  3.9290e-02],\n",
            "          [ 2.2000e-01,  6.5011e-02,  2.0143e-01,  4.1002e-01,  5.2715e-01],\n",
            "          [ 2.9672e-03,  9.9079e-02,  2.1415e-01,  3.2660e-01,  1.8095e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2418]]],\n",
            "\n",
            "\n",
            "        [[[3.3466]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2418, 3.3466], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 3] = 0.24177047610282898\n",
            " somado na saída em [0, 1, 6, 3] = 3.3465662002563477\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 4\n",
            " alvo do kernel em x: x[0,0,18:23, 12:17]\n",
            " \n",
            " tensor([[0.2058, 0.7846, 0.8677, 0.5138, 0.9070],\n",
            "        [0.1015, 0.4929, 0.6992, 0.3420, 0.5117],\n",
            "        [0.2849, 0.0669, 0.8378, 0.7638, 0.8304],\n",
            "        [0.5300, 0.9068, 0.8950, 0.2662, 0.4623],\n",
            "        [0.6611, 0.5052, 0.9249, 0.3572, 0.6081]])\n",
            " produto: tensor([[[[-1.7759e-03, -1.1202e-04, -5.9306e-04,  1.9658e-03, -1.7110e-02],\n",
            "          [ 1.3690e-03,  5.2251e-03,  1.2693e-02,  9.2321e-04, -3.1936e-03],\n",
            "          [ 5.9161e-03,  2.8518e-03,  2.7128e-02,  8.1753e-03, -6.4535e-03],\n",
            "          [ 3.2776e-02,  6.6694e-02,  7.7154e-02,  1.1467e-02,  9.0949e-03],\n",
            "          [ 6.0735e-02,  5.7188e-02,  9.8575e-02,  2.1771e-02,  7.3340e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.8862e-02, -2.0398e-02,  1.0800e-01,  8.6782e-02,  5.7887e-02],\n",
            "          [ 1.4989e-03,  1.3942e-01,  3.9124e-01,  1.8241e-01,  2.1494e-01],\n",
            "          [ 4.1926e-02,  3.4955e-02,  6.8002e-01,  5.6785e-01,  4.8797e-01],\n",
            "          [ 1.4592e-01,  5.9123e-01,  7.7726e-01,  2.0597e-01,  2.6876e-01],\n",
            "          [ 1.2807e-01,  2.6839e-01,  6.0594e-01,  1.7644e-01,  2.1781e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4798]]],\n",
            "\n",
            "\n",
            "        [[[6.3314]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4798, 6.3314], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 4] = 0.47979873418807983\n",
            " somado na saída em [0, 1, 6, 4] = 6.331419944763184\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 5\n",
            " alvo do kernel em x: x[0,0,18:23, 15:20]\n",
            " \n",
            " tensor([[0.5138, 0.9070, 0.4118, 0.7436, 0.6936],\n",
            "        [0.3420, 0.5117, 0.6690, 0.6647, 0.8437],\n",
            "        [0.7638, 0.8304, 0.2348, 0.6176, 0.4164],\n",
            "        [0.2662, 0.4623, 0.5820, 0.6453, 0.6175],\n",
            "        [0.3572, 0.6081, 0.7061, 0.8912, 0.3403]])\n",
            " produto: tensor([[[[-4.4328e-03, -1.2951e-04, -2.8145e-04,  2.8449e-03, -1.3084e-02],\n",
            "          [ 4.6124e-03,  5.4242e-03,  1.2146e-02,  1.7945e-03, -5.2659e-03],\n",
            "          [ 1.5858e-02,  3.5418e-02,  7.6031e-03,  6.6099e-03, -3.2363e-03],\n",
            "          [ 1.6465e-02,  3.4003e-02,  5.0177e-02,  2.7793e-02,  1.2147e-02],\n",
            "          [ 3.2811e-02,  6.8837e-02,  7.5261e-02,  5.4324e-02,  4.1042e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.2045e-02, -2.3583e-02,  5.1252e-02,  1.2559e-01,  4.4266e-02],\n",
            "          [ 5.0503e-03,  1.4473e-01,  3.7437e-01,  3.5458e-01,  3.5441e-01],\n",
            "          [ 1.1238e-01,  4.3412e-01,  1.9059e-01,  4.5912e-01,  2.4470e-01],\n",
            "          [ 7.3302e-02,  3.0143e-01,  5.0549e-01,  4.9922e-01,  3.5894e-01],\n",
            "          [ 6.9189e-02,  3.2306e-01,  4.6264e-01,  4.4026e-01,  1.2189e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4418]]],\n",
            "\n",
            "\n",
            "        [[[5.9549]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4418, 5.9549], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 5] = 0.4418036639690399\n",
            " somado na saída em [0, 1, 6, 5] = 5.954948425292969\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 6\n",
            " alvo do kernel em x: x[0,0,18:23, 18:23]\n",
            " \n",
            " tensor([[0.7436, 0.6936, 0.7246, 0.6506, 0.1683],\n",
            "        [0.6647, 0.8437, 0.9484, 0.7611, 0.2054],\n",
            "        [0.6176, 0.4164, 0.2063, 0.7231, 0.6208],\n",
            "        [0.6453, 0.6175, 0.0297, 0.9788, 0.4667],\n",
            "        [0.8912, 0.3403, 0.8835, 0.7122, 0.6094]])\n",
            " produto: tensor([[[[-6.4149e-03, -9.9033e-05, -4.9524e-04,  2.4891e-03, -3.1753e-03],\n",
            "          [ 8.9657e-03,  8.9440e-03,  1.7218e-02,  2.0548e-03, -1.2821e-03],\n",
            "          [ 1.2822e-02,  1.7761e-02,  6.6787e-03,  7.7399e-03, -4.8250e-03],\n",
            "          [ 3.9907e-02,  4.5413e-02,  2.5611e-03,  4.2154e-02,  9.1815e-03],\n",
            "          [ 8.1871e-02,  3.8522e-02,  9.4167e-02,  4.3416e-02,  7.3504e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0426e-01, -1.8034e-02,  9.0183e-02,  1.0988e-01,  1.0743e-02],\n",
            "          [ 9.8168e-03,  2.3864e-01,  5.3069e-01,  4.0600e-01,  8.6289e-02],\n",
            "          [ 9.0865e-02,  2.1770e-01,  1.6741e-01,  5.3760e-01,  3.6483e-01],\n",
            "          [ 1.7767e-01,  4.0258e-01,  2.5801e-02,  7.5719e-01,  2.7132e-01],\n",
            "          [ 1.7264e-01,  1.8079e-01,  5.7885e-01,  3.5186e-01,  2.1829e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4729]]],\n",
            "\n",
            "\n",
            "        [[[5.8753]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4729, 5.8753], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 6] = 0.47292494773864746\n",
            " somado na saída em [0, 1, 6, 6] = 5.875349521636963\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 7\n",
            " alvo do kernel em x: x[0,0,18:23, 21:26]\n",
            " \n",
            " tensor([[0.6506, 0.1683, 0.4719, 0.0071, 0.3828],\n",
            "        [0.7611, 0.2054, 0.3938, 0.3947, 0.2218],\n",
            "        [0.7231, 0.6208, 0.4562, 0.9173, 0.5390],\n",
            "        [0.9788, 0.4667, 0.2137, 0.0541, 0.0818],\n",
            "        [0.7122, 0.6094, 0.3924, 0.9768, 0.1783]])\n",
            " produto: tensor([[[[-5.6127e-03, -2.4034e-05, -3.2254e-04,  2.7269e-05, -7.2211e-03],\n",
            "          [ 1.0266e-02,  2.1776e-03,  7.1496e-03,  1.0657e-03, -1.3843e-03],\n",
            "          [ 1.5014e-02,  2.6480e-02,  1.4773e-02,  9.8176e-03, -4.1893e-03],\n",
            "          [ 6.0529e-02,  3.4326e-02,  1.8426e-02,  2.3305e-03,  1.6090e-03],\n",
            "          [ 6.5432e-02,  6.8990e-02,  4.1828e-02,  5.9546e-02,  2.1508e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.1222e-02, -4.3766e-03,  5.8734e-02,  1.2038e-03,  2.4431e-02],\n",
            "          [ 1.1240e-02,  5.8103e-02,  2.2037e-01,  2.1056e-01,  9.3164e-02],\n",
            "          [ 1.0640e-01,  3.2457e-01,  3.7031e-01,  6.8192e-01,  3.1676e-01],\n",
            "          [ 2.6947e-01,  3.0430e-01,  1.8563e-01,  4.1862e-02,  4.7548e-02],\n",
            "          [ 1.3798e-01,  3.2378e-01,  2.5712e-01,  4.8258e-01,  6.3875e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4232]]],\n",
            "\n",
            "\n",
            "        [[[4.4963]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4232, 4.4963], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 6, 7] = 0.4231850504875183\n",
            " somado na saída em [0, 1, 6, 7] = 4.4963178634643555\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 0\n",
            " alvo do kernel em x: x[0,0,21:26, 0:5]\n",
            " \n",
            " tensor([[0.5955, 0.4283, 0.8619, 0.3221, 0.1653],\n",
            "        [0.5441, 0.9790, 0.7058, 0.4529, 0.1724],\n",
            "        [0.3082, 0.2879, 0.0802, 0.0084, 0.4248],\n",
            "        [0.0684, 0.5830, 0.1911, 0.1550, 0.1225],\n",
            "        [0.4897, 0.1014, 0.9633, 0.4375, 0.7549]])\n",
            " produto: tensor([[[[-5.1377e-03, -6.1152e-05, -5.8913e-04,  1.2324e-03, -3.1185e-03],\n",
            "          [ 7.3390e-03,  1.0378e-02,  1.2813e-02,  1.2226e-03, -1.0759e-03],\n",
            "          [ 6.3992e-03,  1.2279e-02,  2.5963e-03,  8.9977e-05, -3.3018e-03],\n",
            "          [ 4.2302e-03,  4.2879e-02,  1.6475e-02,  6.6747e-03,  2.4101e-03],\n",
            "          [ 4.4989e-02,  1.1476e-02,  1.0267e-01,  2.6672e-02,  9.1045e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.3502e-02, -1.1136e-02,  1.0728e-01,  5.4403e-02,  1.0551e-02],\n",
            "          [ 8.0357e-03,  2.7691e-01,  3.9495e-01,  2.4156e-01,  7.2412e-02],\n",
            "          [ 4.5349e-02,  1.5051e-01,  6.5080e-02,  6.2497e-03,  2.4966e-01],\n",
            "          [ 1.8833e-02,  3.8012e-01,  1.6597e-01,  1.1989e-01,  7.1219e-02],\n",
            "          [ 9.4869e-02,  5.3857e-02,  6.3110e-01,  2.1616e-01,  2.7038e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3086]]],\n",
            "\n",
            "\n",
            "        [[[3.6107]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3086, 3.6107], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 0] = 0.30864351987838745\n",
            " somado na saída em [0, 1, 7, 0] = 3.6107137203216553\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 1\n",
            " alvo do kernel em x: x[0,0,21:26, 3:8]\n",
            " \n",
            " tensor([[3.2211e-01, 1.6532e-01, 8.1419e-01, 9.6032e-01, 1.1885e-01],\n",
            "        [4.5287e-01, 1.7238e-01, 9.0919e-01, 8.0965e-01, 2.1211e-01],\n",
            "        [8.4065e-03, 4.2484e-01, 5.6430e-01, 2.1914e-01, 5.0838e-01],\n",
            "        [1.5498e-01, 1.2252e-01, 3.6907e-01, 1.9279e-01, 7.2318e-04],\n",
            "        [4.3754e-01, 7.5486e-01, 8.1360e-01, 6.0697e-01, 6.9467e-01]])\n",
            " produto: tensor([[[[-2.7789e-03, -2.3604e-05, -5.5650e-04,  3.6741e-03, -2.2420e-03],\n",
            "          [ 6.1080e-03,  1.8274e-03,  1.6506e-02,  2.1857e-03, -1.3239e-03],\n",
            "          [ 1.7454e-04,  1.8121e-02,  1.8272e-02,  2.3455e-03, -3.9511e-03],\n",
            "          [ 9.5842e-03,  9.0105e-03,  3.1818e-02,  8.3033e-03,  1.4226e-05],\n",
            "          [ 4.0197e-02,  8.5454e-02,  8.6716e-02,  3.6999e-02,  8.3786e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.5164e-02, -4.2983e-03,  1.0134e-01,  1.6219e-01,  7.5850e-03],\n",
            "          [ 6.6879e-03,  4.8758e-02,  5.0877e-01,  4.3188e-01,  8.9100e-02],\n",
            "          [ 1.2369e-03,  2.2211e-01,  4.5803e-01,  1.6291e-01,  2.9875e-01],\n",
            "          [ 4.2669e-02,  7.9877e-02,  3.2053e-01,  1.4915e-01,  4.2039e-04],\n",
            "          [ 8.4764e-02,  4.0104e-01,  5.3305e-01,  2.9986e-01,  2.4883e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3748]]],\n",
            "\n",
            "\n",
            "        [[[4.6101]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3748, 4.6101], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 1] = 0.37481409311294556\n",
            " somado na saída em [0, 1, 7, 1] = 4.610073089599609\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 2\n",
            " alvo do kernel em x: x[0,0,21:26, 6:11]\n",
            " \n",
            " tensor([[9.6032e-01, 1.1885e-01, 9.3778e-01, 7.9908e-01, 9.9715e-02],\n",
            "        [8.0965e-01, 2.1211e-01, 5.6442e-01, 1.5317e-02, 1.8649e-01],\n",
            "        [2.1914e-01, 5.0838e-01, 3.0504e-01, 1.2130e-01, 3.6044e-01],\n",
            "        [1.9279e-01, 7.2318e-04, 9.2748e-01, 5.2299e-01, 6.4182e-01],\n",
            "        [6.0697e-01, 6.9467e-01, 9.0250e-01, 4.6830e-01, 5.1524e-01]])\n",
            " produto: tensor([[[[-8.2847e-03, -1.6969e-05, -6.4097e-04,  3.0572e-03, -1.8810e-03],\n",
            "          [ 1.0920e-02,  2.2486e-03,  1.0247e-02,  4.1349e-05, -1.1640e-03],\n",
            "          [ 4.5497e-03,  2.1684e-02,  9.8773e-03,  1.2983e-03, -2.8013e-03],\n",
            "          [ 1.1923e-02,  5.3186e-05,  7.9958e-02,  2.2524e-02,  1.2625e-02],\n",
            "          [ 5.5762e-02,  7.8641e-02,  9.6191e-02,  2.8547e-02,  6.2144e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3465e-01, -3.0901e-03,  1.1672e-01,  1.3496e-01,  6.3637e-03],\n",
            "          [ 1.1957e-02,  5.9995e-02,  3.1584e-01,  8.1701e-03,  7.8338e-02],\n",
            "          [ 3.2242e-02,  2.6579e-01,  2.4759e-01,  9.0181e-02,  2.1181e-01],\n",
            "          [ 5.3079e-02,  4.7149e-04,  8.0550e-01,  4.0459e-01,  3.7309e-01],\n",
            "          [ 1.1758e-01,  3.6907e-01,  5.9129e-01,  2.3135e-01,  1.8455e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4416]]],\n",
            "\n",
            "\n",
            "        [[[4.5728]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4416, 4.5728], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 2] = 0.44157344102859497\n",
            " somado na saída em [0, 1, 7, 2] = 4.572811603546143\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 3\n",
            " alvo do kernel em x: x[0,0,21:26, 9:14]\n",
            " \n",
            " tensor([[0.7991, 0.0997, 0.2319, 0.5300, 0.9068],\n",
            "        [0.0153, 0.1865, 0.3269, 0.6611, 0.5052],\n",
            "        [0.1213, 0.3604, 0.6421, 0.4991, 0.4629],\n",
            "        [0.5230, 0.6418, 0.2463, 0.6202, 0.2004],\n",
            "        [0.4683, 0.5152, 0.4266, 0.9482, 0.0918]])\n",
            " produto: tensor([[[[-6.8936e-03, -1.4237e-05, -1.5853e-04,  2.0277e-03, -1.7106e-02],\n",
            "          [ 2.0658e-04,  1.9770e-03,  5.9341e-03,  1.7847e-03, -3.1530e-03],\n",
            "          [ 2.5185e-03,  1.5374e-02,  2.0792e-02,  5.3417e-03, -3.5976e-03],\n",
            "          [ 3.2343e-02,  4.7202e-02,  2.1237e-02,  2.6713e-02,  3.9413e-03],\n",
            "          [ 4.3022e-02,  5.8328e-02,  4.5472e-02,  5.7802e-02,  1.1067e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1204e-01, -2.5926e-03,  2.8867e-02,  8.9514e-02,  5.7874e-02],\n",
            "          [ 2.2620e-04,  5.2748e-02,  1.8291e-01,  3.5263e-01,  2.1220e-01],\n",
            "          [ 1.7848e-02,  1.8844e-01,  5.2119e-01,  3.7103e-01,  2.7202e-01],\n",
            "          [ 1.4399e-01,  4.1844e-01,  2.1394e-01,  4.7983e-01,  1.1647e-01],\n",
            "          [ 9.0722e-02,  2.7374e-01,  2.7952e-01,  4.6845e-01,  3.2867e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3622]]],\n",
            "\n",
            "\n",
            "        [[[4.7508]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3622, 4.7508], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 3] = 0.3621993064880371\n",
            " somado na saída em [0, 1, 7, 3] = 4.750826358795166\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 4\n",
            " alvo do kernel em x: x[0,0,21:26, 12:17]\n",
            " \n",
            " tensor([[0.5300, 0.9068, 0.8950, 0.2662, 0.4623],\n",
            "        [0.6611, 0.5052, 0.9249, 0.3572, 0.6081],\n",
            "        [0.4991, 0.4629, 0.1017, 0.2532, 0.2362],\n",
            "        [0.6202, 0.2004, 0.8454, 0.6420, 0.0426],\n",
            "        [0.9482, 0.0918, 0.8967, 0.6234, 0.3993]])\n",
            " produto: tensor([[[[-4.5723e-03, -1.2948e-04, -6.1170e-04,  1.0186e-03, -8.7214e-03],\n",
            "          [ 8.9165e-03,  5.3553e-03,  1.6791e-02,  9.6416e-04, -3.7952e-03],\n",
            "          [ 1.0362e-02,  1.9744e-02,  3.2931e-03,  2.7099e-03, -1.8359e-03],\n",
            "          [ 3.8357e-02,  1.4735e-02,  7.2879e-02,  2.7652e-02,  8.3805e-04],\n",
            "          [ 8.7113e-02,  1.0388e-02,  9.5575e-02,  3.7999e-02,  4.8160e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.4312e-02, -2.3578e-02,  1.1139e-01,  4.4967e-02,  2.9506e-02],\n",
            "          [ 9.7630e-03,  1.4289e-01,  5.1753e-01,  1.9051e-01,  2.5543e-01],\n",
            "          [ 7.3430e-02,  2.4201e-01,  8.2549e-02,  1.8823e-01,  1.3882e-01],\n",
            "          [ 1.7076e-01,  1.3063e-01,  7.3419e-01,  4.9670e-01,  2.4765e-02],\n",
            "          [ 1.8370e-01,  4.8750e-02,  5.8750e-01,  3.0796e-01,  1.4302e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4398]]],\n",
            "\n",
            "\n",
            "        [[[4.7571]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4398, 4.7571], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 4] = 0.43983978033065796\n",
            " somado na saída em [0, 1, 7, 4] = 4.757097244262695\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 5\n",
            " alvo do kernel em x: x[0,0,21:26, 15:20]\n",
            " \n",
            " tensor([[0.2662, 0.4623, 0.5820, 0.6453, 0.6175],\n",
            "        [0.3572, 0.6081, 0.7061, 0.8912, 0.3403],\n",
            "        [0.2532, 0.2362, 0.9250, 0.4828, 0.9354],\n",
            "        [0.6420, 0.0426, 0.1792, 0.1442, 0.8218],\n",
            "        [0.6234, 0.3993, 0.9316, 0.1697, 0.2218]])\n",
            " produto: tensor([[[[-2.2969e-03, -6.6012e-05, -3.9782e-04,  2.4689e-03, -1.1648e-02],\n",
            "          [ 4.8171e-03,  6.4461e-03,  1.2820e-02,  2.4058e-03, -2.1238e-03],\n",
            "          [ 5.2566e-03,  1.0076e-02,  2.9952e-02,  5.1674e-03, -7.2699e-03],\n",
            "          [ 3.9706e-02,  3.1332e-03,  1.5446e-02,  6.2087e-03,  1.6166e-02],\n",
            "          [ 5.7268e-02,  4.5202e-02,  9.9291e-02,  1.0342e-02,  2.6750e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.7330e-02, -1.2021e-02,  7.2442e-02,  1.0899e-01,  3.9407e-02],\n",
            "          [ 5.2744e-03,  1.7199e-01,  3.9513e-01,  4.7536e-01,  1.4294e-01],\n",
            "          [ 3.7252e-02,  1.2350e-01,  7.5081e-01,  3.5892e-01,  5.4970e-01],\n",
            "          [ 1.7677e-01,  2.7776e-02,  1.5560e-01,  1.1152e-01,  4.7773e-01],\n",
            "          [ 1.2076e-01,  2.1214e-01,  6.1035e-01,  8.3816e-02,  7.9440e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3510]]],\n",
            "\n",
            "\n",
            "        [[[5.2383]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3510, 5.2383], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 5] = 0.35104551911354065\n",
            " somado na saída em [0, 1, 7, 5] = 5.23826265335083\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 6\n",
            " alvo do kernel em x: x[0,0,21:26, 18:23]\n",
            " \n",
            " tensor([[0.6453, 0.6175, 0.0297, 0.9788, 0.4667],\n",
            "        [0.8912, 0.3403, 0.8835, 0.7122, 0.6094],\n",
            "        [0.4828, 0.9354, 0.8345, 0.1694, 0.8611],\n",
            "        [0.1442, 0.8218, 0.4643, 0.5685, 0.5054],\n",
            "        [0.1697, 0.2218, 0.3251, 0.3184, 0.5437]])\n",
            " produto: tensor([[[[-5.5671e-03, -8.8163e-05, -2.0305e-05,  3.7446e-03, -8.8044e-03],\n",
            "          [ 1.2020e-02,  3.6073e-03,  1.6040e-02,  1.9227e-03, -3.8037e-03],\n",
            "          [ 1.0024e-02,  3.9899e-02,  2.7022e-02,  1.8136e-03, -6.6924e-03],\n",
            "          [ 8.9151e-03,  6.0440e-02,  4.0029e-02,  2.4486e-02,  9.9419e-03],\n",
            "          [ 1.5587e-02,  2.5107e-02,  3.4651e-02,  1.9406e-02,  6.5578e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.0480e-02, -1.6054e-02,  3.6976e-03,  1.6531e-01,  2.9787e-02],\n",
            "          [ 1.3161e-02,  9.6248e-02,  4.9439e-01,  3.7991e-01,  2.5600e-01],\n",
            "          [ 7.1034e-02,  4.8904e-01,  6.7737e-01,  1.2597e-01,  5.0604e-01],\n",
            "          [ 3.9690e-02,  5.3580e-01,  4.0326e-01,  4.3982e-01,  2.9379e-01],\n",
            "          [ 3.2867e-02,  1.1783e-01,  2.1300e-01,  1.5728e-01,  1.9475e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3362]]],\n",
            "\n",
            "\n",
            "        [[[5.6295]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3362, 5.6295], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 6] = 0.3362376391887665\n",
            " somado na saída em [0, 1, 7, 6] = 5.629506587982178\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 7\n",
            " alvo do kernel em x: x[0,0,21:26, 21:26]\n",
            " \n",
            " tensor([[0.9788, 0.4667, 0.2137, 0.0541, 0.0818],\n",
            "        [0.7122, 0.6094, 0.3924, 0.9768, 0.1783],\n",
            "        [0.1694, 0.8611, 0.3283, 0.0643, 0.4008],\n",
            "        [0.5685, 0.5054, 0.3062, 0.3153, 0.0631],\n",
            "        [0.3184, 0.5437, 0.1645, 0.1536, 0.0432]])\n",
            " produto: tensor([[[[-8.4438e-03, -6.6640e-05, -1.4609e-04,  2.0703e-04, -1.5429e-03],\n",
            "          [ 9.6062e-03,  6.4604e-03,  7.1246e-03,  2.6371e-03, -1.1130e-03],\n",
            "          [ 3.5180e-03,  3.6730e-02,  1.0632e-02,  6.8778e-04, -3.1149e-03],\n",
            "          [ 3.5159e-02,  3.7170e-02,  2.6401e-02,  1.3579e-02,  1.2415e-03],\n",
            "          [ 2.9247e-02,  6.1551e-02,  1.7534e-02,  9.3648e-03,  5.2158e-04]]],\n",
            "\n",
            "\n",
            "        [[[-1.3723e-01, -1.2135e-02,  2.6602e-02,  9.1393e-03,  5.2201e-03],\n",
            "          [ 1.0518e-02,  1.7238e-01,  2.1960e-01,  5.2105e-01,  7.4909e-02],\n",
            "          [ 2.4931e-02,  4.5020e-01,  2.6651e-01,  4.7773e-02,  2.3553e-01],\n",
            "          [ 1.5653e-01,  3.2950e-01,  2.6596e-01,  2.4392e-01,  3.6687e-02],\n",
            "          [ 6.1674e-02,  2.8886e-01,  1.0778e-01,  7.5896e-02,  1.5490e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2949]]],\n",
            "\n",
            "\n",
            "        [[[3.4973]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2949, 3.4973], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 7, 7] = 0.29494336247444153\n",
            " somado na saída em [0, 1, 7, 7] = 3.497286081314087\n",
            "\n",
            "ndx_amostra: 1\n",
            "\n",
            "ndx_in_channels: 0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 0\n",
            " alvo do kernel em x: x[1,0,0:5, 0:5]\n",
            " \n",
            " tensor([[0.7377, 0.1925, 0.7885, 0.9719, 0.3827],\n",
            "        [0.7311, 0.8451, 0.5695, 0.1233, 0.2492],\n",
            "        [0.8360, 0.3942, 0.3983, 0.3108, 0.4642],\n",
            "        [0.5344, 0.1883, 0.4399, 0.7952, 0.3370],\n",
            "        [0.9918, 0.1789, 0.4808, 0.2339, 0.8803]])\n",
            " produto: tensor([[[[-6.3640e-03, -2.7490e-05, -5.3893e-04,  3.7183e-03, -7.2196e-03],\n",
            "          [ 9.8602e-03,  8.9590e-03,  1.0339e-02,  3.3276e-04, -1.5556e-03],\n",
            "          [ 1.7357e-02,  1.6815e-02,  1.2898e-02,  3.3266e-03, -3.6077e-03],\n",
            "          [ 3.3046e-02,  1.3849e-02,  3.7921e-02,  3.4247e-02,  6.6289e-03],\n",
            "          [ 9.1119e-02,  2.0254e-02,  5.1245e-02,  1.4258e-02,  1.0618e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0343e-01, -5.0059e-03,  9.8138e-02,  1.6415e-01,  2.4425e-02],\n",
            "          [ 1.0796e-02,  2.3904e-01,  3.1868e-01,  6.5750e-02,  1.0469e-01],\n",
            "          [ 1.2301e-01,  2.0611e-01,  3.2332e-01,  2.3106e-01,  2.7279e-01],\n",
            "          [ 1.4712e-01,  1.2277e-01,  3.8203e-01,  6.1515e-01,  1.9589e-01],\n",
            "          [ 1.9214e-01,  9.5052e-02,  3.1501e-01,  1.1555e-01,  3.1533e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3775]]],\n",
            "\n",
            "\n",
            "        [[[4.5696]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3775, 4.5696], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 0] = 0.37747955322265625\n",
            " somado na saída em [1, 1, 0, 0] = 4.569552898406982\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 1\n",
            " alvo do kernel em x: x[1,0,0:5, 3:8]\n",
            " \n",
            " tensor([[0.9719, 0.3827, 0.3329, 0.6197, 0.7214],\n",
            "        [0.1233, 0.2492, 0.2677, 0.2394, 0.2399],\n",
            "        [0.3108, 0.4642, 0.8866, 0.1977, 0.1214],\n",
            "        [0.7952, 0.3370, 0.5276, 0.9645, 0.1278],\n",
            "        [0.2339, 0.8803, 0.4246, 0.8684, 0.8080]])\n",
            " produto: tensor([[[[-8.3844e-03, -5.4645e-05, -2.2755e-04,  2.3710e-03, -1.3609e-02],\n",
            "          [ 1.6625e-03,  2.6421e-03,  4.8593e-03,  6.4630e-04, -1.4975e-03],\n",
            "          [ 6.4529e-03,  1.9800e-02,  2.8709e-02,  2.1161e-03, -9.4322e-04],\n",
            "          [ 4.9175e-02,  2.4783e-02,  4.5484e-02,  4.1538e-02,  2.5130e-03],\n",
            "          [ 2.1489e-02,  9.9660e-02,  4.5253e-02,  5.2935e-02,  9.7460e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3627e-01, -9.9508e-03,  4.1437e-02,  1.0467e-01,  4.6041e-02],\n",
            "          [ 1.8204e-03,  7.0495e-02,  1.4978e-01,  1.2770e-01,  1.0079e-01],\n",
            "          [ 4.5730e-02,  2.4269e-01,  7.1964e-01,  1.4698e-01,  7.1320e-02],\n",
            "          [ 2.1892e-01,  2.1970e-01,  4.5821e-01,  7.4613e-01,  7.4262e-02],\n",
            "          [ 4.5313e-02,  4.6771e-01,  2.7817e-01,  4.2901e-01,  2.8943e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4371]]],\n",
            "\n",
            "\n",
            "        [[[4.9497]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4371, 4.9497], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 1] = 0.43711793422698975\n",
            " somado na saída em [1, 1, 0, 1] = 4.949728012084961\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 2\n",
            " alvo do kernel em x: x[1,0,0:5, 6:11]\n",
            " \n",
            " tensor([[0.6197, 0.7214, 0.2160, 0.7776, 0.6226],\n",
            "        [0.2394, 0.2399, 0.4422, 0.3711, 0.2933],\n",
            "        [0.1977, 0.1214, 0.1794, 0.2603, 0.6972],\n",
            "        [0.9645, 0.1278, 0.0227, 0.7495, 0.1774],\n",
            "        [0.8684, 0.8080, 0.6249, 0.7392, 0.0308]])\n",
            " produto: tensor([[[[-5.3465e-03, -1.0301e-04, -1.4761e-04,  2.9750e-03, -1.1745e-02],\n",
            "          [ 3.2290e-03,  2.5435e-03,  8.0283e-03,  1.0017e-03, -1.8305e-03],\n",
            "          [ 4.1047e-03,  5.1766e-03,  5.8094e-03,  2.7856e-03, -5.4183e-03],\n",
            "          [ 5.9645e-02,  9.3954e-03,  1.9567e-03,  3.2281e-02,  3.4905e-03],\n",
            "          [ 7.9779e-02,  9.1475e-02,  6.6602e-02,  4.5062e-02,  3.7200e-04]]],\n",
            "\n",
            "\n",
            "        [[[-8.6894e-02, -1.8757e-02,  2.6879e-02,  1.3133e-01,  3.9735e-02],\n",
            "          [ 3.5356e-03,  6.7865e-02,  2.4746e-01,  1.9793e-01,  1.2319e-01],\n",
            "          [ 2.9089e-02,  6.3450e-02,  1.4562e-01,  1.9349e-01,  4.0970e-01],\n",
            "          [ 2.6554e-01,  8.3290e-02,  1.9712e-02,  5.7984e-01,  1.0315e-01],\n",
            "          [ 1.6823e-01,  4.2930e-01,  4.0941e-01,  3.6520e-01,  1.1048e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4011]]],\n",
            "\n",
            "\n",
            "        [[[4.0083]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4011, 4.0083], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 2] = 0.4011217951774597\n",
            " somado na saída em [1, 1, 0, 2] = 4.008337497711182\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 3\n",
            " alvo do kernel em x: x[1,0,0:5, 9:14]\n",
            " \n",
            " tensor([[0.7776, 0.6226, 0.8310, 0.2406, 0.9336],\n",
            "        [0.3711, 0.2933, 0.9148, 0.6330, 0.9484],\n",
            "        [0.2603, 0.6972, 0.2019, 0.0327, 0.0957],\n",
            "        [0.7495, 0.1774, 0.4112, 0.4229, 0.3025],\n",
            "        [0.7392, 0.0308, 0.9588, 0.3093, 0.3740]])\n",
            " produto: tensor([[[[-6.7084e-03, -8.8896e-05, -5.6802e-04,  9.2044e-04, -1.7611e-02],\n",
            "          [ 5.0048e-03,  3.1090e-03,  1.6607e-02,  1.7088e-03, -5.9194e-03],\n",
            "          [ 5.4035e-03,  2.9737e-02,  6.5373e-03,  3.4958e-04, -7.4401e-04],\n",
            "          [ 4.6352e-02,  1.3050e-02,  3.5446e-02,  1.8215e-02,  5.9506e-03],\n",
            "          [ 6.7913e-02,  3.4915e-03,  1.0219e-01,  1.8856e-02,  4.5112e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0903e-01, -1.6188e-02,  1.0343e-01,  4.0633e-02,  5.9580e-02],\n",
            "          [ 5.4799e-03,  8.2953e-02,  5.1188e-01,  3.3764e-01,  3.9839e-01],\n",
            "          [ 3.8293e-02,  3.6449e-01,  1.6387e-01,  2.4281e-02,  5.6257e-02],\n",
            "          [ 2.0636e-01,  1.1569e-01,  3.5709e-01,  3.2719e-01,  1.7584e-01],\n",
            "          [ 1.4321e-01,  1.6386e-02,  6.2818e-01,  1.5281e-01,  1.3397e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3537]]],\n",
            "\n",
            "\n",
            "        [[[4.3187]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3537, 4.3187], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 3] = 0.3537158668041229\n",
            " somado na saída em [1, 1, 0, 3] = 4.318696022033691\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 4\n",
            " alvo do kernel em x: x[1,0,0:5, 12:17]\n",
            " \n",
            " tensor([[0.2406, 0.9336, 0.3330, 0.9553, 0.8108],\n",
            "        [0.6330, 0.9484, 0.3823, 0.1131, 0.0424],\n",
            "        [0.0327, 0.0957, 0.8498, 0.3343, 0.3563],\n",
            "        [0.4229, 0.3025, 0.5903, 0.3261, 0.2608],\n",
            "        [0.3093, 0.3740, 0.5957, 0.9616, 0.6283]])\n",
            " produto: tensor([[[[-2.0755e-03, -1.3329e-04, -2.2757e-04,  3.6549e-03, -1.5295e-02],\n",
            "          [ 8.5375e-03,  1.0054e-02,  6.9414e-03,  3.0539e-04, -2.6442e-04],\n",
            "          [ 6.7811e-04,  4.0833e-03,  2.7517e-02,  3.5784e-03, -2.7694e-03],\n",
            "          [ 2.6156e-02,  2.2247e-02,  5.0886e-02,  1.4045e-02,  5.1298e-03],\n",
            "          [ 2.8418e-02,  4.2342e-02,  6.3491e-02,  5.8615e-02,  7.5779e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.3733e-02, -2.4273e-02,  4.1440e-02,  1.6135e-01,  5.1747e-02],\n",
            "          [ 9.3480e-03,  2.6826e-01,  2.1395e-01,  6.0340e-02,  1.7796e-02],\n",
            "          [ 4.8055e-03,  5.0049e-02,  6.8977e-01,  2.4855e-01,  2.0940e-01],\n",
            "          [ 1.1644e-01,  1.9722e-01,  5.1264e-01,  2.5229e-01,  1.5159e-01],\n",
            "          [ 5.9924e-02,  1.9872e-01,  3.9028e-01,  4.7503e-01,  2.2505e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3635]]],\n",
            "\n",
            "\n",
            "        [[[4.5480]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3635, 4.5480], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 4] = 0.3634920120239258\n",
            " somado na saída em [1, 1, 0, 4] = 4.547977447509766\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 5\n",
            " alvo do kernel em x: x[1,0,0:5, 15:20]\n",
            " \n",
            " tensor([[0.9553, 0.8108, 0.7991, 0.4993, 0.4651],\n",
            "        [0.1131, 0.0424, 0.7112, 0.1904, 0.6669],\n",
            "        [0.3343, 0.3563, 0.2753, 0.2350, 0.8561],\n",
            "        [0.3261, 0.2608, 0.7853, 0.2874, 0.0529],\n",
            "        [0.9616, 0.6283, 0.7833, 0.7414, 0.4963]])\n",
            " produto: tensor([[[[-8.2414e-03, -1.1577e-04, -5.4615e-04,  1.9103e-03, -8.7739e-03],\n",
            "          [ 1.5257e-03,  4.4910e-04,  1.2913e-02,  5.1413e-04, -4.1625e-03],\n",
            "          [ 6.9413e-03,  1.5199e-02,  8.9158e-03,  2.5150e-03, -6.6538e-03],\n",
            "          [ 2.0168e-02,  1.9179e-02,  6.7699e-02,  1.2376e-02,  1.0407e-03],\n",
            "          [ 8.8338e-02,  7.1125e-02,  8.3488e-02,  4.5191e-02,  5.9854e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3395e-01, -2.1082e-02,  9.9453e-02,  8.4332e-02,  2.9684e-02],\n",
            "          [ 1.6706e-03,  1.1983e-02,  3.9800e-01,  1.0159e-01,  2.8015e-01],\n",
            "          [ 4.9191e-02,  1.8629e-01,  2.2349e-01,  1.7469e-01,  5.0311e-01],\n",
            "          [ 8.9785e-02,  1.7002e-01,  6.8201e-01,  2.2231e-01,  3.0753e-02],\n",
            "          [ 1.8628e-01,  3.3380e-01,  5.1320e-01,  3.6625e-01,  1.7775e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4370]]],\n",
            "\n",
            "\n",
            "        [[[4.7608]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4370, 4.7608], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 5] = 0.4369792342185974\n",
            " somado na saída em [1, 1, 0, 5] = 4.760759353637695\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 6\n",
            " alvo do kernel em x: x[1,0,0:5, 18:23]\n",
            " \n",
            " tensor([[0.4993, 0.4651, 0.1271, 0.3907, 0.6973],\n",
            "        [0.1904, 0.6669, 0.0325, 0.9823, 0.5632],\n",
            "        [0.2350, 0.8561, 0.7323, 0.5726, 0.5056],\n",
            "        [0.2874, 0.0529, 0.4392, 0.2921, 0.2099],\n",
            "        [0.7414, 0.4963, 0.1538, 0.4666, 0.4519]])\n",
            " produto: tensor([[[[-4.3076e-03, -6.6409e-05, -8.6843e-05,  1.4946e-03, -1.3153e-02],\n",
            "          [ 2.5686e-03,  7.0699e-03,  5.9050e-04,  2.6518e-03, -3.5153e-03],\n",
            "          [ 4.8786e-03,  3.6517e-02,  2.3710e-02,  6.1284e-03, -3.9296e-03],\n",
            "          [ 1.7771e-02,  3.8907e-03,  3.7865e-02,  1.2580e-02,  4.1297e-03],\n",
            "          [ 6.8108e-02,  5.6179e-02,  1.6396e-02,  2.8443e-02,  5.4507e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.0010e-02, -1.2093e-02,  1.5814e-02,  6.5979e-02,  4.4501e-02],\n",
            "          [ 2.8125e-03,  1.8864e-01,  1.8201e-02,  5.2396e-01,  2.3659e-01],\n",
            "          [ 3.4573e-02,  4.4760e-01,  5.9435e-01,  4.2567e-01,  2.9713e-01],\n",
            "          [ 7.9116e-02,  3.4491e-02,  3.8146e-01,  2.2597e-01,  1.2204e-01],\n",
            "          [ 1.4362e-01,  2.6365e-01,  1.0079e-01,  2.3051e-01,  1.6187e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3114]]],\n",
            "\n",
            "\n",
            "        [[[4.5572]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3114, 4.5572], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 6] = 0.31136399507522583\n",
            " somado na saída em [1, 1, 0, 6] = 4.55722188949585\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 7\n",
            " alvo do kernel em x: x[1,0,0:5, 21:26]\n",
            " \n",
            " tensor([[0.3907, 0.6973, 0.3425, 0.1798, 0.7508],\n",
            "        [0.9823, 0.5632, 0.0988, 0.8640, 0.3861],\n",
            "        [0.5726, 0.5056, 0.4275, 0.7434, 0.4764],\n",
            "        [0.2921, 0.2099, 0.6127, 0.6638, 0.3955],\n",
            "        [0.4666, 0.4519, 0.2322, 0.6684, 0.9718]])\n",
            " produto: tensor([[[[-3.3701e-03, -9.9558e-05, -2.3411e-04,  6.8795e-04, -1.4163e-02],\n",
            "          [ 1.3249e-02,  5.9707e-03,  1.7940e-03,  2.3324e-03, -2.4097e-03],\n",
            "          [ 1.1888e-02,  2.1566e-02,  1.3842e-02,  7.9572e-03, -3.7026e-03],\n",
            "          [ 1.8063e-02,  1.5440e-02,  5.2819e-02,  2.8591e-02,  7.7799e-03],\n",
            "          [ 4.2866e-02,  5.1160e-02,  2.4752e-02,  4.0746e-02,  1.1721e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.4774e-02, -1.8130e-02,  4.2631e-02,  3.0370e-02,  4.7915e-02],\n",
            "          [ 1.4506e-02,  1.5931e-01,  5.5296e-02,  4.6085e-01,  1.6218e-01],\n",
            "          [ 8.4245e-02,  2.6434e-01,  3.4699e-01,  5.5270e-01,  2.7997e-01],\n",
            "          [ 8.0418e-02,  1.3687e-01,  5.3210e-01,  5.1356e-01,  2.2990e-01],\n",
            "          [ 9.0392e-02,  2.4010e-01,  1.5215e-01,  3.3022e-01,  3.4807e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3492]]],\n",
            "\n",
            "\n",
            "        [[[5.0822]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3492, 5.0822], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 0, 7] = 0.34924545884132385\n",
            " somado na saída em [1, 1, 0, 7] = 5.082174301147461\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 0\n",
            " alvo do kernel em x: x[1,0,3:8, 0:5]\n",
            " \n",
            " tensor([[0.5344, 0.1883, 0.4399, 0.7952, 0.3370],\n",
            "        [0.9918, 0.1789, 0.4808, 0.2339, 0.8803],\n",
            "        [0.6659, 0.9243, 0.6295, 0.6239, 0.6413],\n",
            "        [0.4327, 0.0011, 0.9412, 0.8398, 0.1461],\n",
            "        [0.1966, 0.4825, 0.1710, 0.5037, 0.6770]])\n",
            " produto: tensor([[[[-4.6100e-03, -2.6886e-05, -3.0065e-04,  3.0422e-03, -6.3567e-03],\n",
            "          [ 1.3377e-02,  1.8966e-03,  8.7288e-03,  6.3144e-04, -5.4946e-03],\n",
            "          [ 1.3826e-02,  3.9423e-02,  2.0384e-02,  6.6777e-03, -4.9841e-03],\n",
            "          [ 2.6759e-02,  7.8524e-05,  8.1138e-02,  3.6167e-02,  2.8736e-03],\n",
            "          [ 1.8060e-02,  5.4625e-02,  1.8223e-02,  3.0703e-02,  8.1653e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.4925e-02, -4.8959e-03,  5.4748e-02,  1.3430e-01,  2.1506e-02],\n",
            "          [ 1.4647e-02,  5.0604e-02,  2.6904e-01,  1.2477e-01,  3.6980e-01],\n",
            "          [ 9.7983e-02,  4.8322e-01,  5.1097e-01,  4.6382e-01,  3.7686e-01],\n",
            "          [ 1.1913e-01,  6.9610e-04,  8.1740e-01,  6.4966e-01,  8.4916e-02],\n",
            "          [ 3.8083e-02,  2.5636e-01,  1.1202e-01,  2.4883e-01,  2.4249e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3630]]],\n",
            "\n",
            "\n",
            "        [[[5.4620]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3630, 5.4620], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 0] = 0.3630070090293884\n",
            " somado na saída em [1, 1, 1, 0] = 5.462017059326172\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 1\n",
            " alvo do kernel em x: x[1,0,3:8, 3:8]\n",
            " \n",
            " tensor([[0.7952, 0.3370, 0.5276, 0.9645, 0.1278],\n",
            "        [0.2339, 0.8803, 0.4246, 0.8684, 0.8080],\n",
            "        [0.6239, 0.6413, 0.7733, 0.2482, 0.4480],\n",
            "        [0.8398, 0.1461, 0.1436, 0.3236, 0.9132],\n",
            "        [0.5037, 0.6770, 0.8126, 0.1988, 0.2089]])\n",
            " produto: tensor([[[[-6.8599e-03, -4.8114e-05, -3.6061e-04,  3.6899e-03, -2.4098e-03],\n",
            "          [ 3.1548e-03,  9.3324e-03,  7.7081e-03,  2.3443e-03, -5.0433e-03],\n",
            "          [ 1.2953e-02,  2.7354e-02,  2.5041e-02,  2.6567e-03, -3.4819e-03],\n",
            "          [ 5.1933e-02,  1.0743e-02,  1.2377e-02,  1.3937e-02,  1.7963e-02],\n",
            "          [ 4.6272e-02,  7.6639e-02,  8.6605e-02,  1.2120e-02,  2.5194e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1149e-01, -8.7615e-03,  6.5666e-02,  1.6289e-01,  8.1530e-03],\n",
            "          [ 3.4543e-03,  2.4900e-01,  2.3758e-01,  4.6321e-01,  3.3943e-01],\n",
            "          [ 9.1795e-02,  3.3528e-01,  6.2770e-01,  1.8453e-01,  2.6327e-01],\n",
            "          [ 2.3120e-01,  9.5238e-02,  1.2468e-01,  2.5035e-01,  5.3082e-01],\n",
            "          [ 9.7574e-02,  3.5967e-01,  5.3237e-01,  9.8226e-02,  7.4821e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4071]]],\n",
            "\n",
            "\n",
            "        [[[5.3067]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4071, 5.3067], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 1] = 0.4071393311023712\n",
            " somado na saída em [1, 1, 1, 1] = 5.306671142578125\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 2\n",
            " alvo do kernel em x: x[1,0,3:8, 6:11]\n",
            " \n",
            " tensor([[0.9645, 0.1278, 0.0227, 0.7495, 0.1774],\n",
            "        [0.8684, 0.8080, 0.6249, 0.7392, 0.0308],\n",
            "        [0.2482, 0.4480, 0.2042, 0.3692, 0.8055],\n",
            "        [0.3236, 0.9132, 0.6247, 0.6025, 0.0708],\n",
            "        [0.1988, 0.2089, 0.3134, 0.4174, 0.3869]])\n",
            " produto: tensor([[[[-8.3204e-03, -1.8240e-05, -1.5514e-05,  2.8676e-03, -3.3472e-03],\n",
            "          [ 1.1712e-02,  8.5659e-03,  1.1345e-02,  1.9956e-03, -1.9250e-04],\n",
            "          [ 5.1534e-03,  1.9109e-02,  6.6110e-03,  3.9514e-03, -6.2602e-03],\n",
            "          [ 2.0013e-02,  6.7158e-02,  5.3859e-02,  2.5948e-02,  1.3926e-03],\n",
            "          [ 1.8266e-02,  2.3647e-02,  3.3407e-02,  2.5444e-02,  4.6659e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3523e-01, -3.3215e-03,  2.8250e-03,  1.2659e-01,  1.1324e-02],\n",
            "          [ 1.2824e-02,  2.2855e-01,  3.4967e-01,  3.9431e-01,  1.2956e-02],\n",
            "          [ 3.6521e-02,  2.3422e-01,  1.6572e-01,  2.7446e-01,  4.7335e-01],\n",
            "          [ 8.9095e-02,  5.9535e-01,  5.4258e-01,  4.6608e-01,  4.1154e-02],\n",
            "          [ 3.8518e-02,  1.1098e-01,  2.0535e-01,  2.0621e-01,  1.3857e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3270]]],\n",
            "\n",
            "\n",
            "        [[[4.6187]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3270, 4.6187], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 2] = 0.3269566297531128\n",
            " somado na saída em [1, 1, 1, 2] = 4.618663787841797\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 3\n",
            " alvo do kernel em x: x[1,0,3:8, 9:14]\n",
            " \n",
            " tensor([[0.7495, 0.1774, 0.4112, 0.4229, 0.3025],\n",
            "        [0.7392, 0.0308, 0.9588, 0.3093, 0.3740],\n",
            "        [0.3692, 0.8055, 0.3025, 0.2320, 0.7119],\n",
            "        [0.6025, 0.0708, 0.0657, 0.2055, 0.1180],\n",
            "        [0.4174, 0.3869, 0.0729, 0.9582, 0.6338]])\n",
            " produto: tensor([[[[-6.4661e-03, -2.5335e-05, -2.8103e-04,  1.6181e-03, -5.7062e-03],\n",
            "          [ 9.9704e-03,  3.2696e-04,  1.7407e-02,  8.3506e-04, -2.3345e-03],\n",
            "          [ 7.6648e-03,  3.4357e-02,  9.7946e-03,  2.4830e-03, -5.5331e-03],\n",
            "          [ 3.7258e-02,  5.2067e-03,  5.6622e-03,  8.8503e-03,  2.3210e-03],\n",
            "          [ 3.8346e-02,  4.3794e-02,  7.7729e-03,  5.8410e-02,  7.6441e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0509e-01, -4.6135e-03,  5.1175e-02,  7.1432e-02,  1.9305e-02],\n",
            "          [ 1.0917e-02,  8.7238e-03,  5.3652e-01,  1.6500e-01,  1.5712e-01],\n",
            "          [ 5.4318e-02,  4.2112e-01,  2.4552e-01,  1.7247e-01,  4.1838e-01],\n",
            "          [ 1.6587e-01,  4.6157e-02,  5.7041e-02,  1.5897e-01,  6.8588e-02],\n",
            "          [ 8.0861e-02,  2.0553e-01,  4.7780e-02,  4.7337e-01,  2.2701e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2794]]],\n",
            "\n",
            "\n",
            "        [[[3.7535]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2794, 3.7535], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 3] = 0.27937597036361694\n",
            " somado na saída em [1, 1, 1, 3] = 3.7534732818603516\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 4\n",
            " alvo do kernel em x: x[1,0,3:8, 12:17]\n",
            " \n",
            " tensor([[0.4229, 0.3025, 0.5903, 0.3261, 0.2608],\n",
            "        [0.3093, 0.3740, 0.5957, 0.9616, 0.6283],\n",
            "        [0.2320, 0.7119, 0.7585, 0.0248, 0.5517],\n",
            "        [0.2055, 0.1180, 0.3861, 0.1058, 0.2682],\n",
            "        [0.9582, 0.6338, 0.4486, 0.2879, 0.6869]])\n",
            " produto: tensor([[[[-3.6487e-03, -4.3190e-05, -4.0344e-04,  1.2477e-03, -4.9191e-03],\n",
            "          [ 4.1720e-03,  3.9650e-03,  1.0815e-02,  2.5958e-03, -3.9214e-03],\n",
            "          [ 4.8165e-03,  3.0367e-02,  2.4562e-02,  2.6537e-04, -4.2880e-03],\n",
            "          [ 1.2708e-02,  8.6776e-03,  3.3284e-02,  4.5585e-03,  5.2753e-03],\n",
            "          [ 8.8029e-02,  7.1747e-02,  4.7813e-02,  1.7549e-02,  8.2849e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.9301e-02, -7.8650e-03,  7.3466e-02,  5.5079e-02,  1.6642e-02],\n",
            "          [ 4.5681e-03,  1.0579e-01,  3.3334e-01,  5.1290e-01,  2.6392e-01],\n",
            "          [ 3.4133e-02,  3.7221e-01,  6.1569e-01,  1.8432e-02,  3.2423e-01],\n",
            "          [ 5.6576e-02,  7.6926e-02,  3.3531e-01,  8.1882e-02,  1.5589e-01],\n",
            "          [ 1.8563e-01,  3.3671e-01,  2.9391e-01,  1.4222e-01,  2.4604e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3635]]],\n",
            "\n",
            "\n",
            "        [[[4.5743]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3635, 4.5743], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 4] = 0.36350834369659424\n",
            " somado na saída em [1, 1, 1, 4] = 4.574331283569336\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 5\n",
            " alvo do kernel em x: x[1,0,3:8, 15:20]\n",
            " \n",
            " tensor([[0.3261, 0.2608, 0.7853, 0.2874, 0.0529],\n",
            "        [0.9616, 0.6283, 0.7833, 0.7414, 0.4963],\n",
            "        [0.0248, 0.5517, 0.2081, 0.6474, 0.6638],\n",
            "        [0.1058, 0.2682, 0.3871, 0.2605, 0.0714],\n",
            "        [0.2879, 0.6869, 0.9915, 0.7763, 0.0241]])\n",
            " produto: tensor([[[[-2.8134e-03, -3.7233e-05, -5.3674e-04,  1.0994e-03, -9.9793e-04],\n",
            "          [ 1.2969e-02,  6.6604e-03,  1.4221e-02,  2.0014e-03, -3.0973e-03],\n",
            "          [ 5.1476e-04,  2.3533e-02,  6.7396e-03,  6.9296e-03, -5.1591e-03],\n",
            "          [ 6.5455e-03,  1.9723e-02,  3.3371e-02,  1.1218e-02,  1.4052e-03],\n",
            "          [ 2.6448e-02,  7.7762e-02,  1.0568e-01,  4.7320e-02,  2.9030e-04]]],\n",
            "\n",
            "\n",
            "        [[[-4.5725e-02, -6.7801e-03,  9.7739e-02,  4.8534e-02,  3.3762e-03],\n",
            "          [ 1.4200e-02,  1.7771e-01,  4.3832e-01,  3.9544e-01,  2.0846e-01],\n",
            "          [ 3.6480e-03,  2.8845e-01,  1.6894e-01,  4.8132e-01,  3.9009e-01],\n",
            "          [ 2.9140e-02,  1.7484e-01,  3.3619e-01,  2.0151e-01,  4.1525e-02],\n",
            "          [ 5.5770e-02,  3.6494e-01,  6.4959e-01,  3.8349e-01,  8.6212e-03]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3918]]],\n",
            "\n",
            "\n",
            "        [[[4.9094]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3918, 4.9094], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 5] = 0.3917839527130127\n",
            " somado na saída em [1, 1, 1, 5] = 4.909350395202637\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 6\n",
            " alvo do kernel em x: x[1,0,3:8, 18:23]\n",
            " \n",
            " tensor([[0.2874, 0.0529, 0.4392, 0.2921, 0.2099],\n",
            "        [0.7414, 0.4963, 0.1538, 0.4666, 0.4519],\n",
            "        [0.6474, 0.6638, 0.3056, 0.2521, 0.9368],\n",
            "        [0.2605, 0.0714, 0.6465, 0.9311, 0.0269],\n",
            "        [0.7763, 0.0241, 0.2452, 0.0327, 0.3735]])\n",
            " produto: tensor([[[[-2.4791e-03, -7.5534e-06, -3.0021e-04,  1.1175e-03, -3.9601e-03],\n",
            "          [ 9.9990e-03,  5.2607e-03,  2.7927e-03,  1.2596e-03, -2.8206e-03],\n",
            "          [ 1.3442e-02,  2.8314e-02,  9.8969e-03,  2.6984e-03, -7.2805e-03],\n",
            "          [ 1.6108e-02,  5.2537e-03,  5.5737e-02,  4.0101e-02,  5.2894e-04],\n",
            "          [ 7.1315e-02,  2.7247e-03,  2.6134e-02,  1.9960e-03,  4.5043e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.0292e-02, -1.3755e-03,  5.4667e-02,  4.9332e-02,  1.3398e-02],\n",
            "          [ 1.0948e-02,  1.4037e-01,  8.6080e-02,  2.4889e-01,  1.8983e-01],\n",
            "          [ 9.5259e-02,  3.4705e-01,  2.4809e-01,  1.8743e-01,  5.5050e-01],\n",
            "          [ 7.1713e-02,  4.6573e-02,  5.6150e-01,  7.2032e-01,  1.5631e-02],\n",
            "          [ 1.5038e-01,  1.2787e-02,  1.6065e-01,  1.6176e-02,  1.3377e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2823]]],\n",
            "\n",
            "\n",
            "        [[[4.0697]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2823, 4.0697], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 6] = 0.2823360860347748\n",
            " somado na saída em [1, 1, 1, 6] = 4.069664478302002\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 7\n",
            " alvo do kernel em x: x[1,0,3:8, 21:26]\n",
            " \n",
            " tensor([[0.2921, 0.2099, 0.6127, 0.6638, 0.3955],\n",
            "        [0.4666, 0.4519, 0.2322, 0.6684, 0.9718],\n",
            "        [0.2521, 0.9368, 0.4429, 0.9389, 0.2883],\n",
            "        [0.9311, 0.0269, 0.9952, 0.3209, 0.8291],\n",
            "        [0.0327, 0.3735, 0.4311, 0.1305, 0.1297]])\n",
            " produto: tensor([[[[-2.5199e-03, -2.9974e-05, -4.1876e-04,  2.5398e-03, -7.4604e-03],\n",
            "          [ 6.2933e-03,  4.7907e-03,  4.2161e-03,  1.8045e-03, -6.0652e-03],\n",
            "          [ 5.2343e-03,  3.9957e-02,  1.4342e-02,  1.0050e-02, -2.2404e-03],\n",
            "          [ 5.7581e-02,  1.9775e-03,  8.5793e-02,  1.3819e-02,  1.6310e-02],\n",
            "          [ 3.0082e-03,  4.2277e-02,  4.5951e-02,  7.9545e-03,  1.5648e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.0954e-02, -5.4583e-03,  7.6256e-02,  1.1212e-01,  2.5240e-02],\n",
            "          [ 6.8907e-03,  1.2782e-01,  1.2995e-01,  3.5654e-01,  4.0820e-01],\n",
            "          [ 3.7094e-02,  4.8976e-01,  3.5952e-01,  6.9803e-01,  1.6940e-01],\n",
            "          [ 2.5635e-01,  1.7531e-02,  8.6429e-01,  2.4822e-01,  4.8197e-01],\n",
            "          [ 6.3434e-03,  1.9841e-01,  2.8246e-01,  6.4467e-02,  4.6472e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3467]]],\n",
            "\n",
            "\n",
            "        [[[5.4169]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3467, 5.4169], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 1, 7] = 0.34672972559928894\n",
            " somado na saída em [1, 1, 1, 7] = 5.416932582855225\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 0\n",
            " alvo do kernel em x: x[1,0,6:11, 0:5]\n",
            " \n",
            " tensor([[0.4327, 0.0011, 0.9412, 0.8398, 0.1461],\n",
            "        [0.1966, 0.4825, 0.1710, 0.5037, 0.6770],\n",
            "        [0.4174, 0.8195, 0.3565, 0.4427, 0.0149],\n",
            "        [0.3401, 0.4882, 0.6295, 0.9036, 0.2148],\n",
            "        [0.1904, 0.3839, 0.0140, 0.2860, 0.3996]])\n",
            " produto: tensor([[[[-3.7328e-03, -1.5244e-07, -6.4329e-04,  3.2128e-03, -2.7555e-03],\n",
            "          [ 2.6514e-03,  5.1152e-03,  3.1039e-03,  1.3597e-03, -4.2254e-03],\n",
            "          [ 8.6656e-03,  3.4957e-02,  1.1544e-02,  4.7380e-03, -1.1576e-04],\n",
            "          [ 2.1030e-02,  3.5901e-02,  5.4265e-02,  3.8917e-02,  4.2253e-03],\n",
            "          [ 1.7491e-02,  4.3462e-02,  1.4934e-03,  1.7433e-02,  4.8198e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.0668e-02, -2.7760e-05,  1.1714e-01,  1.4183e-01,  9.3226e-03],\n",
            "          [ 2.9031e-03,  1.3648e-01,  9.5671e-02,  2.6866e-01,  2.8438e-01],\n",
            "          [ 6.1411e-02,  4.2847e-01,  2.8936e-01,  3.2910e-01,  8.7529e-03],\n",
            "          [ 9.3626e-02,  3.1826e-01,  5.4667e-01,  6.9905e-01,  1.2486e-01],\n",
            "          [ 3.6884e-02,  2.0397e-01,  9.1801e-03,  1.4128e-01,  1.4314e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3029]]],\n",
            "\n",
            "\n",
            "        [[[4.4297]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3029, 4.4297], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 0] = 0.3029126524925232\n",
            " somado na saída em [1, 1, 2, 0] = 4.429712295532227\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 1\n",
            " alvo do kernel em x: x[1,0,6:11, 3:8]\n",
            " \n",
            " tensor([[0.8398, 0.1461, 0.1436, 0.3236, 0.9132],\n",
            "        [0.5037, 0.6770, 0.8126, 0.1988, 0.2089],\n",
            "        [0.4427, 0.0149, 0.3749, 0.0490, 0.6068],\n",
            "        [0.9036, 0.2148, 0.5307, 0.5864, 0.2143],\n",
            "        [0.2860, 0.3996, 0.2564, 0.6389, 0.9075]])\n",
            " produto: tensor([[[[-7.2446e-03, -2.0857e-05, -9.8124e-05,  1.2381e-03, -1.7225e-02],\n",
            "          [ 6.7933e-03,  7.1767e-03,  1.4752e-02,  5.3676e-04, -1.3037e-03],\n",
            "          [ 9.1907e-03,  6.3532e-04,  1.2139e-02,  5.2444e-04, -4.7162e-03],\n",
            "          [ 5.5882e-02,  1.5797e-02,  4.5747e-02,  2.5254e-02,  4.2151e-03],\n",
            "          [ 2.6273e-02,  4.5238e-02,  2.7326e-02,  3.8945e-02,  1.0946e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.1774e-01, -3.7980e-03,  1.7868e-02,  5.4655e-02,  5.8277e-02],\n",
            "          [ 7.4382e-03,  1.9149e-01,  4.5469e-01,  1.0606e-01,  8.7745e-02],\n",
            "          [ 6.5132e-02,  7.7871e-03,  3.0429e-01,  3.6427e-02,  3.5660e-01],\n",
            "          [ 2.4878e-01,  1.4004e-01,  4.6086e-01,  4.5362e-01,  1.2456e-01],\n",
            "          [ 5.5401e-02,  2.1231e-01,  1.6797e-01,  3.1563e-01,  3.2507e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3180]]],\n",
            "\n",
            "\n",
            "        [[[4.1312]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3180, 4.1312], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 1] = 0.31800025701522827\n",
            " somado na saída em [1, 1, 2, 1] = 4.13115930557251\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 2\n",
            " alvo do kernel em x: x[1,0,6:11, 6:11]\n",
            " \n",
            " tensor([[0.3236, 0.9132, 0.6247, 0.6025, 0.0708],\n",
            "        [0.1988, 0.2089, 0.3134, 0.4174, 0.3869],\n",
            "        [0.0490, 0.6068, 0.9069, 0.2559, 0.5098],\n",
            "        [0.5864, 0.2143, 0.6673, 0.0734, 0.0206],\n",
            "        [0.6389, 0.9075, 0.4351, 0.0353, 0.8588]])\n",
            " produto: tensor([[[[-2.7917e-03, -1.3038e-04, -4.2701e-04,  2.3050e-03, -1.3355e-03],\n",
            "          [ 2.6817e-03,  2.2144e-03,  5.6904e-03,  1.1268e-03, -2.4145e-03],\n",
            "          [ 1.0173e-03,  2.5883e-02,  2.9366e-02,  2.7385e-03, -3.9623e-03],\n",
            "          [ 3.6262e-02,  1.5759e-02,  5.7529e-02,  3.1594e-03,  4.0480e-04],\n",
            "          [ 5.8694e-02,  1.0274e-01,  4.6379e-02,  2.1524e-03,  1.0358e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.5373e-02, -2.3742e-02,  7.7757e-02,  1.0175e-01,  4.5181e-03],\n",
            "          [ 2.9363e-03,  5.9083e-02,  1.7539e-01,  2.2264e-01,  1.6250e-01],\n",
            "          [ 7.2093e-03,  3.1726e-01,  7.3612e-01,  1.9022e-01,  2.9960e-01],\n",
            "          [ 1.6144e-01,  1.3970e-01,  5.7955e-01,  5.6750e-02,  1.1962e-02],\n",
            "          [ 1.2377e-01,  4.8215e-01,  2.8509e-01,  1.7444e-02,  3.0760e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3954]]],\n",
            "\n",
            "\n",
            "        [[[4.4533]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3954, 4.4533], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 2] = 0.3953959345817566\n",
            " somado na saída em [1, 1, 2, 2] = 4.453337669372559\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 3\n",
            " alvo do kernel em x: x[1,0,6:11, 9:14]\n",
            " \n",
            " tensor([[0.6025, 0.0708, 0.0657, 0.2055, 0.1180],\n",
            "        [0.4174, 0.3869, 0.0729, 0.9582, 0.6338],\n",
            "        [0.2559, 0.5098, 0.4520, 0.8251, 0.9074],\n",
            "        [0.0734, 0.0206, 0.5577, 0.1386, 0.7554],\n",
            "        [0.0353, 0.8588, 0.4359, 0.8693, 0.3570]])\n",
            " produto: tensor([[[[-5.1975e-03, -1.0108e-05, -4.4891e-05,  7.8619e-04, -2.2257e-03],\n",
            "          [ 5.6296e-03,  4.1010e-03,  1.3240e-03,  2.5867e-03, -3.9557e-03],\n",
            "          [ 5.3122e-03,  2.1746e-02,  1.4635e-02,  8.8309e-03, -7.0519e-03],\n",
            "          [ 4.5365e-03,  1.5134e-03,  4.8076e-02,  5.9690e-03,  1.4860e-02],\n",
            "          [ 3.2439e-03,  9.7217e-02,  4.6455e-02,  5.2992e-02,  4.3055e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.4474e-02, -1.8407e-03,  8.1746e-03,  3.4706e-02,  7.5300e-03],\n",
            "          [ 6.1641e-03,  1.0942e-01,  4.0809e-02,  5.1111e-01,  2.6623e-01],\n",
            "          [ 3.7646e-02,  2.6654e-01,  3.6686e-01,  6.1338e-01,  5.3321e-01],\n",
            "          [ 2.0196e-02,  1.3416e-02,  4.8432e-01,  1.0722e-01,  4.3912e-01],\n",
            "          [ 6.8404e-03,  4.5625e-01,  2.8556e-01,  4.2946e-01,  1.2786e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3256]]],\n",
            "\n",
            "\n",
            "        [[[5.0857]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3256, 5.0857], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 3] = 0.3256334960460663\n",
            " somado na saída em [1, 1, 2, 3] = 5.0857133865356445\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 4\n",
            " alvo do kernel em x: x[1,0,6:11, 12:17]\n",
            " \n",
            " tensor([[0.2055, 0.1180, 0.3861, 0.1058, 0.2682],\n",
            "        [0.9582, 0.6338, 0.4486, 0.2879, 0.6869],\n",
            "        [0.8251, 0.9074, 0.0330, 0.3266, 0.3853],\n",
            "        [0.1386, 0.7554, 0.3916, 0.9823, 0.2754],\n",
            "        [0.8693, 0.3570, 0.1370, 0.3827, 0.5958]])\n",
            " produto: tensor([[[[-1.7728e-03, -1.6846e-05, -2.6388e-04,  4.0494e-04, -5.0587e-03],\n",
            "          [ 1.2924e-02,  6.7186e-03,  8.1442e-03,  7.7717e-04, -4.2873e-03],\n",
            "          [ 1.7130e-02,  3.8702e-02,  1.0680e-03,  3.4953e-03, -2.9947e-03],\n",
            "          [ 8.5709e-03,  5.5556e-02,  3.3761e-02,  4.2307e-02,  5.4178e-03],\n",
            "          [ 7.9864e-02,  4.0411e-02,  1.4606e-02,  2.3331e-02,  7.1867e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.8812e-02, -3.0677e-03,  4.8053e-02,  1.7876e-02,  1.7114e-02],\n",
            "          [ 1.4151e-02,  1.7926e-01,  2.5103e-01,  1.5356e-01,  2.8855e-01],\n",
            "          [ 1.2139e-01,  4.7438e-01,  2.6772e-02,  2.4278e-01,  2.2644e-01],\n",
            "          [ 3.8157e-02,  4.9250e-01,  3.4011e-01,  7.5993e-01,  1.6010e-01],\n",
            "          [ 1.6841e-01,  1.8965e-01,  8.9786e-02,  1.8908e-01,  2.1343e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3860]]],\n",
            "\n",
            "\n",
            "        [[[4.6706]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3860, 4.6706], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 4] = 0.38598039746284485\n",
            " somado na saída em [1, 1, 2, 4] = 4.670622825622559\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 5\n",
            " alvo do kernel em x: x[1,0,6:11, 15:20]\n",
            " \n",
            " tensor([[0.1058, 0.2682, 0.3871, 0.2605, 0.0714],\n",
            "        [0.2879, 0.6869, 0.9915, 0.7763, 0.0241],\n",
            "        [0.3266, 0.3853, 0.5837, 0.4391, 0.1506],\n",
            "        [0.9823, 0.2754, 0.2881, 0.7124, 0.5895],\n",
            "        [0.3827, 0.5958, 0.7276, 0.9324, 0.6346]])\n",
            " produto: tensor([[[[-9.1310e-04, -3.8289e-05, -2.6458e-04,  9.9654e-04, -1.3475e-03],\n",
            "          [ 3.8828e-03,  7.2818e-03,  1.8000e-02,  2.0956e-03, -1.5022e-04],\n",
            "          [ 6.7801e-03,  1.6436e-02,  1.8899e-02,  4.7003e-03, -1.1701e-03],\n",
            "          [ 6.0748e-02,  2.0255e-02,  2.4834e-02,  3.0681e-02,  1.1596e-02],\n",
            "          [ 3.5161e-02,  6.7453e-02,  7.7554e-02,  5.6836e-02,  7.6537e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.4840e-02, -6.9724e-03,  4.8179e-02,  4.3992e-02,  4.5589e-03],\n",
            "          [ 4.2514e-03,  1.9429e-01,  5.5481e-01,  4.1407e-01,  1.0110e-02],\n",
            "          [ 4.8049e-02,  2.0145e-01,  4.7374e-01,  3.2648e-01,  8.8474e-02],\n",
            "          [ 2.7045e-01,  1.7956e-01,  2.5018e-01,  5.5110e-01,  3.4268e-01],\n",
            "          [ 7.4145e-02,  3.1657e-01,  4.7673e-01,  4.6062e-01,  2.2730e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4680]]],\n",
            "\n",
            "\n",
            "        [[[5.5400]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4680, 5.5400], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 5] = 0.4679621458053589\n",
            " somado na saída em [1, 1, 2, 5] = 5.539977550506592\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 6\n",
            " alvo do kernel em x: x[1,0,6:11, 18:23]\n",
            " \n",
            " tensor([[0.2605, 0.0714, 0.6465, 0.9311, 0.0269],\n",
            "        [0.7763, 0.0241, 0.2452, 0.0327, 0.3735],\n",
            "        [0.4391, 0.1506, 0.8370, 0.4837, 0.2002],\n",
            "        [0.7124, 0.5895, 0.6836, 0.6448, 0.3651],\n",
            "        [0.9324, 0.6346, 0.6749, 0.3059, 0.4694]])\n",
            " produto: tensor([[[[-2.2471e-03, -1.0199e-05, -4.4190e-04,  3.5623e-03, -5.0722e-04],\n",
            "          [ 1.0470e-02,  2.5515e-04,  4.4516e-03,  8.8396e-05, -2.3309e-03],\n",
            "          [ 9.1176e-03,  6.4217e-03,  2.7101e-02,  5.1767e-03, -1.5560e-03],\n",
            "          [ 4.4054e-02,  4.3355e-02,  5.8934e-02,  2.7771e-02,  7.1815e-03],\n",
            "          [ 8.5658e-02,  7.1837e-02,  7.1937e-02,  1.8650e-02,  5.6616e-03]]],\n",
            "\n",
            "\n",
            "        [[[-3.6521e-02, -1.8573e-03,  8.0469e-02,  1.5726e-01,  1.7160e-03],\n",
            "          [ 1.1464e-02,  6.8078e-03,  1.3721e-01,  1.7466e-02,  1.5687e-01],\n",
            "          [ 6.4614e-02,  7.8712e-02,  6.7935e-01,  3.5957e-01,  1.1765e-01],\n",
            "          [ 1.9613e-01,  3.8434e-01,  5.9371e-01,  4.9884e-01,  2.1222e-01],\n",
            "          [ 1.8063e-01,  3.3714e-01,  4.4220e-01,  1.5115e-01,  1.6814e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4946]]],\n",
            "\n",
            "\n",
            "        [[[4.9953]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4946, 4.9953], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 6] = 0.49459126591682434\n",
            " somado na saída em [1, 1, 2, 6] = 4.995268821716309\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 2, 7\n",
            " alvo do kernel em x: x[1,0,6:11, 21:26]\n",
            " \n",
            " tensor([[0.9311, 0.0269, 0.9952, 0.3209, 0.8291],\n",
            "        [0.0327, 0.3735, 0.4311, 0.1305, 0.1297],\n",
            "        [0.4837, 0.2002, 0.0849, 0.1045, 0.0386],\n",
            "        [0.6448, 0.3651, 0.8036, 0.7954, 0.2887],\n",
            "        [0.3059, 0.4694, 0.6436, 0.5481, 0.8730]])\n",
            " produto: tensor([[[[-8.0326e-03, -3.8391e-06, -6.8019e-04,  1.2276e-03, -1.5640e-02],\n",
            "          [ 4.4164e-04,  3.9589e-03,  7.8271e-03,  3.5228e-04, -8.0976e-04],\n",
            "          [ 1.0042e-02,  8.5397e-03,  2.7477e-03,  1.1182e-03, -2.9975e-04],\n",
            "          [ 3.9877e-02,  2.6849e-02,  6.9275e-02,  3.4257e-02,  5.6788e-03],\n",
            "          [ 2.8107e-02,  5.3140e-02,  6.8592e-02,  3.3410e-02,  1.0530e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.3055e-01, -6.9910e-04,  1.2386e-01,  5.4191e-02,  5.2913e-02],\n",
            "          [ 4.8356e-04,  1.0563e-01,  2.4125e-01,  6.9606e-02,  5.4499e-02],\n",
            "          [ 7.1162e-02,  1.0467e-01,  6.8877e-02,  7.7670e-02,  2.2665e-02],\n",
            "          [ 1.7753e-01,  2.3802e-01,  6.9789e-01,  6.1534e-01,  1.6781e-01],\n",
            "          [ 5.9270e-02,  2.4939e-01,  4.2164e-01,  2.7077e-01,  3.1271e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3805]]],\n",
            "\n",
            "\n",
            "        [[[4.1266]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3805, 4.1266], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 2, 7] = 0.3805054724216461\n",
            " somado na saída em [1, 1, 2, 7] = 4.126605033874512\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 0\n",
            " alvo do kernel em x: x[1,0,9:14, 0:5]\n",
            " \n",
            " tensor([[0.3401, 0.4882, 0.6295, 0.9036, 0.2148],\n",
            "        [0.1904, 0.3839, 0.0140, 0.2860, 0.3996],\n",
            "        [0.9328, 0.0920, 0.8339, 0.7615, 0.2345],\n",
            "        [0.6679, 0.2251, 0.0826, 0.3848, 0.2431],\n",
            "        [0.7820, 0.2238, 0.3555, 0.8934, 0.0381]])\n",
            " produto: tensor([[[[-2.9337e-03, -6.9698e-05, -4.3023e-04,  3.4571e-03, -4.0518e-03],\n",
            "          [ 2.5679e-03,  4.0699e-03,  2.5438e-04,  7.7202e-04, -2.4941e-03],\n",
            "          [ 1.9367e-02,  3.9248e-03,  2.7003e-02,  8.1504e-03, -1.8229e-03],\n",
            "          [ 4.1306e-02,  1.6556e-02,  7.1235e-03,  1.6571e-02,  4.7811e-03],\n",
            "          [ 7.1839e-02,  2.5341e-02,  3.7896e-02,  5.4460e-02,  4.5933e-04]]],\n",
            "\n",
            "\n",
            "        [[[-4.7681e-02, -1.2692e-02,  7.8343e-02,  1.5262e-01,  1.3708e-02],\n",
            "          [ 2.8117e-03,  1.0859e-01,  7.8407e-03,  1.5254e-01,  1.6786e-01],\n",
            "          [ 1.3725e-01,  4.8107e-02,  6.7688e-01,  5.6612e-01,  1.3783e-01],\n",
            "          [ 1.8389e-01,  1.4677e-01,  7.1763e-02,  2.9766e-01,  1.4129e-01],\n",
            "          [ 1.5149e-01,  1.1893e-01,  2.3295e-01,  4.4136e-01,  1.3641e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3341]]],\n",
            "\n",
            "\n",
            "        [[[3.9899]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3341, 3.9899], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 0] = 0.33409637212753296\n",
            " somado na saída em [1, 1, 3, 0] = 3.9898605346679688\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 1\n",
            " alvo do kernel em x: x[1,0,9:14, 3:8]\n",
            " \n",
            " tensor([[0.9036, 0.2148, 0.5307, 0.5864, 0.2143],\n",
            "        [0.2860, 0.3996, 0.2564, 0.6389, 0.9075],\n",
            "        [0.7615, 0.2345, 0.8163, 0.5386, 0.5673],\n",
            "        [0.3848, 0.2431, 0.0771, 0.2051, 0.5077],\n",
            "        [0.8934, 0.0381, 0.7379, 0.0178, 0.3336]])\n",
            " produto: tensor([[[[-7.7955e-03, -3.0668e-05, -3.6270e-04,  2.2434e-03, -4.0420e-03],\n",
            "          [ 3.8571e-03,  4.2362e-03,  4.6545e-03,  1.7247e-03, -5.6642e-03],\n",
            "          [ 1.5810e-02,  1.0004e-02,  2.6433e-02,  5.7643e-03, -4.4093e-03],\n",
            "          [ 2.3794e-02,  1.7875e-02,  6.6509e-03,  8.8346e-03,  9.9878e-03],\n",
            "          [ 8.2076e-02,  4.3112e-03,  7.8650e-02,  1.0849e-03,  4.0241e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2670e-01, -5.5846e-03,  6.6047e-02,  9.9033e-02,  1.3675e-02],\n",
            "          [ 4.2233e-03,  1.1303e-01,  1.4347e-01,  3.4079e-01,  3.8122e-01],\n",
            "          [ 1.1204e-01,  1.2262e-01,  6.6260e-01,  4.0038e-01,  3.3340e-01],\n",
            "          [ 1.0593e-01,  1.5846e-01,  6.7002e-02,  1.5869e-01,  2.9515e-01],\n",
            "          [ 1.7307e-01,  2.0233e-02,  4.8346e-01,  8.7928e-03,  1.1951e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2897]]],\n",
            "\n",
            "\n",
            "        [[[4.2506]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2897, 4.2506], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 1] = 0.2897126078605652\n",
            " somado na saída em [1, 1, 3, 1] = 4.250550270080566\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 2\n",
            " alvo do kernel em x: x[1,0,9:14, 6:11]\n",
            " \n",
            " tensor([[0.5864, 0.2143, 0.6673, 0.0734, 0.0206],\n",
            "        [0.6389, 0.9075, 0.4351, 0.0353, 0.8588],\n",
            "        [0.5386, 0.5673, 0.3147, 0.4351, 0.0392],\n",
            "        [0.2051, 0.5077, 0.4849, 0.2854, 0.3406],\n",
            "        [0.0178, 0.3336, 0.5277, 0.4599, 0.6675]])\n",
            " produto: tensor([[[[-5.0585e-03, -3.0594e-05, -4.5611e-04,  2.8065e-04, -3.8817e-04],\n",
            "          [ 8.6170e-03,  9.6205e-03,  7.8999e-03,  9.5323e-05, -5.3599e-03],\n",
            "          [ 1.1182e-02,  2.4199e-02,  1.0192e-02,  4.6568e-03, -3.0447e-04],\n",
            "          [ 1.2686e-02,  3.7341e-02,  4.1802e-02,  1.2291e-02,  6.7005e-03],\n",
            "          [ 1.6351e-03,  3.7770e-02,  5.6249e-02,  2.8036e-02,  8.0508e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.2215e-02, -5.5711e-03,  8.3056e-02,  1.2389e-02,  1.3133e-03],\n",
            "          [ 9.4350e-03,  2.5669e-01,  2.4350e-01,  1.8835e-02,  3.6074e-01],\n",
            "          [ 7.9240e-02,  2.9661e-01,  2.5547e-01,  3.2346e-01,  2.3022e-02],\n",
            "          [ 5.6476e-02,  3.3103e-01,  4.2112e-01,  2.2078e-01,  1.9800e-01],\n",
            "          [ 3.4480e-03,  1.7726e-01,  3.4576e-01,  2.2722e-01,  2.3909e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3077]]],\n",
            "\n",
            "\n",
            "        [[[4.0961]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3077, 4.0961], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 2] = 0.30770576000213623\n",
            " somado na saída em [1, 1, 3, 2] = 4.096146583557129\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 3\n",
            " alvo do kernel em x: x[1,0,9:14, 9:14]\n",
            " \n",
            " tensor([[0.0734, 0.0206, 0.5577, 0.1386, 0.7554],\n",
            "        [0.0353, 0.8588, 0.4359, 0.8693, 0.3570],\n",
            "        [0.4351, 0.0392, 0.8008, 0.0077, 0.7662],\n",
            "        [0.2854, 0.3406, 0.5900, 0.9263, 0.2351],\n",
            "        [0.4599, 0.6675, 0.6438, 0.2685, 0.5808]])\n",
            " produto: tensor([[[[-6.3284e-04, -2.9381e-06, -3.8116e-04,  5.3024e-04, -1.4250e-02],\n",
            "          [ 4.7624e-04,  9.1036e-03,  7.9129e-03,  2.3468e-03, -2.2280e-03],\n",
            "          [ 9.0332e-03,  1.6710e-03,  2.5930e-02,  8.2169e-05, -5.9547e-03],\n",
            "          [ 1.7649e-02,  2.5051e-02,  5.0859e-02,  3.9893e-02,  4.6256e-03],\n",
            "          [ 4.2253e-02,  7.5564e-02,  6.8615e-02,  1.6368e-02,  7.0054e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0285e-02, -5.3502e-04,  6.9408e-02,  2.3408e-02,  4.8209e-02],\n",
            "          [ 5.2145e-04,  2.4290e-01,  2.4390e-01,  4.6370e-01,  1.4995e-01],\n",
            "          [ 6.4016e-02,  2.0482e-02,  6.4998e-01,  5.7074e-03,  4.5025e-01],\n",
            "          [ 7.8571e-02,  2.2207e-01,  5.1236e-01,  7.1658e-01,  1.3669e-01],\n",
            "          [ 8.9100e-02,  3.5463e-01,  4.2178e-01,  1.3265e-01,  2.0804e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3815]]],\n",
            "\n",
            "\n",
            "        [[[5.2941]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3815, 5.2941], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 3] = 0.3815206289291382\n",
            " somado na saída em [1, 1, 3, 3] = 5.294093608856201\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 4\n",
            " alvo do kernel em x: x[1,0,9:14, 12:17]\n",
            " \n",
            " tensor([[0.1386, 0.7554, 0.3916, 0.9823, 0.2754],\n",
            "        [0.8693, 0.3570, 0.1370, 0.3827, 0.5958],\n",
            "        [0.0077, 0.7662, 0.9468, 0.9483, 0.9652],\n",
            "        [0.9263, 0.2351, 0.8221, 0.1634, 0.3905],\n",
            "        [0.2685, 0.5808, 0.8092, 0.1689, 0.7309]])\n",
            " produto: tensor([[[[-1.1956e-03, -1.0786e-04, -2.6767e-04,  3.7582e-03, -5.1953e-03],\n",
            "          [ 1.1725e-02,  3.7842e-03,  2.4880e-03,  1.0332e-03, -3.7190e-03],\n",
            "          [ 1.5939e-04,  3.2681e-02,  3.0656e-02,  1.0150e-02, -7.5015e-03],\n",
            "          [ 5.7283e-02,  1.7294e-02,  7.0876e-02,  7.0371e-03,  7.6820e-03],\n",
            "          [ 2.4669e-02,  6.5752e-02,  8.6246e-02,  1.0294e-02,  8.8152e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.9432e-02, -1.9640e-02,  4.8742e-02,  1.6591e-01,  1.7577e-02],\n",
            "          [ 1.2838e-02,  1.0097e-01,  7.6685e-02,  2.0415e-01,  2.5029e-01],\n",
            "          [ 1.1296e-03,  4.0057e-01,  7.6846e-01,  7.0504e-01,  5.6721e-01],\n",
            "          [ 2.5502e-01,  1.5331e-01,  7.1401e-01,  1.2640e-01,  2.2701e-01],\n",
            "          [ 5.2019e-02,  3.0858e-01,  5.3016e-01,  8.3429e-02,  2.6179e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4344]]],\n",
            "\n",
            "\n",
            "        [[[5.9922]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4344, 5.9922], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 4] = 0.43439561128616333\n",
            " somado na saída em [1, 1, 3, 4] = 5.9922261238098145\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 5\n",
            " alvo do kernel em x: x[1,0,9:14, 15:20]\n",
            " \n",
            " tensor([[0.9823, 0.2754, 0.2881, 0.7124, 0.5895],\n",
            "        [0.3827, 0.5958, 0.7276, 0.9324, 0.6346],\n",
            "        [0.9483, 0.9652, 0.3650, 0.0692, 0.6211],\n",
            "        [0.1634, 0.3905, 0.9530, 0.3082, 0.1318],\n",
            "        [0.1689, 0.7309, 0.1570, 0.4448, 0.8679]])\n",
            " produto: tensor([[[[-8.4743e-03, -3.9323e-05, -1.9689e-04,  2.7254e-03, -1.1120e-02],\n",
            "          [ 5.1621e-03,  6.3165e-03,  1.3210e-02,  2.5171e-03, -3.9606e-03],\n",
            "          [ 1.9690e-02,  4.1170e-02,  1.1818e-02,  7.4041e-04, -4.8275e-03],\n",
            "          [ 1.0105e-02,  2.8721e-02,  8.2160e-02,  1.3276e-02,  2.5935e-03],\n",
            "          [ 1.5515e-02,  8.2739e-02,  1.6731e-02,  2.7114e-02,  1.0467e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.3773e-01, -7.1607e-03,  3.5854e-02,  1.2031e-01,  3.7622e-02],\n",
            "          [ 5.6522e-03,  1.6854e-01,  4.0717e-01,  4.9734e-01,  2.6656e-01],\n",
            "          [ 1.3953e-01,  5.0463e-01,  2.9624e-01,  5.1428e-02,  3.6502e-01],\n",
            "          [ 4.4985e-02,  2.5461e-01,  8.2769e-01,  2.3847e-01,  7.6640e-02],\n",
            "          [ 3.2716e-02,  3.8830e-01,  1.0284e-01,  2.1974e-01,  3.1086e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3642]]],\n",
            "\n",
            "\n",
            "        [[[5.2479]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3642, 5.2479], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 5] = 0.36415135860443115\n",
            " somado na saída em [1, 1, 3, 5] = 5.247864723205566\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 6\n",
            " alvo do kernel em x: x[1,0,9:14, 18:23]\n",
            " \n",
            " tensor([[0.7124, 0.5895, 0.6836, 0.6448, 0.3651],\n",
            "        [0.9324, 0.6346, 0.6749, 0.3059, 0.4694],\n",
            "        [0.0692, 0.6211, 0.6352, 0.6405, 0.1844],\n",
            "        [0.3082, 0.1318, 0.2593, 0.9461, 0.0547],\n",
            "        [0.4448, 0.8679, 0.8502, 0.6489, 0.4459]])\n",
            " produto: tensor([[[[-6.1456e-03, -8.4169e-05, -4.6724e-04,  2.4670e-03, -6.8866e-03],\n",
            "          [ 1.2576e-02,  6.7270e-03,  1.2253e-02,  8.2594e-04, -2.9298e-03],\n",
            "          [ 1.4362e-03,  2.6494e-02,  2.0566e-02,  6.8557e-03, -1.4331e-03],\n",
            "          [ 1.9063e-02,  9.6962e-03,  2.2357e-02,  4.0748e-02,  1.0755e-03],\n",
            "          [ 4.0864e-02,  9.8246e-02,  9.0619e-02,  3.9556e-02,  5.3780e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.9882e-02, -1.5327e-02,  8.5084e-02,  1.0891e-01,  2.3299e-02],\n",
            "          [ 1.3769e-02,  1.7949e-01,  3.7768e-01,  1.6320e-01,  1.9718e-01],\n",
            "          [ 1.0178e-02,  3.2474e-01,  5.1554e-01,  4.7619e-01,  1.0836e-01],\n",
            "          [ 8.4867e-02,  8.5956e-02,  2.2522e-01,  7.3193e-01,  3.1783e-02],\n",
            "          [ 8.6170e-02,  4.6108e-01,  5.5704e-01,  3.2057e-01,  1.5972e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4399]]],\n",
            "\n",
            "\n",
            "        [[[5.2127]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4399, 5.2127], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 6] = 0.4398571252822876\n",
            " somado na saída em [1, 1, 3, 6] = 5.212740898132324\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 3, 7\n",
            " alvo do kernel em x: x[1,0,9:14, 21:26]\n",
            " \n",
            " tensor([[0.6448, 0.3651, 0.8036, 0.7954, 0.2887],\n",
            "        [0.3059, 0.4694, 0.6436, 0.5481, 0.8730],\n",
            "        [0.6405, 0.1844, 0.8982, 0.8400, 0.3688],\n",
            "        [0.9461, 0.0547, 0.3743, 0.1900, 0.5323],\n",
            "        [0.6489, 0.4459, 0.0609, 0.9443, 0.2344]])\n",
            " produto: tensor([[[[-5.5628e-03, -5.2125e-05, -5.4924e-04,  3.0431e-03, -5.4456e-03],\n",
            "          [ 4.1265e-03,  4.9761e-03,  1.1684e-02,  1.4796e-03, -5.4490e-03],\n",
            "          [ 1.3299e-02,  7.8651e-03,  2.9085e-02,  8.9910e-03, -2.8666e-03],\n",
            "          [ 5.8509e-02,  4.0210e-03,  3.2265e-02,  8.1826e-03,  1.0472e-02],\n",
            "          [ 5.9614e-02,  5.0478e-02,  6.4907e-03,  5.7561e-02,  2.8268e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.0411e-02, -9.4919e-03,  1.0001e-01,  1.3434e-01,  1.8424e-02],\n",
            "          [ 4.5182e-03,  1.3277e-01,  3.6012e-01,  2.9235e-01,  3.6673e-01],\n",
            "          [ 9.4243e-02,  9.6403e-02,  7.2908e-01,  6.2450e-01,  2.1675e-01],\n",
            "          [ 2.6048e-01,  3.5646e-02,  3.2504e-01,  1.4698e-01,  3.0945e-01],\n",
            "          [ 1.2571e-01,  2.3690e-01,  3.9898e-02,  4.6649e-01,  8.3949e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3550]]],\n",
            "\n",
            "\n",
            "        [[[5.1009]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3550, 5.1009], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 3, 7] = 0.3550436198711395\n",
            " somado na saída em [1, 1, 3, 7] = 5.100883960723877\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 0\n",
            " alvo do kernel em x: x[1,0,12:17, 0:5]\n",
            " \n",
            " tensor([[0.6679, 0.2251, 0.0826, 0.3848, 0.2431],\n",
            "        [0.7820, 0.2238, 0.3555, 0.8934, 0.0381],\n",
            "        [0.8101, 0.2875, 0.3874, 0.5917, 0.1821],\n",
            "        [0.8265, 0.9085, 0.3120, 0.1799, 0.9101],\n",
            "        [0.0365, 0.4925, 0.1533, 0.1850, 0.6348]])\n",
            " produto: tensor([[[[-5.7622e-03, -3.2142e-05, -5.6477e-05,  1.4720e-03, -4.5848e-03],\n",
            "          [ 1.0547e-02,  2.3730e-03,  6.4549e-03,  2.4118e-03, -2.3769e-04],\n",
            "          [ 1.6819e-02,  1.2265e-02,  1.2544e-02,  6.3327e-03, -1.4155e-03],\n",
            "          [ 5.1114e-02,  6.6817e-02,  2.6899e-02,  7.7495e-03,  1.7903e-02],\n",
            "          [ 3.3549e-03,  5.5752e-02,  1.6340e-02,  1.1278e-02,  7.6564e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.3652e-02, -5.8530e-03,  1.0284e-02,  6.4983e-02,  1.5511e-02],\n",
            "          [ 1.1548e-02,  6.3315e-02,  1.9896e-01,  4.7655e-01,  1.5997e-02],\n",
            "          [ 1.1919e-01,  1.5033e-01,  3.1443e-01,  4.3986e-01,  1.0703e-01],\n",
            "          [ 2.2756e-01,  5.9232e-01,  2.7099e-01,  1.3920e-01,  5.2904e-01],\n",
            "          [ 7.0745e-03,  2.6165e-01,  1.0045e-01,  9.1399e-02,  2.2738e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3240]]],\n",
            "\n",
            "\n",
            "        [[[4.3355]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3240, 4.3355], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 0] = 0.32399386167526245\n",
            " somado na saída em [1, 1, 4, 0] = 4.335531234741211\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 1\n",
            " alvo do kernel em x: x[1,0,12:17, 3:8]\n",
            " \n",
            " tensor([[0.3848, 0.2431, 0.0771, 0.2051, 0.5077],\n",
            "        [0.8934, 0.0381, 0.7379, 0.0178, 0.3336],\n",
            "        [0.5917, 0.1821, 0.3478, 0.3519, 0.8559],\n",
            "        [0.1799, 0.9101, 0.6591, 0.5461, 0.4248],\n",
            "        [0.1850, 0.6348, 0.0123, 0.7394, 0.8368]])\n",
            " produto: tensor([[[[-3.3193e-03, -3.4702e-05, -5.2730e-05,  7.8480e-04, -9.5777e-03],\n",
            "          [ 1.2050e-02,  4.0372e-04,  1.3397e-02,  4.8048e-05, -2.0824e-03],\n",
            "          [ 1.2284e-02,  7.7684e-03,  1.1262e-02,  3.7665e-03, -6.6521e-03],\n",
            "          [ 1.1128e-02,  6.6932e-02,  5.6820e-02,  2.3518e-02,  8.3567e-03],\n",
            "          [ 1.6997e-02,  7.1862e-02,  1.3148e-03,  4.5075e-02,  1.0093e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.3948e-02, -6.3193e-03,  9.6021e-03,  3.4645e-02,  3.2403e-02],\n",
            "          [ 1.3194e-02,  1.0772e-02,  4.1292e-01,  9.4937e-03,  1.4015e-01],\n",
            "          [ 8.7053e-02,  9.5218e-02,  2.8230e-01,  2.6162e-01,  5.0298e-01],\n",
            "          [ 4.9539e-02,  5.9335e-01,  5.7241e-01,  4.2244e-01,  2.4695e-01],\n",
            "          [ 3.5841e-02,  3.3726e-01,  8.0821e-03,  3.6530e-01,  2.9974e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3521]]],\n",
            "\n",
            "\n",
            "        [[[4.7630]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3521, 4.7630], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 1] = 0.35214152932167053\n",
            " somado na saída em [1, 1, 4, 1] = 4.763003349304199\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 2\n",
            " alvo do kernel em x: x[1,0,12:17, 6:11]\n",
            " \n",
            " tensor([[0.2051, 0.5077, 0.4849, 0.2854, 0.3406],\n",
            "        [0.0178, 0.3336, 0.5277, 0.4599, 0.6675],\n",
            "        [0.3519, 0.8559, 0.1255, 0.6026, 0.8938],\n",
            "        [0.5461, 0.4248, 0.5401, 0.7634, 0.6491],\n",
            "        [0.7394, 0.8368, 0.4989, 0.9842, 0.4907]])\n",
            " produto: tensor([[[[-1.7696e-03, -7.2493e-05, -3.3142e-04,  1.0918e-03, -6.4253e-03],\n",
            "          [ 2.4005e-04,  3.5369e-03,  9.5810e-03,  1.2416e-03, -4.1661e-03],\n",
            "          [ 7.3062e-03,  3.6508e-02,  4.0626e-03,  6.4502e-03, -6.9465e-03],\n",
            "          [ 3.3770e-02,  3.1243e-02,  4.6563e-02,  3.2879e-02,  1.2769e-02],\n",
            "          [ 6.7932e-02,  9.4733e-02,  5.3172e-02,  5.9994e-02,  5.9187e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.8761e-02, -1.3201e-02,  6.0351e-02,  4.8199e-02,  2.1738e-02],\n",
            "          [ 2.6284e-04,  9.4369e-02,  2.9531e-01,  2.4533e-01,  2.8039e-01],\n",
            "          [ 5.1777e-02,  4.4748e-01,  1.0184e-01,  4.4803e-01,  5.2524e-01],\n",
            "          [ 1.5034e-01,  2.7696e-01,  4.6908e-01,  5.9060e-01,  3.7733e-01],\n",
            "          [ 1.4325e-01,  4.4459e-01,  3.2685e-01,  4.8621e-01,  1.7577e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4893]]],\n",
            "\n",
            "\n",
            "        [[[6.0193]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4893, 6.0193], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 2] = 0.4892798364162445\n",
            " somado na saída em [1, 1, 4, 2] = 6.019341468811035\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 3\n",
            " alvo do kernel em x: x[1,0,12:17, 9:14]\n",
            " \n",
            " tensor([[0.2854, 0.3406, 0.5900, 0.9263, 0.2351],\n",
            "        [0.4599, 0.6675, 0.6438, 0.2685, 0.5808],\n",
            "        [0.6026, 0.8938, 0.6516, 0.4874, 0.5464],\n",
            "        [0.7634, 0.6491, 0.3994, 0.5752, 0.1820],\n",
            "        [0.9842, 0.4907, 0.2652, 0.7893, 0.3663]])\n",
            " produto: tensor([[[[-2.4620e-03, -4.8633e-05, -4.0323e-04,  3.5438e-03, -4.4356e-03],\n",
            "          [ 6.2033e-03,  7.0760e-03,  1.1687e-02,  7.2489e-04, -3.6251e-03],\n",
            "          [ 1.2512e-02,  3.8124e-02,  2.1100e-02,  5.2169e-03, -4.2464e-03],\n",
            "          [ 4.7212e-02,  4.7739e-02,  3.4432e-02,  2.4774e-02,  3.5797e-03],\n",
            "          [ 9.0417e-02,  5.5552e-02,  2.8264e-02,  4.8116e-02,  4.4175e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.0014e-02, -8.8561e-03,  7.3427e-02,  1.5644e-01,  1.5007e-02],\n",
            "          [ 6.7922e-03,  1.8880e-01,  3.6024e-01,  1.4323e-01,  2.4398e-01],\n",
            "          [ 8.8669e-02,  4.6729e-01,  5.2890e-01,  3.6236e-01,  3.2108e-01],\n",
            "          [ 2.1018e-01,  4.2320e-01,  3.4687e-01,  4.4500e-01,  1.0578e-01],\n",
            "          [ 1.9066e-01,  2.6071e-01,  1.7374e-01,  3.8995e-01,  1.3119e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4755]]],\n",
            "\n",
            "\n",
            "        [[[5.5846]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4755, 5.5846], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 3] = 0.4754698574542999\n",
            " somado na saída em [1, 1, 4, 3] = 5.584640026092529\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 4\n",
            " alvo do kernel em x: x[1,0,12:17, 12:17]\n",
            " \n",
            " tensor([[0.9263, 0.2351, 0.8221, 0.1634, 0.3905],\n",
            "        [0.2685, 0.5808, 0.8092, 0.1689, 0.7309],\n",
            "        [0.4874, 0.5464, 0.3860, 0.1600, 0.5672],\n",
            "        [0.5752, 0.1820, 0.5232, 0.9686, 0.1312],\n",
            "        [0.7893, 0.3663, 0.9927, 0.0080, 0.4963]])\n",
            " produto: tensor([[[[-7.9909e-03, -3.3573e-05, -5.6193e-04,  6.2512e-04, -7.3665e-03],\n",
            "          [ 3.6216e-03,  6.1572e-03,  1.4691e-02,  4.5590e-04, -4.5617e-03],\n",
            "          [ 1.0120e-02,  2.3305e-02,  1.2499e-02,  1.7122e-03, -4.4081e-03],\n",
            "          [ 3.5573e-02,  1.3383e-02,  4.5105e-02,  4.1718e-02,  2.5818e-03],\n",
            "          [ 7.2516e-02,  4.1462e-02,  1.0580e-01,  4.8995e-04,  5.9860e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2987e-01, -6.1137e-03,  1.0233e-01,  2.7596e-02,  2.4922e-02],\n",
            "          [ 3.9655e-03,  1.6428e-01,  4.5280e-01,  9.0080e-02,  3.0701e-01],\n",
            "          [ 7.1714e-02,  2.8565e-01,  3.1331e-01,  1.1893e-01,  3.3331e-01],\n",
            "          [ 1.5837e-01,  1.1864e-01,  4.5439e-01,  7.4937e-01,  7.6293e-02],\n",
            "          [ 1.5291e-01,  1.9459e-01,  6.5037e-01,  3.9707e-03,  1.7777e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4129]]],\n",
            "\n",
            "\n",
            "        [[[4.8966]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4129, 4.8966], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 4] = 0.4128810465335846\n",
            " somado na saída em [1, 1, 4, 4] = 4.89659309387207\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 5\n",
            " alvo do kernel em x: x[1,0,12:17, 15:20]\n",
            " \n",
            " tensor([[0.1634, 0.3905, 0.9530, 0.3082, 0.1318],\n",
            "        [0.1689, 0.7309, 0.1570, 0.4448, 0.8679],\n",
            "        [0.1600, 0.5672, 0.0375, 0.2662, 0.2619],\n",
            "        [0.9686, 0.1312, 0.1912, 0.7674, 0.0146],\n",
            "        [0.0080, 0.4963, 0.9293, 0.3773, 0.6408]])\n",
            " produto: tensor([[[[-1.4096e-03, -5.5757e-05, -6.5139e-04,  1.1793e-03, -2.4870e-03],\n",
            "          [ 2.2777e-03,  7.7479e-03,  2.8498e-03,  1.2008e-03, -5.4167e-03],\n",
            "          [ 3.3214e-03,  2.4193e-02,  1.2129e-03,  2.8495e-03, -2.0354e-03],\n",
            "          [ 5.9904e-02,  9.6524e-03,  1.6481e-02,  3.3051e-02,  2.8704e-04],\n",
            "          [ 7.3840e-04,  5.6185e-02,  9.9046e-02,  2.3002e-02,  7.7283e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.2909e-02, -1.0153e-02,  1.1862e-01,  5.2062e-02,  8.4140e-03],\n",
            "          [ 2.4940e-03,  2.0673e-01,  8.7838e-02,  2.3726e-01,  3.6455e-01],\n",
            "          [ 2.3537e-02,  2.9653e-01,  3.0403e-02,  1.9792e-01,  1.5390e-01],\n",
            "          [ 2.6669e-01,  8.5567e-02,  1.6603e-01,  5.9368e-01,  8.4823e-03],\n",
            "          [ 1.5571e-03,  2.6368e-01,  6.0884e-01,  1.8641e-01,  2.2951e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3408]]],\n",
            "\n",
            "\n",
            "        [[[4.1577]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3408, 4.1577], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 5] = 0.3408498764038086\n",
            " somado na saída em [1, 1, 4, 5] = 4.157650947570801\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 6\n",
            " alvo do kernel em x: x[1,0,12:17, 18:23]\n",
            " \n",
            " tensor([[0.3082, 0.1318, 0.2593, 0.9461, 0.0547],\n",
            "        [0.4448, 0.8679, 0.8502, 0.6489, 0.4459],\n",
            "        [0.2662, 0.2619, 0.5162, 0.5395, 0.9904],\n",
            "        [0.7674, 0.0146, 0.8622, 0.1116, 0.9300],\n",
            "        [0.3773, 0.6408, 0.7394, 0.8035, 0.5533]])\n",
            " produto: tensor([[[[-2.6593e-03, -1.8824e-05, -1.7725e-04,  3.6197e-03, -1.0314e-03],\n",
            "          [ 5.9993e-03,  9.2000e-03,  1.5436e-02,  1.7518e-03, -2.7830e-03],\n",
            "          [ 5.5274e-03,  1.1171e-02,  1.6716e-02,  5.7739e-03, -7.6974e-03],\n",
            "          [ 4.7458e-02,  1.0732e-03,  7.4333e-02,  4.8075e-03,  1.8294e-02],\n",
            "          [ 3.4666e-02,  7.2537e-02,  7.8805e-02,  4.8980e-02,  6.6738e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.3220e-02, -3.4278e-03,  3.2277e-02,  1.5979e-01,  3.4893e-03],\n",
            "          [ 6.5688e-03,  2.4547e-01,  4.7576e-01,  3.4613e-01,  1.8730e-01],\n",
            "          [ 3.9171e-02,  1.3692e-01,  4.1902e-01,  4.0105e-01,  5.8202e-01],\n",
            "          [ 2.1128e-01,  9.5134e-03,  7.4884e-01,  8.6355e-02,  5.4059e-01],\n",
            "          [ 7.3099e-02,  3.4042e-01,  4.8442e-01,  3.9695e-01,  1.9820e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4485]]],\n",
            "\n",
            "\n",
            "        [[[6.0780]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4485, 6.0780], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 6] = 0.44845420122146606\n",
            " somado na saída em [1, 1, 4, 6] = 6.0779924392700195\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 4, 7\n",
            " alvo do kernel em x: x[1,0,12:17, 21:26]\n",
            " \n",
            " tensor([[0.9461, 0.0547, 0.3743, 0.1900, 0.5323],\n",
            "        [0.6489, 0.4459, 0.0609, 0.9443, 0.2344],\n",
            "        [0.5395, 0.9904, 0.6840, 0.4836, 0.5402],\n",
            "        [0.1116, 0.9300, 0.4988, 0.8155, 0.0903],\n",
            "        [0.8035, 0.5533, 0.4133, 0.6517, 0.1078]])\n",
            " produto: tensor([[[[-8.1621e-03, -7.8063e-06, -2.5581e-04,  7.2688e-04, -1.0042e-02],\n",
            "          [ 8.7521e-03,  4.7269e-03,  1.1056e-03,  2.5492e-03, -1.4628e-03],\n",
            "          [ 1.1200e-02,  4.2245e-02,  2.2148e-02,  5.1764e-03, -4.1985e-03],\n",
            "          [ 6.9031e-03,  6.8393e-02,  4.3005e-02,  3.5121e-02,  1.7772e-03],\n",
            "          [ 7.3817e-02,  6.2639e-02,  4.4048e-02,  3.9728e-02,  1.3004e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3266e-01, -1.4215e-03,  4.6582e-02,  3.2088e-02,  3.3973e-02],\n",
            "          [ 9.5830e-03,  1.2612e-01,  3.4077e-02,  5.0368e-01,  9.8450e-02],\n",
            "          [ 7.9372e-02,  5.1780e-01,  5.5520e-01,  3.5955e-01,  3.1746e-01],\n",
            "          [ 3.0732e-02,  6.0630e-01,  4.3324e-01,  6.3086e-01,  5.2516e-02],\n",
            "          [ 1.5566e-01,  2.9397e-01,  2.7076e-01,  3.2197e-01,  3.8620e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4512]]],\n",
            "\n",
            "\n",
            "        [[[5.4145]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4512, 5.4145], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 4, 7] = 0.45123401284217834\n",
            " somado na saída em [1, 1, 4, 7] = 5.414475440979004\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 0\n",
            " alvo do kernel em x: x[1,0,15:20, 0:5]\n",
            " \n",
            " tensor([[0.8265, 0.9085, 0.3120, 0.1799, 0.9101],\n",
            "        [0.0365, 0.4925, 0.1533, 0.1850, 0.6348],\n",
            "        [0.5041, 0.2393, 0.9607, 0.3068, 0.7787],\n",
            "        [0.1000, 0.9975, 0.7570, 0.9336, 0.0261],\n",
            "        [0.4694, 0.8200, 0.2349, 0.3183, 0.4343]])\n",
            " produto: tensor([[[[-7.1305e-03, -1.2972e-04, -2.1327e-04,  6.8841e-04, -1.7167e-02],\n",
            "          [ 4.9254e-04,  5.2208e-03,  2.7833e-03,  4.9945e-04, -3.9620e-03],\n",
            "          [ 1.0466e-02,  1.0208e-02,  3.1109e-02,  3.2833e-03, -6.0519e-03],\n",
            "          [ 6.1862e-03,  7.3363e-02,  6.5258e-02,  4.0211e-02,  5.1340e-04],\n",
            "          [ 4.3125e-02,  9.2823e-02,  2.5038e-02,  1.9404e-02,  5.2376e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1589e-01, -2.3621e-02,  3.8835e-02,  3.0390e-02,  5.8081e-02],\n",
            "          [ 5.3930e-04,  1.3930e-01,  8.5790e-02,  9.8685e-02,  2.6665e-01],\n",
            "          [ 7.4173e-02,  1.2512e-01,  7.7981e-01,  2.2805e-01,  4.5760e-01],\n",
            "          [ 2.7541e-02,  6.5036e-01,  6.5742e-01,  7.2229e-01,  1.5171e-02],\n",
            "          [ 9.0939e-02,  4.3563e-01,  1.5391e-01,  1.5726e-01,  1.5555e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4013]]],\n",
            "\n",
            "\n",
            "        [[[5.3096]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4013, 5.3096], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 0] = 0.4012564718723297\n",
            " somado na saída em [1, 1, 5, 0] = 5.309573650360107\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 1\n",
            " alvo do kernel em x: x[1,0,15:20, 3:8]\n",
            " \n",
            " tensor([[0.1799, 0.9101, 0.6591, 0.5461, 0.4248],\n",
            "        [0.1850, 0.6348, 0.0123, 0.7394, 0.8368],\n",
            "        [0.3068, 0.7787, 0.0846, 0.0055, 0.8854],\n",
            "        [0.9336, 0.0261, 0.7146, 0.6828, 0.1701],\n",
            "        [0.3183, 0.4343, 0.0319, 0.8467, 0.4079]])\n",
            " produto: tensor([[[[-1.5523e-03, -1.2994e-04, -4.5049e-04,  2.0892e-03, -8.0135e-03],\n",
            "          [ 2.4953e-03,  6.7294e-03,  2.2395e-04,  1.9962e-03, -5.2230e-03],\n",
            "          [ 6.3689e-03,  3.3214e-02,  2.7387e-03,  5.8935e-05, -6.8810e-03],\n",
            "          [ 5.7739e-02,  1.9194e-03,  6.1603e-02,  2.9408e-02,  3.3468e-03],\n",
            "          [ 2.9244e-02,  4.9160e-02,  3.3985e-03,  5.1615e-02,  4.9200e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.5229e-02, -2.3662e-02,  8.2033e-02,  9.2227e-02,  2.7111e-02],\n",
            "          [ 2.7322e-03,  1.7955e-01,  6.9029e-03,  3.9442e-01,  3.5152e-01],\n",
            "          [ 4.5134e-02,  4.0711e-01,  6.8651e-02,  4.0936e-03,  5.2029e-01],\n",
            "          [ 2.5705e-01,  1.7016e-02,  6.2060e-01,  5.2824e-01,  9.8901e-02],\n",
            "          [ 6.1667e-02,  2.3071e-01,  2.0891e-02,  4.1831e-01,  1.4611e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3260]]],\n",
            "\n",
            "\n",
            "        [[[4.5324]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3260, 4.5324], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 1] = 0.32601797580718994\n",
            " somado na saída em [1, 1, 5, 1] = 4.532376289367676\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 2\n",
            " alvo do kernel em x: x[1,0,15:20, 6:11]\n",
            " \n",
            " tensor([[0.5461, 0.4248, 0.5401, 0.7634, 0.6491],\n",
            "        [0.7394, 0.8368, 0.4989, 0.9842, 0.4907],\n",
            "        [0.0055, 0.8854, 0.8619, 0.6925, 0.2236],\n",
            "        [0.6828, 0.1701, 0.0904, 0.1711, 0.3325],\n",
            "        [0.8467, 0.4079, 0.7925, 0.6389, 0.8770]])\n",
            " produto: tensor([[[[-4.7109e-03, -6.0654e-05, -3.6916e-04,  2.9208e-03, -1.2245e-02],\n",
            "          [ 9.9731e-03,  8.8711e-03,  9.0569e-03,  2.6569e-03, -3.0628e-03],\n",
            "          [ 1.1432e-04,  3.7764e-02,  2.7908e-02,  7.4117e-03, -1.7375e-03],\n",
            "          [ 4.2227e-02,  1.2513e-02,  7.7898e-03,  7.3682e-03,  6.5401e-03],\n",
            "          [ 7.7789e-02,  4.6179e-02,  8.4465e-02,  3.8947e-02,  1.0578e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.6564e-02, -1.1045e-02,  6.7224e-02,  1.2894e-01,  4.1426e-02],\n",
            "          [ 1.0920e-02,  2.3670e-01,  2.7916e-01,  5.2497e-01,  2.0613e-01],\n",
            "          [ 8.1016e-04,  4.6288e-01,  6.9958e-01,  5.1481e-01,  1.3138e-01],\n",
            "          [ 1.8799e-01,  1.1092e-01,  7.8476e-02,  1.3235e-01,  1.9326e-01],\n",
            "          [ 1.6403e-01,  2.1672e-01,  5.1921e-01,  3.1564e-01,  3.1415e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4189]]],\n",
            "\n",
            "\n",
            "        [[[5.4501]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4189, 5.4501], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 2] = 0.41888725757598877\n",
            " somado na saída em [1, 1, 5, 2] = 5.450074672698975\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 3\n",
            " alvo do kernel em x: x[1,0,15:20, 9:14]\n",
            " \n",
            " tensor([[0.7634, 0.6491, 0.3994, 0.5752, 0.1820],\n",
            "        [0.9842, 0.4907, 0.2652, 0.7893, 0.3663],\n",
            "        [0.6925, 0.2236, 0.0915, 0.9276, 0.4429],\n",
            "        [0.1711, 0.3325, 0.6415, 0.3993, 0.0693],\n",
            "        [0.6389, 0.8770, 0.1611, 0.2185, 0.6780]])\n",
            " produto: tensor([[[[-6.5860e-03, -9.2679e-05, -2.7299e-04,  2.2007e-03, -3.4327e-03],\n",
            "          [ 1.3274e-02,  5.2021e-03,  4.8143e-03,  2.1309e-03, -2.2860e-03],\n",
            "          [ 1.4377e-02,  9.5359e-03,  2.9627e-03,  9.9281e-03, -3.4419e-03],\n",
            "          [ 1.0580e-02,  2.4451e-02,  5.5301e-02,  1.7195e-02,  1.3629e-03],\n",
            "          [ 5.8696e-02,  9.9286e-02,  1.7167e-02,  1.3322e-02,  8.1781e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0704e-01, -1.6877e-02,  4.9711e-02,  9.7150e-02,  1.1613e-02],\n",
            "          [ 1.4534e-02,  1.3880e-01,  1.4839e-01,  4.2104e-01,  1.5385e-01],\n",
            "          [ 1.0189e-01,  1.1688e-01,  7.4266e-02,  6.8960e-01,  2.6025e-01],\n",
            "          [ 4.7102e-02,  2.1676e-01,  5.5711e-01,  3.0887e-01,  4.0275e-02],\n",
            "          [ 1.2377e-01,  4.6596e-01,  1.0553e-01,  1.0797e-01,  2.4287e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3539]]],\n",
            "\n",
            "\n",
            "        [[[4.3703]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3539, 4.3703], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 3] = 0.3538544178009033\n",
            " somado na saída em [1, 1, 5, 3] = 4.370267391204834\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 4\n",
            " alvo do kernel em x: x[1,0,15:20, 12:17]\n",
            " \n",
            " tensor([[0.5752, 0.1820, 0.5232, 0.9686, 0.1312],\n",
            "        [0.7893, 0.3663, 0.9927, 0.0080, 0.4963],\n",
            "        [0.9276, 0.4429, 0.9930, 0.8635, 0.7224],\n",
            "        [0.3993, 0.0693, 0.9869, 0.9412, 0.7387],\n",
            "        [0.2185, 0.6780, 0.6991, 0.6472, 0.1481]])\n",
            " produto: tensor([[[[-4.9624e-03, -2.5982e-05, -3.5760e-04,  3.7059e-03, -2.4757e-03],\n",
            "          [ 1.0646e-02,  3.8827e-03,  1.8022e-02,  2.1698e-05, -3.0977e-03],\n",
            "          [ 1.9258e-02,  1.8890e-02,  3.2152e-02,  9.2425e-03, -5.6141e-03],\n",
            "          [ 2.4691e-02,  5.0955e-03,  8.5084e-02,  4.0535e-02,  1.4532e-02],\n",
            "          [ 2.0078e-02,  7.6759e-02,  7.4515e-02,  3.9452e-02,  1.7866e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.0652e-02, -4.7313e-03,  6.5119e-02,  1.6360e-01,  8.3759e-03],\n",
            "          [ 1.1657e-02,  1.0360e-01,  5.5548e-01,  4.2872e-03,  2.0848e-01],\n",
            "          [ 1.3648e-01,  2.3153e-01,  8.0595e-01,  6.4198e-01,  4.2450e-01],\n",
            "          [ 1.0992e-01,  4.5171e-02,  8.5715e-01,  7.2812e-01,  4.2944e-01],\n",
            "          [ 4.2338e-02,  3.6024e-01,  4.5804e-01,  3.1973e-01,  5.3057e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4818]]],\n",
            "\n",
            "\n",
            "        [[[6.6789]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4818, 6.6789], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 4] = 0.48181551694869995\n",
            " somado na saída em [1, 1, 5, 4] = 6.678850173950195\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 5\n",
            " alvo do kernel em x: x[1,0,15:20, 15:20]\n",
            " \n",
            " tensor([[0.9686, 0.1312, 0.1912, 0.7674, 0.0146],\n",
            "        [0.0080, 0.4963, 0.9293, 0.3773, 0.6408],\n",
            "        [0.8635, 0.7224, 0.1242, 0.7182, 0.4897],\n",
            "        [0.9412, 0.7387, 0.5746, 0.4230, 0.9776],\n",
            "        [0.6472, 0.1481, 0.6575, 0.6107, 0.7830]])\n",
            " produto: tensor([[[[-8.3565e-03, -1.8739e-05, -1.3066e-04,  2.9360e-03, -2.7525e-04],\n",
            "          [ 1.0840e-04,  5.2613e-03,  1.6871e-02,  1.0187e-03, -3.9992e-03],\n",
            "          [ 1.7929e-02,  3.0811e-02,  4.0203e-03,  7.6874e-03, -3.8056e-03],\n",
            "          [ 5.8205e-02,  5.4331e-02,  4.9538e-02,  1.8216e-02,  1.9231e-02],\n",
            "          [ 5.9458e-02,  1.6769e-02,  7.0082e-02,  3.7229e-02,  9.4435e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3582e-01, -3.4123e-03,  2.3793e-02,  1.2961e-01,  9.3124e-04],\n",
            "          [ 1.1870e-04,  1.4038e-01,  5.2001e-01,  2.0127e-01,  2.6916e-01],\n",
            "          [ 1.2705e-01,  3.7766e-01,  1.0078e-01,  5.3396e-01,  2.8775e-01],\n",
            "          [ 2.5912e-01,  4.8164e-01,  4.9905e-01,  3.2721e-01,  5.6828e-01],\n",
            "          [ 1.2538e-01,  7.8697e-02,  4.3080e-01,  3.0172e-01,  2.8045e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4626]]],\n",
            "\n",
            "\n",
            "        [[[5.9256]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4626, 5.9256], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 5] = 0.4625585973262787\n",
            " somado na saída em [1, 1, 5, 5] = 5.925597667694092\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 6\n",
            " alvo do kernel em x: x[1,0,15:20, 18:23]\n",
            " \n",
            " tensor([[0.7674, 0.0146, 0.8622, 0.1116, 0.9300],\n",
            "        [0.3773, 0.6408, 0.7394, 0.8035, 0.5533],\n",
            "        [0.7182, 0.4897, 0.8620, 0.4625, 0.3835],\n",
            "        [0.4230, 0.9776, 0.6635, 0.7803, 0.7064],\n",
            "        [0.6107, 0.7830, 0.7642, 0.9717, 0.6954]])\n",
            " produto: tensor([[[[-6.6204e-03, -2.0834e-06, -5.8934e-04,  4.2706e-04, -1.7542e-02],\n",
            "          [ 5.0893e-03,  6.7926e-03,  1.3423e-02,  2.1691e-03, -3.4535e-03],\n",
            "          [ 1.4912e-02,  2.0886e-02,  2.7912e-02,  4.9500e-03, -2.9804e-03],\n",
            "          [ 2.6157e-02,  7.1898e-02,  5.7203e-02,  3.3607e-02,  1.3896e-02],\n",
            "          [ 5.6107e-02,  8.8636e-02,  8.1453e-02,  5.9236e-02,  8.3872e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0760e-01, -3.7939e-04,  1.0732e-01,  1.8853e-02,  5.9349e-02],\n",
            "          [ 5.5724e-03,  1.8124e-01,  4.1374e-01,  4.2859e-01,  2.3243e-01],\n",
            "          [ 1.0568e-01,  2.5600e-01,  6.9967e-01,  3.4382e-01,  2.2536e-01],\n",
            "          [ 1.1645e-01,  6.3737e-01,  5.7627e-01,  6.0366e-01,  4.1065e-01],\n",
            "          [ 1.1831e-01,  4.1598e-01,  5.0070e-01,  4.8007e-01,  2.4908e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.5620]]],\n",
            "\n",
            "\n",
            "        [[[7.0782]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.5620, 7.0782], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 6] = 0.5619542002677917\n",
            " somado na saída em [1, 1, 5, 6] = 7.07818078994751\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 5, 7\n",
            " alvo do kernel em x: x[1,0,15:20, 21:26]\n",
            " \n",
            " tensor([[0.1116, 0.9300, 0.4988, 0.8155, 0.0903],\n",
            "        [0.8035, 0.5533, 0.4133, 0.6517, 0.1078],\n",
            "        [0.4625, 0.3835, 0.5524, 0.5703, 0.1827],\n",
            "        [0.7803, 0.7064, 0.9658, 0.9520, 0.0155],\n",
            "        [0.9717, 0.6954, 0.2992, 0.9333, 0.2662]])\n",
            " produto: tensor([[[[-9.6298e-04, -1.3278e-04, -3.4095e-04,  3.1199e-03, -1.7042e-03],\n",
            "          [ 1.0837e-02,  5.8657e-03,  7.5028e-03,  1.7594e-03, -6.7295e-04],\n",
            "          [ 9.6020e-03,  1.6357e-02,  1.7886e-02,  6.1038e-03, -1.4198e-03],\n",
            "          [ 4.8256e-02,  5.1954e-02,  8.3261e-02,  4.1000e-02,  3.0583e-04],\n",
            "          [ 8.9274e-02,  7.8721e-02,  3.1885e-02,  5.6891e-02,  3.2103e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.5651e-02, -2.4179e-02,  6.2087e-02,  1.3773e-01,  5.7656e-03],\n",
            "          [ 1.1866e-02,  1.5651e-01,  2.3126e-01,  3.4764e-01,  4.5291e-02],\n",
            "          [ 6.8046e-02,  2.0049e-01,  4.4835e-01,  4.2397e-01,  1.0735e-01],\n",
            "          [ 2.1483e-01,  4.6057e-01,  8.3878e-01,  7.3646e-01,  9.0376e-03],\n",
            "          [ 1.8825e-01,  3.6945e-01,  1.9600e-01,  4.6106e-01,  9.5338e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.5586]]],\n",
            "\n",
            "\n",
            "        [[[5.7763]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.5586, 5.7763], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 5, 7] = 0.5585576891899109\n",
            " somado na saída em [1, 1, 5, 7] = 5.776290416717529\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 0\n",
            " alvo do kernel em x: x[1,0,18:23, 0:5]\n",
            " \n",
            " tensor([[0.1000, 0.9975, 0.7570, 0.9336, 0.0261],\n",
            "        [0.4694, 0.8200, 0.2349, 0.3183, 0.4343],\n",
            "        [0.2339, 0.6732, 0.4439, 0.8162, 0.5434],\n",
            "        [0.0886, 0.3315, 0.4983, 0.0313, 0.2001],\n",
            "        [0.3781, 0.0341, 0.7302, 0.8009, 0.4006]])\n",
            " produto: tensor([[[[-8.6298e-04, -1.4243e-04, -5.1739e-04,  3.5720e-03, -4.9231e-04],\n",
            "          [ 6.3313e-03,  8.6922e-03,  4.2648e-03,  8.5934e-04, -2.7104e-03],\n",
            "          [ 4.8558e-03,  2.8713e-02,  1.4375e-02,  8.7357e-03, -4.2229e-03],\n",
            "          [ 5.4793e-03,  2.4378e-02,  4.2956e-02,  1.3487e-03,  3.9353e-03],\n",
            "          [ 3.4738e-02,  3.8638e-03,  7.7827e-02,  4.8820e-02,  4.8318e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.4026e-02, -2.5936e-02,  9.4215e-02,  1.5769e-01,  1.6656e-03],\n",
            "          [ 6.9324e-03,  2.3192e-01,  1.3145e-01,  1.6980e-01,  1.8241e-01],\n",
            "          [ 3.4411e-02,  3.5194e-01,  3.6033e-01,  6.0677e-01,  3.1931e-01],\n",
            "          [ 2.4393e-02,  2.1611e-01,  4.3275e-01,  2.4226e-02,  1.1629e-01],\n",
            "          [ 7.3252e-02,  1.8133e-02,  4.7840e-01,  3.9565e-01,  1.4350e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3196]]],\n",
            "\n",
            "\n",
            "        [[[4.5316]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3196, 4.5316], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 0] = 0.31962859630584717\n",
            " somado na saída em [1, 1, 6, 0] = 4.531589031219482\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 1\n",
            " alvo do kernel em x: x[1,0,18:23, 3:8]\n",
            " \n",
            " tensor([[0.9336, 0.0261, 0.7146, 0.6828, 0.1701],\n",
            "        [0.3183, 0.4343, 0.0319, 0.8467, 0.4079],\n",
            "        [0.8162, 0.5434, 0.0746, 0.6593, 0.8650],\n",
            "        [0.0313, 0.2001, 0.0714, 0.9715, 0.7599],\n",
            "        [0.8009, 0.4006, 0.0537, 0.1695, 0.4521]])\n",
            " produto: tensor([[[[-8.0545e-03, -3.7263e-06, -4.8841e-04,  2.6124e-03, -3.2094e-03],\n",
            "          [ 4.2934e-03,  4.6034e-03,  5.7888e-04,  2.2858e-03, -2.5460e-03],\n",
            "          [ 1.6945e-02,  2.3176e-02,  2.4156e-03,  7.0570e-03, -6.7228e-03],\n",
            "          [ 1.9366e-03,  1.4713e-02,  6.1515e-03,  4.1841e-02,  1.4949e-02],\n",
            "          [ 7.3576e-02,  4.5351e-02,  5.7216e-03,  1.0333e-02,  5.4525e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3091e-01, -6.7856e-04,  8.8938e-02,  1.1532e-01,  1.0858e-02],\n",
            "          [ 4.7010e-03,  1.2283e-01,  1.7843e-02,  4.5165e-01,  1.7135e-01],\n",
            "          [ 1.2009e-01,  2.8407e-01,  6.0552e-02,  4.9017e-01,  5.0833e-01],\n",
            "          [ 8.6217e-03,  1.3043e-01,  6.1971e-02,  7.5157e-01,  4.4175e-01],\n",
            "          [ 1.5515e-01,  2.1284e-01,  3.5171e-02,  8.3741e-02,  1.6193e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2630]]],\n",
            "\n",
            "\n",
            "        [[[4.3583]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2630, 4.3583], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 1] = 0.2629680037498474\n",
            " somado na saída em [1, 1, 6, 1] = 4.358290672302246\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 2\n",
            " alvo do kernel em x: x[1,0,18:23, 6:11]\n",
            " \n",
            " tensor([[0.6828, 0.1701, 0.0904, 0.1711, 0.3325],\n",
            "        [0.8467, 0.4079, 0.7925, 0.6389, 0.8770],\n",
            "        [0.6593, 0.8650, 0.3682, 0.3388, 0.0825],\n",
            "        [0.9715, 0.7599, 0.3429, 0.5165, 0.0050],\n",
            "        [0.1695, 0.4521, 0.9934, 0.0979, 0.6096]])\n",
            " produto: tensor([[[[-5.8906e-03, -2.4292e-05, -6.1760e-05,  6.5453e-04, -6.2715e-03],\n",
            "          [ 1.1420e-02,  4.3243e-03,  1.4387e-02,  1.7248e-03, -5.4740e-03],\n",
            "          [ 1.3689e-02,  3.6896e-02,  1.1923e-02,  3.6262e-03, -6.4089e-04],\n",
            "          [ 6.0080e-02,  5.5889e-02,  2.9564e-02,  2.2244e-02,  9.8695e-05],\n",
            "          [ 1.5572e-02,  5.1177e-02,  1.0588e-01,  5.9705e-03,  7.3528e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.5738e-02, -4.4235e-03,  1.1246e-02,  2.8894e-02,  2.1218e-02],\n",
            "          [ 1.2504e-02,  1.1538e-01,  4.4345e-01,  3.4080e-01,  3.6841e-01],\n",
            "          [ 9.7009e-02,  4.5224e-01,  2.9887e-01,  2.5187e-01,  4.8460e-02],\n",
            "          [ 2.6747e-01,  4.9545e-01,  2.9783e-01,  3.9956e-01,  2.9165e-03],\n",
            "          [ 3.2838e-02,  2.4018e-01,  6.5083e-01,  4.8387e-02,  2.1836e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4341]]],\n",
            "\n",
            "\n",
            "        [[[5.0440]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4341, 5.0440], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 2] = 0.4341077208518982\n",
            " somado na saída em [1, 1, 6, 2] = 5.044030666351318\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 3\n",
            " alvo do kernel em x: x[1,0,18:23, 9:14]\n",
            " \n",
            " tensor([[0.1711, 0.3325, 0.6415, 0.3993, 0.0693],\n",
            "        [0.6389, 0.8770, 0.1611, 0.2185, 0.6780],\n",
            "        [0.3388, 0.0825, 0.8961, 0.1626, 0.5643],\n",
            "        [0.5165, 0.0050, 0.4328, 0.5748, 0.1646],\n",
            "        [0.0979, 0.6096, 0.4883, 0.4901, 0.5007]])\n",
            " produto: tensor([[[[-1.4759e-03, -4.7469e-05, -4.3844e-04,  1.5275e-03, -1.3069e-03],\n",
            "          [ 8.6173e-03,  9.2974e-03,  2.9242e-03,  5.8999e-04, -4.2320e-03],\n",
            "          [ 7.0341e-03,  3.5173e-03,  2.9017e-02,  1.7407e-03, -4.3853e-03],\n",
            "          [ 3.1941e-02,  3.6899e-04,  3.7310e-02,  2.4757e-02,  3.2372e-03],\n",
            "          [ 8.9980e-03,  6.9013e-02,  5.2040e-02,  2.9876e-02,  6.0385e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3988e-02, -8.6441e-03,  7.9840e-02,  6.7432e-02,  4.4216e-03],\n",
            "          [ 9.4354e-03,  2.4807e-01,  9.0131e-02,  1.1658e-01,  2.8482e-01],\n",
            "          [ 4.9849e-02,  4.3112e-02,  7.2736e-01,  1.2091e-01,  3.3159e-01],\n",
            "          [ 1.4220e-01,  3.2711e-03,  3.7587e-01,  4.4470e-01,  9.5662e-02],\n",
            "          [ 1.8974e-02,  3.2388e-01,  3.1989e-01,  2.4213e-01,  1.7933e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3160]]],\n",
            "\n",
            "\n",
            "        [[[4.2868]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3160, 4.2868], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 3] = 0.31595924496650696\n",
            " somado na saída em [1, 1, 6, 3] = 4.286818027496338\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 4\n",
            " alvo do kernel em x: x[1,0,18:23, 12:17]\n",
            " \n",
            " tensor([[0.3993, 0.0693, 0.9869, 0.9412, 0.7387],\n",
            "        [0.2185, 0.6780, 0.6991, 0.6472, 0.1481],\n",
            "        [0.1626, 0.5643, 0.9157, 0.5698, 0.0616],\n",
            "        [0.5748, 0.1646, 0.3817, 0.5781, 0.8151],\n",
            "        [0.4901, 0.5007, 0.6884, 0.8664, 0.8997]])\n",
            " produto: tensor([[[[-3.4444e-03, -9.8922e-06, -6.7457e-04,  3.6008e-03, -1.3935e-02],\n",
            "          [ 2.9477e-03,  7.1879e-03,  1.2692e-02,  1.7472e-03, -9.2452e-04],\n",
            "          [ 3.3765e-03,  2.4068e-02,  2.9650e-02,  6.0987e-03, -4.7874e-04],\n",
            "          [ 3.5549e-02,  1.2103e-02,  3.2905e-02,  2.4896e-02,  1.6034e-02],\n",
            "          [ 4.5026e-02,  5.6676e-02,  7.3373e-02,  5.2817e-02,  1.0852e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.5981e-02, -1.8014e-03,  1.2284e-01,  1.5896e-01,  4.7146e-02],\n",
            "          [ 3.2275e-03,  1.9179e-01,  3.9121e-01,  3.4522e-01,  6.2222e-02],\n",
            "          [ 2.3928e-02,  2.9500e-01,  7.4323e-01,  4.2361e-01,  3.6199e-02],\n",
            "          [ 1.5826e-01,  1.0729e-01,  3.3149e-01,  4.4720e-01,  4.7382e-01],\n",
            "          [ 9.4946e-02,  2.6599e-01,  4.5103e-01,  4.2805e-01,  3.2228e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4321]]],\n",
            "\n",
            "\n",
            "        [[[5.8671]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4321, 5.8671], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 4] = 0.4321322739124298\n",
            " somado na saída em [1, 1, 6, 4] = 5.867142677307129\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 5\n",
            " alvo do kernel em x: x[1,0,18:23, 15:20]\n",
            " \n",
            " tensor([[0.9412, 0.7387, 0.5746, 0.4230, 0.9776],\n",
            "        [0.6472, 0.1481, 0.6575, 0.6107, 0.7830],\n",
            "        [0.5698, 0.0616, 0.4652, 0.0462, 0.4758],\n",
            "        [0.5781, 0.8151, 0.5611, 0.7951, 0.8138],\n",
            "        [0.8664, 0.8997, 0.0382, 0.7181, 0.8949]])\n",
            " produto: tensor([[[[-8.1195e-03, -1.0548e-04, -3.9275e-04,  1.6182e-03, -1.8441e-02],\n",
            "          [ 8.7291e-03,  1.5703e-03,  1.1937e-02,  1.6487e-03, -4.8868e-03],\n",
            "          [ 1.1830e-02,  2.6275e-03,  1.5063e-02,  4.9448e-04, -3.6976e-03],\n",
            "          [ 3.5748e-02,  5.9947e-02,  4.8372e-02,  3.4245e-02,  1.6009e-02],\n",
            "          [ 7.9600e-02,  1.0186e-01,  4.0746e-03,  4.3773e-02,  1.0793e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.3196e-01, -1.9207e-02,  7.1519e-02,  7.1436e-02,  6.2390e-02],\n",
            "          [ 9.5578e-03,  4.1897e-02,  3.6794e-01,  3.2577e-01,  3.2890e-01],\n",
            "          [ 8.3836e-02,  3.2205e-02,  3.7760e-01,  3.4346e-02,  2.7959e-01],\n",
            "          [ 1.5915e-01,  5.3142e-01,  4.8730e-01,  6.1513e-01,  4.7309e-01],\n",
            "          [ 1.6785e-01,  4.7802e-01,  2.5047e-02,  3.5475e-01,  3.2054e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4543]]],\n",
            "\n",
            "\n",
            "        [[[5.5481]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4543, 5.5481], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 5] = 0.4542955160140991\n",
            " somado na saída em [1, 1, 6, 5] = 5.548106670379639\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 6\n",
            " alvo do kernel em x: x[1,0,18:23, 18:23]\n",
            " \n",
            " tensor([[0.4230, 0.9776, 0.6635, 0.7803, 0.7064],\n",
            "        [0.6107, 0.7830, 0.7642, 0.9717, 0.6954],\n",
            "        [0.0462, 0.4758, 0.9554, 0.5254, 0.7749],\n",
            "        [0.7951, 0.8138, 0.5186, 0.4616, 0.8413],\n",
            "        [0.7181, 0.8949, 0.0928, 0.1199, 0.0902]])\n",
            " produto: tensor([[[[-3.6489e-03, -1.3958e-04, -4.5352e-04,  2.9854e-03, -1.3326e-02],\n",
            "          [ 8.2372e-03,  8.3001e-03,  1.3874e-02,  2.6233e-03, -4.3402e-03],\n",
            "          [ 9.5918e-04,  2.0293e-02,  3.0934e-02,  5.6236e-03, -6.0222e-03],\n",
            "          [ 4.9173e-02,  5.9854e-02,  4.4712e-02,  1.9881e-02,  1.6550e-02],\n",
            "          [ 6.5971e-02,  1.0131e-01,  9.8920e-03,  7.3097e-03,  1.0875e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.9305e-02, -2.5418e-02,  8.2586e-02,  1.3179e-01,  4.5084e-02],\n",
            "          [ 9.0192e-03,  2.2146e-01,  4.2764e-01,  5.1833e-01,  2.9211e-01],\n",
            "          [ 6.7974e-03,  2.4873e-01,  7.7543e-01,  3.9061e-01,  4.5536e-01],\n",
            "          [ 2.1891e-01,  5.3060e-01,  4.5044e-01,  3.5711e-01,  4.8907e-01],\n",
            "          [ 1.3911e-01,  4.7543e-01,  6.0807e-02,  5.9241e-02,  3.2295e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4416]]],\n",
            "\n",
            "\n",
            "        [[[6.3333]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4416, 6.3333], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 6] = 0.441635400056839\n",
            " somado na saída em [1, 1, 6, 6] = 6.333251953125\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 6, 7\n",
            " alvo do kernel em x: x[1,0,18:23, 21:26]\n",
            " \n",
            " tensor([[0.7803, 0.7064, 0.9658, 0.9520, 0.0155],\n",
            "        [0.9717, 0.6954, 0.2992, 0.9333, 0.2662],\n",
            "        [0.5254, 0.7749, 0.0384, 0.7792, 0.5845],\n",
            "        [0.4616, 0.8413, 0.5506, 0.0674, 0.8557],\n",
            "        [0.1199, 0.0902, 0.9340, 0.1653, 0.6339]])\n",
            " produto: tensor([[[[-6.7317e-03, -1.0086e-04, -6.6012e-04,  3.6421e-03, -2.9327e-04],\n",
            "          [ 1.3106e-02,  7.3717e-03,  5.4310e-03,  2.5195e-03, -1.6612e-03],\n",
            "          [ 1.0909e-02,  3.3051e-02,  1.2433e-03,  8.3401e-03, -4.5425e-03],\n",
            "          [ 2.8547e-02,  6.1876e-02,  4.7465e-02,  2.9028e-03,  1.6833e-02],\n",
            "          [ 1.1016e-02,  1.0207e-02,  9.9546e-02,  1.0076e-02,  7.6461e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0941e-01, -1.8367e-02,  1.2021e-01,  1.6078e-01,  9.9220e-04],\n",
            "          [ 1.4351e-02,  1.9669e-01,  1.6740e-01,  4.9782e-01,  1.1181e-01],\n",
            "          [ 7.7306e-02,  4.0511e-01,  3.1166e-02,  5.7929e-01,  3.4347e-01],\n",
            "          [ 1.2709e-01,  5.4852e-01,  4.7817e-01,  5.2142e-02,  4.9742e-01],\n",
            "          [ 2.3230e-02,  4.7902e-02,  6.1191e-01,  8.1659e-02,  2.2707e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3677]]],\n",
            "\n",
            "\n",
            "        [[[5.2737]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3677, 5.2737], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 6, 7] = 0.3677384555339813\n",
            " somado na saída em [1, 1, 6, 7] = 5.273723125457764\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 0\n",
            " alvo do kernel em x: x[1,0,21:26, 0:5]\n",
            " \n",
            " tensor([[0.0886, 0.3315, 0.4983, 0.0313, 0.2001],\n",
            "        [0.3781, 0.0341, 0.7302, 0.8009, 0.4006],\n",
            "        [0.4045, 0.2864, 0.4871, 0.5926, 0.3107],\n",
            "        [0.7601, 0.1911, 0.8360, 0.3965, 0.4580],\n",
            "        [0.6245, 0.3001, 0.6014, 0.4300, 0.3140]])\n",
            " produto: tensor([[[[-7.6436e-04, -4.7327e-05, -3.4057e-04,  1.1981e-04, -3.7737e-03],\n",
            "          [ 5.0999e-03,  3.6182e-04,  1.3257e-02,  2.1620e-03, -2.5004e-03],\n",
            "          [ 8.3985e-03,  1.2218e-02,  1.5773e-02,  6.3425e-03, -2.4150e-03],\n",
            "          [ 4.7008e-02,  1.4058e-02,  7.2069e-02,  1.7077e-02,  9.0100e-03],\n",
            "          [ 5.7376e-02,  3.3976e-02,  6.4103e-02,  2.6214e-02,  3.7875e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2423e-02, -8.6182e-03,  6.2017e-02,  5.2890e-03,  1.2767e-02],\n",
            "          [ 5.5841e-03,  9.6539e-03,  4.0860e-01,  4.2719e-01,  1.6828e-01],\n",
            "          [ 5.9517e-02,  1.4975e-01,  3.9539e-01,  4.4054e-01,  1.8261e-01],\n",
            "          [ 2.0928e-01,  1.2462e-01,  7.2603e-01,  3.0675e-01,  2.6625e-01],\n",
            "          [ 1.2099e-01,  1.5945e-01,  3.9404e-01,  2.1245e-01,  1.1248e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3986]]],\n",
            "\n",
            "\n",
            "        [[[4.9385]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3986, 4.9385], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 0] = 0.39856845140457153\n",
            " somado na saída em [1, 1, 7, 0] = 4.938490867614746\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 1\n",
            " alvo do kernel em x: x[1,0,21:26, 3:8]\n",
            " \n",
            " tensor([[0.0313, 0.2001, 0.0714, 0.9715, 0.7599],\n",
            "        [0.8009, 0.4006, 0.0537, 0.1695, 0.4521],\n",
            "        [0.5926, 0.3107, 0.0393, 0.8588, 0.4856],\n",
            "        [0.3965, 0.4580, 0.2380, 0.5905, 0.3862],\n",
            "        [0.4300, 0.3140, 0.2298, 0.5382, 0.2262]])\n",
            " produto: tensor([[[[-2.7016e-04, -2.8563e-05, -4.8771e-05,  3.7168e-03, -1.4335e-02],\n",
            "          [ 1.0802e-02,  4.2468e-03,  9.7458e-04,  4.5760e-04, -2.8215e-03],\n",
            "          [ 1.2303e-02,  1.3254e-02,  1.2728e-03,  9.1916e-03, -3.7740e-03],\n",
            "          [ 2.4521e-02,  3.3686e-02,  2.0517e-02,  2.5430e-02,  7.5962e-03],\n",
            "          [ 3.9507e-02,  3.5549e-02,  2.4490e-02,  3.2810e-02,  2.7282e-03]]],\n",
            "\n",
            "\n",
            "        [[[-4.3908e-03, -5.2013e-03,  8.8811e-03,  1.6408e-01,  4.8498e-02],\n",
            "          [ 1.1827e-02,  1.1331e-01,  3.0039e-02,  9.0416e-02,  1.8990e-01],\n",
            "          [ 8.7188e-02,  1.6246e-01,  3.1904e-02,  6.3844e-01,  2.8537e-01],\n",
            "          [ 1.0917e-01,  2.9862e-01,  2.0669e-01,  4.5679e-01,  2.2447e-01],\n",
            "          [ 8.3308e-02,  1.6683e-01,  1.5054e-01,  2.6591e-01,  8.1021e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.2818]]],\n",
            "\n",
            "\n",
            "        [[[3.8961]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.2818, 3.8961], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 1] = 0.2817755937576294\n",
            " somado na saída em [1, 1, 7, 1] = 3.896064281463623\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 2\n",
            " alvo do kernel em x: x[1,0,21:26, 6:11]\n",
            " \n",
            " tensor([[0.9715, 0.7599, 0.3429, 0.5165, 0.0050],\n",
            "        [0.1695, 0.4521, 0.9934, 0.0979, 0.6096],\n",
            "        [0.8588, 0.4856, 0.4644, 0.5450, 0.3092],\n",
            "        [0.5905, 0.3862, 0.2228, 0.4558, 0.2586],\n",
            "        [0.5382, 0.2262, 0.8589, 0.6265, 0.3948]])\n",
            " produto: tensor([[[[-8.3811e-03, -1.0850e-04, -2.3439e-04,  1.9760e-03, -9.4642e-05],\n",
            "          [ 2.2862e-03,  4.7923e-03,  1.8034e-02,  2.6441e-04, -3.8049e-03],\n",
            "          [ 1.7830e-02,  2.0713e-02,  1.5037e-02,  5.8330e-03, -2.4032e-03],\n",
            "          [ 3.6515e-02,  2.8400e-02,  1.9206e-02,  1.9630e-02,  5.0871e-03],\n",
            "          [ 4.9448e-02,  2.5606e-02,  9.1547e-02,  3.8193e-02,  4.7613e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.3622e-01, -1.9758e-02,  4.2682e-02,  8.7231e-02,  3.2019e-04],\n",
            "          [ 2.5033e-03,  1.2787e-01,  5.5587e-01,  5.2244e-02,  2.5608e-01],\n",
            "          [ 1.2635e-01,  2.5388e-01,  3.7693e-01,  4.0516e-01,  1.8172e-01],\n",
            "          [ 1.6256e-01,  2.5176e-01,  1.9348e-01,  3.5260e-01,  1.5033e-01],\n",
            "          [ 1.0427e-01,  1.2017e-01,  5.6274e-01,  3.0953e-01,  1.4140e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3901]]],\n",
            "\n",
            "\n",
            "        [[[4.6617]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3901, 4.6617], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 2] = 0.39013221859931946\n",
            " somado na saída em [1, 1, 7, 2] = 4.661709785461426\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 3\n",
            " alvo do kernel em x: x[1,0,21:26, 9:14]\n",
            " \n",
            " tensor([[0.5165, 0.0050, 0.4328, 0.5748, 0.1646],\n",
            "        [0.0979, 0.6096, 0.4883, 0.4901, 0.5007],\n",
            "        [0.5450, 0.3092, 0.4794, 0.0247, 0.0557],\n",
            "        [0.4558, 0.2586, 0.4889, 0.6173, 0.9154],\n",
            "        [0.6265, 0.3948, 0.4545, 0.4536, 0.6788]])\n",
            " produto: tensor([[[[-4.4557e-03, -7.1635e-07, -2.9581e-04,  2.1992e-03, -3.1043e-03],\n",
            "          [ 1.3210e-03,  6.4626e-03,  8.8642e-03,  1.3231e-03, -3.1248e-03],\n",
            "          [ 1.1315e-02,  1.3189e-02,  1.5522e-02,  2.6467e-04, -4.3288e-04],\n",
            "          [ 2.8187e-02,  1.9019e-02,  4.2148e-02,  2.6588e-02,  1.8007e-02],\n",
            "          [ 5.7560e-02,  4.4690e-02,  4.8439e-02,  2.7650e-02,  8.1867e-03]]],\n",
            "\n",
            "\n",
            "        [[[-7.2417e-02, -1.3045e-04,  5.3866e-02,  9.7085e-02,  1.0502e-02],\n",
            "          [ 1.4464e-03,  1.7243e-01,  2.7322e-01,  2.6143e-01,  2.1031e-01],\n",
            "          [ 8.0185e-02,  1.6166e-01,  3.8909e-01,  1.8384e-02,  3.2731e-02],\n",
            "          [ 1.2549e-01,  1.6860e-01,  4.2460e-01,  4.7758e-01,  5.3211e-01],\n",
            "          [ 1.2138e-01,  2.0973e-01,  2.9776e-01,  2.2409e-01,  2.4313e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3695]]],\n",
            "\n",
            "\n",
            "        [[[4.5143]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3695, 4.5143], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 3] = 0.36952027678489685\n",
            " somado na saída em [1, 1, 7, 3] = 4.514252662658691\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 4\n",
            " alvo do kernel em x: x[1,0,21:26, 12:17]\n",
            " \n",
            " tensor([[0.5748, 0.1646, 0.3817, 0.5781, 0.8151],\n",
            "        [0.4901, 0.5007, 0.6884, 0.8664, 0.8997],\n",
            "        [0.0247, 0.0557, 0.6240, 0.7175, 0.4038],\n",
            "        [0.6173, 0.9154, 0.9658, 0.7682, 0.1088],\n",
            "        [0.4536, 0.6788, 0.1741, 0.2084, 0.4194]])\n",
            " produto: tensor([[[[-4.9591e-03, -2.3496e-05, -2.6088e-04,  2.2116e-03, -1.5376e-02],\n",
            "          [ 6.6103e-03,  5.3073e-03,  1.2498e-02,  2.3391e-03, -5.6157e-03],\n",
            "          [ 5.1340e-04,  2.3758e-03,  2.0205e-02,  7.6801e-03, -3.1385e-03],\n",
            "          [ 3.8177e-02,  6.7321e-02,  8.3262e-02,  3.3084e-02,  2.1401e-03],\n",
            "          [ 4.1672e-02,  7.6840e-02,  1.8555e-02,  1.2702e-02,  5.0582e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.0598e-02, -4.2787e-03,  4.7506e-02,  9.7631e-02,  5.2019e-02],\n",
            "          [ 7.2379e-03,  1.4161e-01,  3.8522e-01,  4.6217e-01,  3.7795e-01],\n",
            "          [ 3.6383e-03,  2.9120e-02,  5.0649e-01,  5.3345e-01,  2.3731e-01],\n",
            "          [ 1.6996e-01,  5.9680e-01,  8.3879e-01,  5.9427e-01,  6.3241e-02],\n",
            "          [ 8.7873e-02,  3.6062e-01,  1.1406e-01,  1.0294e-01,  1.5022e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4092]]],\n",
            "\n",
            "\n",
            "        [[[5.8753]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4092, 5.8753], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 4] = 0.40917956829071045\n",
            " somado na saída em [1, 1, 7, 4] = 5.875252723693848\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 5\n",
            " alvo do kernel em x: x[1,0,21:26, 15:20]\n",
            " \n",
            " tensor([[0.5781, 0.8151, 0.5611, 0.7951, 0.8138],\n",
            "        [0.8664, 0.8997, 0.0382, 0.7181, 0.8949],\n",
            "        [0.7175, 0.4038, 0.5229, 0.7233, 0.1118],\n",
            "        [0.7682, 0.1088, 0.1413, 0.6270, 0.3297],\n",
            "        [0.2084, 0.4194, 0.9619, 0.1052, 0.7262]])\n",
            " produto: tensor([[[[-4.9869e-03, -1.1638e-04, -3.8351e-04,  3.0421e-03, -1.5352e-02],\n",
            "          [ 1.1686e-02,  9.5381e-03,  6.9405e-04,  1.9386e-03, -5.5853e-03],\n",
            "          [ 1.4898e-02,  1.7225e-02,  1.6931e-02,  7.7414e-03, -8.6879e-04],\n",
            "          [ 4.7506e-02,  8.0010e-03,  1.2184e-02,  2.7005e-02,  6.4847e-03],\n",
            "          [ 1.9142e-02,  4.7476e-02,  1.0253e-01,  6.4105e-03,  8.7583e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.1051e-02, -2.1192e-02,  6.9836e-02,  1.3429e-01,  5.1939e-02],\n",
            "          [ 1.2796e-02,  2.5449e-01,  2.1392e-02,  3.8303e-01,  3.7591e-01],\n",
            "          [ 1.0558e-01,  2.1112e-01,  4.2441e-01,  5.3771e-01,  6.5692e-02],\n",
            "          [ 2.1149e-01,  7.0928e-02,  1.2274e-01,  4.8507e-01,  1.9163e-01],\n",
            "          [ 4.0366e-02,  2.2281e-01,  6.3023e-01,  5.1953e-02,  2.6010e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.3419]]],\n",
            "\n",
            "\n",
            "        [[[4.8333]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.3419, 4.8333], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 5] = 0.34189343452453613\n",
            " somado na saída em [1, 1, 7, 5] = 4.83327579498291\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 6\n",
            " alvo do kernel em x: x[1,0,21:26, 18:23]\n",
            " \n",
            " tensor([[0.7951, 0.8138, 0.5186, 0.4616, 0.8413],\n",
            "        [0.7181, 0.8949, 0.0928, 0.1199, 0.0902],\n",
            "        [0.7233, 0.1118, 0.9677, 0.0905, 0.4709],\n",
            "        [0.6270, 0.3297, 0.7171, 0.3167, 0.9850],\n",
            "        [0.1052, 0.7262, 0.6340, 0.5407, 0.6349]])\n",
            " produto: tensor([[[[-6.8596e-03, -1.1620e-04, -3.5449e-04,  1.7661e-03, -1.5871e-02],\n",
            "          [ 9.6852e-03,  9.4865e-03,  1.6849e-03,  3.2372e-04, -5.6274e-04],\n",
            "          [ 1.5017e-02,  4.7681e-03,  3.1334e-02,  9.6833e-04, -3.6600e-03],\n",
            "          [ 3.8776e-02,  2.4244e-02,  6.1825e-02,  1.3638e-02,  1.9376e-02],\n",
            "          [ 9.6613e-03,  8.2205e-02,  6.7570e-02,  3.2960e-02,  7.6572e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1149e-01, -2.1160e-02,  6.4552e-02,  7.7963e-02,  5.3693e-02],\n",
            "          [ 1.0605e-02,  2.5312e-01,  5.1935e-02,  6.3963e-02,  3.7874e-02],\n",
            "          [ 1.0642e-01,  5.8443e-02,  7.8544e-01,  6.7259e-02,  2.7675e-01],\n",
            "          [ 1.7263e-01,  2.1492e-01,  6.2283e-01,  2.4497e-01,  5.7258e-01],\n",
            "          [ 2.0373e-02,  3.8580e-01,  4.1535e-01,  2.6712e-01,  2.2740e-01]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4055]]],\n",
            "\n",
            "\n",
            "        [[[4.9193]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4055, 4.9193], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 6] = 0.40552258491516113\n",
            " somado na saída em [1, 1, 7, 6] = 4.919347286224365\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 7, 7\n",
            " alvo do kernel em x: x[1,0,21:26, 21:26]\n",
            " \n",
            " tensor([[0.4616, 0.8413, 0.5506, 0.0674, 0.8557],\n",
            "        [0.1199, 0.0902, 0.9340, 0.1653, 0.6339],\n",
            "        [0.0905, 0.4709, 0.4344, 0.1702, 0.4631],\n",
            "        [0.3167, 0.9850, 0.0940, 0.9277, 0.5286],\n",
            "        [0.5407, 0.6349, 0.6119, 0.8015, 0.0493]])\n",
            " produto: tensor([[[[-3.9823e-03, -1.2012e-04, -3.7632e-04,  2.5786e-04, -1.6141e-02],\n",
            "          [ 1.6173e-03,  9.5580e-04,  1.6956e-02,  4.4623e-04, -3.9567e-03],\n",
            "          [ 1.8783e-03,  2.0087e-02,  1.4065e-02,  1.8216e-03, -3.5993e-03],\n",
            "          [ 1.9583e-02,  7.2442e-02,  8.1076e-03,  3.9953e-02,  1.0397e-02],\n",
            "          [ 4.9674e-02,  7.1870e-02,  6.5218e-02,  4.8857e-02,  5.9449e-04]]],\n",
            "\n",
            "\n",
            "        [[[-6.4723e-02, -2.1875e-02,  6.8526e-02,  1.1383e-02,  5.4609e-02],\n",
            "          [ 1.7709e-03,  2.5502e-02,  5.2263e-01,  8.8169e-02,  2.6629e-01],\n",
            "          [ 1.3311e-02,  2.4621e-01,  3.5257e-01,  1.2653e-01,  2.7215e-01],\n",
            "          [ 8.7182e-02,  6.4219e-01,  8.1677e-02,  7.1766e-01,  3.0725e-01],\n",
            "          [ 1.0475e-01,  3.3729e-01,  4.0090e-01,  3.9595e-01,  1.7655e-02]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[0.4166]]],\n",
            "\n",
            "\n",
            "        [[[5.0556]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([0.4166, 5.0556], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [1, 0, 7, 7] = 0.4166047275066376\n",
            " somado na saída em [1, 1, 7, 7] = 5.055551052093506\n",
            " saida: tensor([[[[0.4532, 0.4274, 0.3197, 0.4404, 0.3630, 0.3665, 0.3368, 0.4017],\n",
            "          [0.3640, 0.4282, 0.3824, 0.4100, 0.4954, 0.3443, 0.3684, 0.3942],\n",
            "          [0.3744, 0.3817, 0.4102, 0.2555, 0.2758, 0.3258, 0.3499, 0.3518],\n",
            "          [0.3846, 0.4530, 0.3316, 0.3320, 0.3891, 0.4545, 0.4794, 0.3398],\n",
            "          [0.3231, 0.2841, 0.4691, 0.3080, 0.4284, 0.4835, 0.4446, 0.3269],\n",
            "          [0.5055, 0.4144, 0.3934, 0.2512, 0.3898, 0.4536, 0.5381, 0.2997],\n",
            "          [0.4870, 0.3975, 0.3895, 0.2418, 0.4798, 0.4418, 0.4729, 0.4232],\n",
            "          [0.3086, 0.3748, 0.4416, 0.3622, 0.4398, 0.3510, 0.3362, 0.2949]],\n",
            "\n",
            "         [[5.1859, 4.7017, 5.5956, 5.5946, 4.2566, 4.6478, 4.9409, 4.5641],\n",
            "          [5.3665, 5.4071, 4.8053, 4.9399, 6.0362, 3.8791, 4.2685, 5.7138],\n",
            "          [4.1752, 4.9405, 5.4898, 3.8603, 3.8344, 4.5160, 4.8805, 4.5350],\n",
            "          [5.5683, 5.5834, 3.4894, 3.9547, 5.2107, 5.6781, 5.5512, 4.4831],\n",
            "          [4.0139, 4.8539, 5.8720, 4.2214, 6.3329, 6.2410, 5.8265, 4.9422],\n",
            "          [6.0706, 5.6438, 4.9768, 3.7373, 6.0675, 6.1532, 5.7374, 4.0055],\n",
            "          [5.7094, 4.7964, 4.3606, 3.3466, 6.3314, 5.9549, 5.8753, 4.4963],\n",
            "          [3.6107, 4.6101, 4.5728, 4.7508, 4.7571, 5.2383, 5.6295, 3.4973]]],\n",
            "\n",
            "\n",
            "        [[[0.3775, 0.4371, 0.4011, 0.3537, 0.3635, 0.4370, 0.3114, 0.3492],\n",
            "          [0.3630, 0.4071, 0.3270, 0.2794, 0.3635, 0.3918, 0.2823, 0.3467],\n",
            "          [0.3029, 0.3180, 0.3954, 0.3256, 0.3860, 0.4680, 0.4946, 0.3805],\n",
            "          [0.3341, 0.2897, 0.3077, 0.3815, 0.4344, 0.3642, 0.4399, 0.3550],\n",
            "          [0.3240, 0.3521, 0.4893, 0.4755, 0.4129, 0.3408, 0.4485, 0.4512],\n",
            "          [0.4013, 0.3260, 0.4189, 0.3539, 0.4818, 0.4626, 0.5620, 0.5586],\n",
            "          [0.3196, 0.2630, 0.4341, 0.3160, 0.4321, 0.4543, 0.4416, 0.3677],\n",
            "          [0.3986, 0.2818, 0.3901, 0.3695, 0.4092, 0.3419, 0.4055, 0.4166]],\n",
            "\n",
            "         [[4.5696, 4.9497, 4.0083, 4.3187, 4.5480, 4.7608, 4.5572, 5.0822],\n",
            "          [5.4620, 5.3067, 4.6187, 3.7535, 4.5743, 4.9094, 4.0697, 5.4169],\n",
            "          [4.4297, 4.1312, 4.4533, 5.0857, 4.6706, 5.5400, 4.9953, 4.1266],\n",
            "          [3.9899, 4.2506, 4.0961, 5.2941, 5.9922, 5.2479, 5.2127, 5.1009],\n",
            "          [4.3355, 4.7630, 6.0193, 5.5846, 4.8966, 4.1577, 6.0780, 5.4145],\n",
            "          [5.3096, 4.5324, 5.4501, 4.3703, 6.6789, 5.9256, 7.0782, 5.7763],\n",
            "          [4.5316, 4.3583, 5.0440, 4.2868, 5.8671, 5.5481, 6.3333, 5.2737],\n",
            "          [4.9385, 3.8961, 4.6617, 4.5143, 5.8753, 4.8333, 4.9193, 5.0556]]]],\n",
            "       grad_fn=<CopySlices>)\n",
            "\n",
            "ndx_amostra: 0\n",
            "\n",
            "ndx_amostra: 1\n",
            " saida apos somar bias: tensor([[[[0.4527, 0.4269, 0.3192, 0.4399, 0.3625, 0.3660, 0.3363, 0.4012],\n",
            "          [0.3635, 0.4277, 0.3819, 0.4095, 0.4949, 0.3438, 0.3679, 0.3937],\n",
            "          [0.3739, 0.3812, 0.4097, 0.2550, 0.2753, 0.3253, 0.3494, 0.3513],\n",
            "          [0.3841, 0.4525, 0.3311, 0.3315, 0.3886, 0.4540, 0.4789, 0.3393],\n",
            "          [0.3226, 0.2836, 0.4686, 0.3075, 0.4279, 0.4830, 0.4441, 0.3264],\n",
            "          [0.5050, 0.4139, 0.3929, 0.2507, 0.3893, 0.4531, 0.5376, 0.2992],\n",
            "          [0.4865, 0.3970, 0.3890, 0.2413, 0.4793, 0.4413, 0.4724, 0.4227],\n",
            "          [0.3081, 0.3743, 0.4411, 0.3617, 0.4393, 0.3505, 0.3357, 0.2944]],\n",
            "\n",
            "         [[5.1804, 4.6962, 5.5901, 5.5892, 4.2512, 4.6424, 4.9355, 4.5587],\n",
            "          [5.3610, 5.4016, 4.7998, 4.9344, 6.0307, 3.8736, 4.2631, 5.7083],\n",
            "          [4.1697, 4.9351, 5.4844, 3.8549, 3.8289, 4.5106, 4.8750, 4.5295],\n",
            "          [5.5628, 5.5779, 3.4839, 3.9492, 5.2052, 5.6726, 5.5458, 4.4776],\n",
            "          [4.0085, 4.8484, 5.8666, 4.2159, 6.3275, 6.2355, 5.8211, 4.9368],\n",
            "          [6.0651, 5.6383, 4.9714, 3.7318, 6.0621, 6.1477, 5.7319, 4.0000],\n",
            "          [5.7040, 4.7909, 4.3551, 3.3411, 6.3260, 5.9495, 5.8699, 4.4909],\n",
            "          [3.6052, 4.6046, 4.5673, 4.7454, 4.7516, 5.2328, 5.6240, 3.4918]]],\n",
            "\n",
            "\n",
            "        [[[0.3770, 0.4366, 0.4006, 0.3532, 0.3630, 0.4365, 0.3109, 0.3487],\n",
            "          [0.3625, 0.4066, 0.3265, 0.2789, 0.3630, 0.3913, 0.2818, 0.3462],\n",
            "          [0.3024, 0.3175, 0.3949, 0.3251, 0.3855, 0.4675, 0.4941, 0.3800],\n",
            "          [0.3336, 0.2892, 0.3072, 0.3810, 0.4339, 0.3637, 0.4394, 0.3545],\n",
            "          [0.3235, 0.3516, 0.4888, 0.4750, 0.4124, 0.3404, 0.4480, 0.4507],\n",
            "          [0.4008, 0.3255, 0.4184, 0.3534, 0.4813, 0.4621, 0.5615, 0.5581],\n",
            "          [0.3191, 0.2625, 0.4336, 0.3155, 0.4316, 0.4538, 0.4411, 0.3672],\n",
            "          [0.3981, 0.2813, 0.3896, 0.3690, 0.4087, 0.3414, 0.4050, 0.4161]],\n",
            "\n",
            "         [[4.5641, 4.9443, 4.0029, 4.3132, 4.5425, 4.7553, 4.5518, 5.0767],\n",
            "          [5.4566, 5.3012, 4.6132, 3.7480, 4.5689, 4.9039, 4.0642, 5.4115],\n",
            "          [4.4242, 4.1257, 4.4479, 5.0802, 4.6652, 5.5345, 4.9898, 4.1211],\n",
            "          [3.9844, 4.2451, 4.0907, 5.2886, 5.9868, 5.2424, 5.2073, 5.0954],\n",
            "          [4.3301, 4.7575, 6.0139, 5.5792, 4.8911, 4.1522, 6.0725, 5.4090],\n",
            "          [5.3041, 4.5269, 5.4446, 4.3648, 6.6734, 5.9201, 7.0727, 5.7708],\n",
            "          [4.5261, 4.3528, 5.0386, 4.2814, 5.8617, 5.5426, 6.3278, 5.2683],\n",
            "          [4.9330, 3.8906, 4.6562, 4.5088, 5.8698, 4.8278, 4.9139, 5.0501]]]],\n",
            "       grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSTrD1WRnGng",
        "outputId": "7b5d641c-e164-4bcf-9b47-f06d0feecccb"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "pytorch_conv_layer = torch.nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=0)\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_conv_weight, bias=initial_conv_bias))\n",
        "\n",
        "target_out = pytorch_conv_layer(x)"
      ],
      "metadata": {
        "id": "HzIjuGpWlbIM"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Sesz6onCEw",
        "outputId": "6aa0eefd-d758-410e-ce6b-115686d7de3e"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ],
      "metadata": {
        "id": "noWWzeCumRIY"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inicializa_seed(123)"
      ],
      "metadata": {
        "id": "A7ToXbzHhkqS"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:10.802708Z",
          "start_time": "2018-08-20T21:03:10.793287Z"
        },
        "id": "_8Eg4h_kQT2H"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, height_in: int, width_in: int, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "   \n",
        "        height_out = (height_in - kernel_size) // stride + 1\n",
        "        width_out = (width_in - kernel_size) // stride + 1\n",
        "        self.classification_layer = torch.nn.Linear(out_channels * height_out * width_out, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.conv_layer(x)\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        hidden = hidden.reshape(x.shape[0], -1)\n",
        "        logits = self.classification_layer(hidden)\n",
        "        return logits"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NHQB4wGQT2K"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "### Definição dos hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:14.146259Z",
          "start_time": "2018-08-20T21:03:14.139515Z"
        },
        "id": "oZuYEkn_QT2M"
      },
      "source": [
        "n_epochs = 50\n",
        "lr = 0.1"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "### Laço de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(height_in=height_in, width_in=width_in, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "# Usa pesos iniciais pré-difinidos\n",
        "model.classification_layer.load_state_dict(dict(weight=initial_classification_weight, bias=initial_classification_bias))\n",
        "model.conv_layer.weight.data = initial_conv_weight\n",
        "model.conv_layer.bias.data = initial_conv_bias\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n"
      ],
      "metadata": {
        "id": "FzPTtf17cH8o"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J6NPaYEPhtA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for x_train, y_train in loader_train:\n",
        "        # predict da rede\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GviNVFSEcJVY",
        "outputId": "f04f838c-c811-40cf-9324-ebbcb78f4e02"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/49 Loss: 2.303267478942871\n",
            "Epoch: 1/49 Loss: 2.227701187133789\n",
            "Epoch: 2/49 Loss: 1.0923893451690674\n",
            "Epoch: 3/49 Loss: 0.5867354869842529\n",
            "Epoch: 4/49 Loss: 0.5144089460372925\n",
            "Epoch: 5/49 Loss: 0.45026639103889465\n",
            "Epoch: 6/49 Loss: 0.4075140655040741\n",
            "Epoch: 7/49 Loss: 0.3771387040615082\n",
            "Epoch: 8/49 Loss: 0.3534485697746277\n",
            "Epoch: 9/49 Loss: 0.33414512872695923\n",
            "Epoch: 10/49 Loss: 0.3181140124797821\n",
            "Epoch: 11/49 Loss: 0.30457887053489685\n",
            "Epoch: 12/49 Loss: 0.29283490777015686\n",
            "Epoch: 13/49 Loss: 0.2827607989311218\n",
            "Epoch: 14/49 Loss: 0.2738332152366638\n",
            "Epoch: 15/49 Loss: 0.2657742500305176\n",
            "Epoch: 16/49 Loss: 0.2583288252353668\n",
            "Epoch: 17/49 Loss: 0.25117504596710205\n",
            "Epoch: 18/49 Loss: 0.2443971037864685\n",
            "Epoch: 19/49 Loss: 0.237899512052536\n",
            "Epoch: 20/49 Loss: 0.23167704045772552\n",
            "Epoch: 21/49 Loss: 0.2256264090538025\n",
            "Epoch: 22/49 Loss: 0.21984538435935974\n",
            "Epoch: 23/49 Loss: 0.21429121494293213\n",
            "Epoch: 24/49 Loss: 0.2089422643184662\n",
            "Epoch: 25/49 Loss: 0.20387302339076996\n",
            "Epoch: 26/49 Loss: 0.19903427362442017\n",
            "Epoch: 27/49 Loss: 0.19439974427223206\n",
            "Epoch: 28/49 Loss: 0.18994106352329254\n",
            "Epoch: 29/49 Loss: 0.18563997745513916\n",
            "Epoch: 30/49 Loss: 0.18147501349449158\n",
            "Epoch: 31/49 Loss: 0.17744918167591095\n",
            "Epoch: 32/49 Loss: 0.173472598195076\n",
            "Epoch: 33/49 Loss: 0.16947467625141144\n",
            "Epoch: 34/49 Loss: 0.16547319293022156\n",
            "Epoch: 35/49 Loss: 0.16150493919849396\n",
            "Epoch: 36/49 Loss: 0.15746402740478516\n",
            "Epoch: 37/49 Loss: 0.15340439975261688\n",
            "Epoch: 38/49 Loss: 0.14926929771900177\n",
            "Epoch: 39/49 Loss: 0.14520639181137085\n",
            "Epoch: 40/49 Loss: 0.1412365585565567\n",
            "Epoch: 41/49 Loss: 0.13712674379348755\n",
            "Epoch: 42/49 Loss: 0.13310375809669495\n",
            "Epoch: 43/49 Loss: 0.12914662063121796\n",
            "Epoch: 44/49 Loss: 0.125150665640831\n",
            "Epoch: 45/49 Loss: 0.1211676299571991\n",
            "Epoch: 46/49 Loss: 0.11731720715761185\n",
            "Epoch: 47/49 Loss: 0.11364629864692688\n",
            "Epoch: 48/49 Loss: 0.1100190058350563\n",
            "Epoch: 49/49 Loss: 0.10655989497900009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLL-GQlKQT2Y"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:55.246851Z",
          "start_time": "2018-08-20T21:03:54.994428Z"
        },
        "id": "w38EtNxhQT2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f24a7234-8c9e-4135-8ce1-348633fd816a"
      },
      "source": [
        "n_batches_train = len(loader_train)\n",
        "plt.plot(epochs[::n_batches_train], loss_history[::n_batches_train])\n",
        "plt.xlabel('época')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'época')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_epoch_end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_0RGftSe0ss",
        "outputId": "5ebfb1f0-2309-4b3f-9cf1-f3ef3fbb9608"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.303267478942871,\n",
              " 2.227701187133789,\n",
              " 1.0923893451690674,\n",
              " 0.5867354869842529,\n",
              " 0.5144089460372925,\n",
              " 0.45026639103889465,\n",
              " 0.4075140655040741,\n",
              " 0.3771387040615082,\n",
              " 0.3534485697746277,\n",
              " 0.33414512872695923,\n",
              " 0.3181140124797821,\n",
              " 0.30457887053489685,\n",
              " 0.29283490777015686,\n",
              " 0.2827607989311218,\n",
              " 0.2738332152366638,\n",
              " 0.2657742500305176,\n",
              " 0.2583288252353668,\n",
              " 0.25117504596710205,\n",
              " 0.2443971037864685,\n",
              " 0.237899512052536,\n",
              " 0.23167704045772552,\n",
              " 0.2256264090538025,\n",
              " 0.21984538435935974,\n",
              " 0.21429121494293213,\n",
              " 0.2089422643184662,\n",
              " 0.20387302339076996,\n",
              " 0.19903427362442017,\n",
              " 0.19439974427223206,\n",
              " 0.18994106352329254,\n",
              " 0.18563997745513916,\n",
              " 0.18147501349449158,\n",
              " 0.17744918167591095,\n",
              " 0.173472598195076,\n",
              " 0.16947467625141144,\n",
              " 0.16547319293022156,\n",
              " 0.16150493919849396,\n",
              " 0.15746402740478516,\n",
              " 0.15340439975261688,\n",
              " 0.14926929771900177,\n",
              " 0.14520639181137085,\n",
              " 0.1412365585565567,\n",
              " 0.13712674379348755,\n",
              " 0.13310375809669495,\n",
              " 0.12914662063121796,\n",
              " 0.125150665640831,\n",
              " 0.1211676299571991,\n",
              " 0.11731720715761185,\n",
              " 0.11364629864692688,\n",
              " 0.1100190058350563,\n",
              " 0.10655989497900009]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:48.786969Z",
          "start_time": "2018-08-20T21:03:48.781787Z"
        },
        "id": "PiuMsjYtQT2R"
      },
      "source": [
        "# Assert do histórico de losses\n",
        "target_loss_epoch_end = np.array([\n",
        "    2.303267478942871,\n",
        "    2.227701187133789,\n",
        "    1.0923893451690674,\n",
        "    0.5867354869842529,\n",
        "    0.5144089460372925,\n",
        "    0.45026642084121704,\n",
        "    0.4075140357017517,\n",
        "    0.37713879346847534,\n",
        "    0.3534485101699829,\n",
        "    0.3341451585292816,\n",
        "    0.3181140422821045,\n",
        "    0.30457887053489685,\n",
        "    0.29283496737480164,\n",
        "    0.2827608287334442,\n",
        "    0.2738332152366638,\n",
        "    0.2657742500305176,\n",
        "    0.2583288848400116,\n",
        "    0.25117507576942444,\n",
        "    0.24439716339111328,\n",
        "    0.23789969086647034,\n",
        "    0.23167723417282104,\n",
        "    0.22562651336193085,\n",
        "    0.21984536945819855,\n",
        "    0.2142913043498993,\n",
        "    0.20894232392311096,\n",
        "    0.203872948884964,\n",
        "    0.19903430342674255,\n",
        "    0.19439971446990967,\n",
        "    0.18994088470935822,\n",
        "    0.18563991785049438,\n",
        "    0.18147490918636322,\n",
        "    0.17744913697242737,\n",
        "    0.17347246408462524,\n",
        "    0.16947467625141144,\n",
        "    0.16547319293022156,\n",
        "    0.16150487959384918,\n",
        "    0.1574639081954956,\n",
        "    0.1534043848514557,\n",
        "    0.14926929771900177,\n",
        "    0.1452063024044037,\n",
        "    0.1412365883588791,\n",
        "    0.13712672889232635,\n",
        "    0.1331038922071457,\n",
        "    0.1291467249393463,\n",
        "    0.1251506358385086,\n",
        "    0.12116757035255432,\n",
        "    0.11731722950935364,\n",
        "    0.11364627629518509,\n",
        "    0.11001908034086227,\n",
        "    0.10655981302261353])\n",
        "\n",
        "assert np.allclose(np.array(loss_epoch_end), target_loss_epoch_end, atol=1e-6)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rascunho"
      ],
      "metadata": {
        "id": "yy2fvfxRnWDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u_spC95IesPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execução com queda maior na loss"
      ],
      "metadata": {
        "id": "BXOsrCloekqY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-08-20T21:03:40.796410Z",
          "start_time": "2018-08-20T21:03:39.771981Z"
        },
        "id": "L5T_jZZPQT2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029019a6-2a0b-445b-83b9-55e699399619"
      },
      "source": [
        "model = Net(height_in=height_in, width_in=width_in, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "# Usa pesos iniciais pré-difinidos\n",
        "model.classification_layer.load_state_dict(dict(weight=initial_classification_weight, bias=initial_classification_bias))\n",
        "model.conv_layer.weight.data = initial_conv_weight\n",
        "model.conv_layer.bias.data = initial_conv_bias\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for x_train, y_train in loader_train:\n",
        "        # predict da rede\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/49 Loss: 0.9586364030838013\n",
            "Epoch: 1/49 Loss: 0.565815269947052\n",
            "Epoch: 2/49 Loss: 0.4770318567752838\n",
            "Epoch: 3/49 Loss: 0.42453765869140625\n",
            "Epoch: 4/49 Loss: 0.38708603382110596\n",
            "Epoch: 5/49 Loss: 0.35843756794929504\n",
            "Epoch: 6/49 Loss: 0.336288183927536\n",
            "Epoch: 7/49 Loss: 0.31845659017562866\n",
            "Epoch: 8/49 Loss: 0.3037392497062683\n",
            "Epoch: 9/49 Loss: 0.2910858988761902\n",
            "Epoch: 10/49 Loss: 0.2799944579601288\n",
            "Epoch: 11/49 Loss: 0.270416259765625\n",
            "Epoch: 12/49 Loss: 0.2619065046310425\n",
            "Epoch: 13/49 Loss: 0.25361135601997375\n",
            "Epoch: 14/49 Loss: 0.24582411348819733\n",
            "Epoch: 15/49 Loss: 0.23841428756713867\n",
            "Epoch: 16/49 Loss: 0.23117700219154358\n",
            "Epoch: 17/49 Loss: 0.2241152971982956\n",
            "Epoch: 18/49 Loss: 0.2175927609205246\n",
            "Epoch: 19/49 Loss: 0.21121275424957275\n",
            "Epoch: 20/49 Loss: 0.2050437331199646\n",
            "Epoch: 21/49 Loss: 0.19928722083568573\n",
            "Epoch: 22/49 Loss: 0.1938953399658203\n",
            "Epoch: 23/49 Loss: 0.1882905215024948\n",
            "Epoch: 24/49 Loss: 0.1828724443912506\n",
            "Epoch: 25/49 Loss: 0.1776149421930313\n",
            "Epoch: 26/49 Loss: 0.17248651385307312\n",
            "Epoch: 27/49 Loss: 0.16733810305595398\n",
            "Epoch: 28/49 Loss: 0.16254152357578278\n",
            "Epoch: 29/49 Loss: 0.15758882462978363\n",
            "Epoch: 30/49 Loss: 0.1527339220046997\n",
            "Epoch: 31/49 Loss: 0.14789022505283356\n",
            "Epoch: 32/49 Loss: 0.14303667843341827\n",
            "Epoch: 33/49 Loss: 0.1380799561738968\n",
            "Epoch: 34/49 Loss: 0.13330009579658508\n",
            "Epoch: 35/49 Loss: 0.12855049967765808\n",
            "Epoch: 36/49 Loss: 0.12378916144371033\n",
            "Epoch: 37/49 Loss: 0.119150809943676\n",
            "Epoch: 38/49 Loss: 0.11474869400262833\n",
            "Epoch: 39/49 Loss: 0.110462486743927\n",
            "Epoch: 40/49 Loss: 0.10636704415082932\n",
            "Epoch: 41/49 Loss: 0.10245434939861298\n",
            "Epoch: 42/49 Loss: 0.09870947152376175\n",
            "Epoch: 43/49 Loss: 0.09508325904607773\n",
            "Epoch: 44/49 Loss: 0.09160909056663513\n",
            "Epoch: 45/49 Loss: 0.08813302963972092\n",
            "Epoch: 46/49 Loss: 0.08475378155708313\n",
            "Epoch: 47/49 Loss: 0.08139956742525101\n",
            "Epoch: 48/49 Loss: 0.07817484438419342\n",
            "Epoch: 49/49 Loss: 0.07505226880311966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_epoch_end"
      ],
      "metadata": {
        "id": "ToktJu4CK94z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd61d51-0172-464c-cc0b-d023eabc5622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9586364030838013,\n",
              " 0.565815269947052,\n",
              " 0.4770318567752838,\n",
              " 0.42453765869140625,\n",
              " 0.38708603382110596,\n",
              " 0.35843756794929504,\n",
              " 0.336288183927536,\n",
              " 0.31845659017562866,\n",
              " 0.3037392497062683,\n",
              " 0.2910858988761902,\n",
              " 0.2799944579601288,\n",
              " 0.270416259765625,\n",
              " 0.2619065046310425,\n",
              " 0.25361135601997375,\n",
              " 0.24582411348819733,\n",
              " 0.23841428756713867,\n",
              " 0.23117700219154358,\n",
              " 0.2241152971982956,\n",
              " 0.2175927609205246,\n",
              " 0.21121275424957275,\n",
              " 0.2050437331199646,\n",
              " 0.19928722083568573,\n",
              " 0.1938953399658203,\n",
              " 0.1882905215024948,\n",
              " 0.1828724443912506,\n",
              " 0.1776149421930313,\n",
              " 0.17248651385307312,\n",
              " 0.16733810305595398,\n",
              " 0.16254152357578278,\n",
              " 0.15758882462978363,\n",
              " 0.1527339220046997,\n",
              " 0.14789022505283356,\n",
              " 0.14303667843341827,\n",
              " 0.1380799561738968,\n",
              " 0.13330009579658508,\n",
              " 0.12855049967765808,\n",
              " 0.12378916144371033,\n",
              " 0.119150809943676,\n",
              " 0.11474869400262833,\n",
              " 0.110462486743927,\n",
              " 0.10636704415082932,\n",
              " 0.10245434939861298,\n",
              " 0.09870947152376175,\n",
              " 0.09508325904607773,\n",
              " 0.09160909056663513,\n",
              " 0.08813302963972092,\n",
              " 0.08475378155708313,\n",
              " 0.08139956742525101,\n",
              " 0.07817484438419342,\n",
              " 0.07505226880311966]"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outros testes de convolução"
      ],
      "metadata": {
        "id": "sAAKJ6rnet3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels_dummy = 1\n",
        "out_channels_dummy = 2\n",
        "kernel_size_dummy = 3\n",
        "stride_dummy = 2\n",
        "num_amostras_dummy = 1\n",
        "x = torch.arange(30).float().reshape(num_amostras_dummy, 1, 5, 6)"
      ],
      "metadata": {
        "id": "eHg1E0xkGn-F"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfc7424-fe9d-4b9b-dbc8-63eef6638f0b",
        "id": "GLcaZY0nGn-F"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
            "          [12., 13., 14., 15., 16., 17.],\n",
            "          [18., 19., 20., 21., 22., 23.],\n",
            "          [24., 25., 26., 27., 28., 29.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# falta stride e tratar out_channels > 1"
      ],
      "metadata": {
        "id": "G4-z1oWfI5j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_layer = MyConv2d( in_channels=in_channels_dummy,out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, verbose=True)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "initial_weights_dummy = torch.arange(in_channels_dummy * out_channels_dummy * kernel_size_dummy * kernel_size_dummy).float()\n",
        "initial_weights_dummy = initial_weights_dummy.reshape(out_channels_dummy, in_channels_dummy,  kernel_size_dummy, kernel_size_dummy)\n",
        "initial_bias_dummy = torch.arange(out_channels_dummy,).float()\n",
        "print(f\"initial_bias_dummy.shape {initial_bias_dummy.shape}, initial_weights_dummy.shape {initial_weights_dummy.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cc0a62-a67c-4d07-a76a-6b039f15dcdf",
        "id": "m5oMLsHsGn-G"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inicializado MyConv2d\n",
            "in_channels: 1 \n",
            "out_channels: 2 \n",
            "kernel_size: 3 \n",
            "stride: 2 \n",
            "weight.shape: torch.Size([2, 1, 3, 3]) \n",
            "weight: Parameter containing:\n",
            "tensor([[[[-0.0082, -0.0041, -0.0035],\n",
            "          [ 0.0023, -0.0072,  0.0030],\n",
            "          [-0.0095,  0.0072, -0.0016]]],\n",
            "\n",
            "\n",
            "        [[[-0.0069, -0.0036,  0.0015],\n",
            "          [ 0.0053, -0.0033, -0.0039],\n",
            "          [ 0.0075,  0.0062, -0.0042]]]], requires_grad=True) \n",
            "bias.shape: torch.Size([2]) \n",
            "bias: Parameter containing:\n",
            "tensor([ 0.0033, -0.0044], requires_grad=True) \n",
            "initial_bias_dummy.shape torch.Size([2]), initial_weights_dummy.shape torch.Size([2, 1, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer.weight.data = initial_weights_dummy\n",
        "conv_layer.bias.data = initial_bias_dummy\n",
        "print(f\"conv_layer.weight.data: {conv_layer.weight.data}\")\n",
        "print(f\"conv_layer.bias.data: {conv_layer.bias.data}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Gr8N8vXkkv",
        "outputId": "95ef5782-bb28-489d-bf51-9e630cd1748d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv_layer.weight.data: tensor([[[[ 0.,  1.,  2.],\n",
            "          [ 3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.]]],\n",
            "\n",
            "\n",
            "        [[[ 9., 10., 11.],\n",
            "          [12., 13., 14.],\n",
            "          [15., 16., 17.]]]])\n",
            "conv_layer.bias.data: tensor([0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW1S0eXhYQ21",
        "outputId": "fa0288df-0420-4e37-e1a5-d10709c8c0e5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
            "          [ 6.,  7.,  8.,  9., 10., 11.],\n",
            "          [12., 13., 14., 15., 16., 17.],\n",
            "          [18., 19., 20., 21., 22., 23.],\n",
            "          [24., 25., 26., 27., 28., 29.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = conv_layer(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVysHnpYX4O2",
        "outputId": "23a3dca3-fb28-4aa2-f2b3-c2d0170fb999"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " num_amostras: 1, self.out_channels: 2, num_linhas_entrada: 5, num_colunas_entrada: 6, num_linhas_saida: 2, num_colunas_saida: 2\n",
            "saida.shape: torch.Size([1, 2, 2, 2])\n",
            "\n",
            "ndx_amostra: 0\n",
            "\n",
            "ndx_in_channels: 0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 0\n",
            " alvo do kernel em x: x[0,0,0:3, 0:3]\n",
            " \n",
            " tensor([[ 0.,  1.,  2.],\n",
            "        [ 6.,  7.,  8.],\n",
            "        [12., 13., 14.]])\n",
            " produto: tensor([[[[  0.,   1.,   4.],\n",
            "          [ 18.,  28.,  40.],\n",
            "          [ 72.,  91., 112.]]],\n",
            "\n",
            "\n",
            "        [[[  0.,  10.,  22.],\n",
            "          [ 72.,  91., 112.],\n",
            "          [180., 208., 238.]]]], grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[366.]]],\n",
            "\n",
            "\n",
            "        [[[933.]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([366., 933.], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 0] = 366.0\n",
            " somado na saída em [0, 1, 0, 0] = 933.0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 0, 1\n",
            " alvo do kernel em x: x[0,0,0:3, 2:5]\n",
            " \n",
            " tensor([[ 2.,  3.,  4.],\n",
            "        [ 8.,  9., 10.],\n",
            "        [14., 15., 16.]])\n",
            " produto: tensor([[[[  0.,   3.,   8.],\n",
            "          [ 24.,  36.,  50.],\n",
            "          [ 84., 105., 128.]]],\n",
            "\n",
            "\n",
            "        [[[ 18.,  30.,  44.],\n",
            "          [ 96., 117., 140.],\n",
            "          [210., 240., 272.]]]], grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[ 438.]]],\n",
            "\n",
            "\n",
            "        [[[1167.]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([ 438., 1167.], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 0, 1] = 438.0\n",
            " somado na saída em [0, 1, 0, 1] = 1167.0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 0\n",
            " alvo do kernel em x: x[0,0,2:5, 0:3]\n",
            " \n",
            " tensor([[12., 13., 14.],\n",
            "        [18., 19., 20.],\n",
            "        [24., 25., 26.]])\n",
            " produto: tensor([[[[  0.,  13.,  28.],\n",
            "          [ 54.,  76., 100.],\n",
            "          [144., 175., 208.]]],\n",
            "\n",
            "\n",
            "        [[[108., 130., 154.],\n",
            "          [216., 247., 280.],\n",
            "          [360., 400., 442.]]]], grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[ 798.]]],\n",
            "\n",
            "\n",
            "        [[[2337.]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([ 798., 2337.], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 0] = 798.0\n",
            " somado na saída em [0, 1, 1, 0] = 2337.0\n",
            "\n",
            "ndx_linhas_saida, ndx_colunas_saida: 1, 1\n",
            " alvo do kernel em x: x[0,0,2:5, 2:5]\n",
            " \n",
            " tensor([[14., 15., 16.],\n",
            "        [20., 21., 22.],\n",
            "        [26., 27., 28.]])\n",
            " produto: tensor([[[[  0.,  15.,  32.],\n",
            "          [ 60.,  84., 110.],\n",
            "          [156., 189., 224.]]],\n",
            "\n",
            "\n",
            "        [[[126., 150., 176.],\n",
            "          [240., 273., 308.],\n",
            "          [390., 432., 476.]]]], grad_fn=<MulBackward0>)\n",
            " soma: tensor([[[[ 870.]]],\n",
            "\n",
            "\n",
            "        [[[2571.]]]], grad_fn=<SumBackward1>)\n",
            " valor_soma: tensor([ 870., 2571.], grad_fn=<SqueezeBackward0>)\n",
            " somado na saída em [0, 0, 1, 1] = 870.0\n",
            " somado na saída em [0, 1, 1, 1] = 2571.0\n",
            " saida: tensor([[[[ 366.,  438.],\n",
            "          [ 798.,  870.]],\n",
            "\n",
            "         [[ 933., 1167.],\n",
            "          [2337., 2571.]]]], grad_fn=<CopySlices>)\n",
            "\n",
            "ndx_amostra: 0\n",
            " saida apos somar bias: tensor([[[[ 366.,  438.],\n",
            "          [ 798.,  870.]],\n",
            "\n",
            "         [[ 934., 1168.],\n",
            "          [2338., 2572.]]]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ED6gJjl7GonF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Vcc7RvMGo2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([[[[34.]]]])"
      ],
      "metadata": {
        "id": "JZUW6dbGeULx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ATxh3DkQlS",
        "outputId": "6dea88af-1ef6-410b-d2e1-772f55b58de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(34.)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijHjn6Fwk_6S",
        "outputId": "fc8bbe28-ae31-472d-d2bd-50cfaa6eae0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze().reshape(1,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrEMRgHxk7UK",
        "outputId": "d9d8d83d-cb40-4844-8e21-c1170d9417b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([34.])"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.view(1,-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx_BICd-kVsC",
        "outputId": "7242672c-05da-4aeb-b8bd-e72aadfa04c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[34.]])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.tensor([[[[ 46.]], [[134.]]]])"
      ],
      "metadata": {
        "id": "xK5LO9EikKLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zgWxyfKeYhZ",
        "outputId": "758c69e0-7669-4021-b5b7-3f364d0222e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 46., 134.])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ouMw6ClWhp",
        "outputId": "f7e23e94-af1e-4251-b57a-ffc992ddede0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze().reshape(2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVj0YgbZknZE",
        "outputId": "634e9b79-0ca4-4b6c-cb05-903b27262855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 46., 134.])"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp.squeeze().shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NKOwsD_eYsI",
        "outputId": "8bade88a-be35-4a88-ba2c-cb85f05613e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    }
  ]
}