{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ex08_patrick_ferreira",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex08/patrick_ferreira/ex08_patrick_ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "id": "jOdQB41_4ZxG"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IbuChoAPMEn"
   },
   "source": [
    "#  Exercício: Modelo de Linguagem com auto-atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DBb0-Klwf2"
   },
   "source": [
    "Este exercício é similar ao da Aula 7, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
    "\n",
    "Na camada de auto-atenção, não se esqueça de implementar:\n",
    "- Embeddings de posição\n",
    "- Projeções lineares (WQ, WK, WV, WO)\n",
    "- Conexões residuais\n",
    "- Camada de feed forward (2-layer MLP)\n",
    "\n",
    "\n",
    "\n",
    "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
    "\n",
    "Alguns conselhos úteis:\n",
    "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
    "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
    "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "3twP0YJC4jmJ"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnyhJZtTRNMx"
   },
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qlIOVCajPWcU"
   },
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "w9f3PfifAwpU"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 25 18:34:51 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 65%   47C    P8    13W / 170W |    387MiB / 12288MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1967      G   /usr/lib/xorg/Xorg                185MiB |\r\n",
      "|    0   N/A  N/A      2100      G   ...ome-remote-desktop-daemon        2MiB |\r\n",
      "|    0   N/A  N/A      2136      G   /usr/bin/gnome-shell               33MiB |\r\n",
      "|    0   N/A  N/A     14610      G   ..._14168.log --shared-files       27MiB |\r\n",
      "|    0   N/A  N/A     23675      G   ...501969289961926581,131072      136MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "id": "whTCe2i7AtoV"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfxgV2DUk58"
   },
   "source": [
    "## Implementação do MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n_xhKm1EZ3bQ"
   },
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize(text: str, tokenizer):\n",
    "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
    "        # Escreva seu código aqui\n",
    "        try:\n",
    "            self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            self.y = np.load(\"y_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "\n",
    "            print(\"Carregando dataset preprocessado\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Excecao: \", e)\n",
    "            print(\"Montando dataset\")\n",
    "\n",
    "            self.x = list()\n",
    "            self.y = list()\n",
    "\n",
    "            for text in tqdm(texts):\n",
    "                tokens_key = tokenize(text, tokenizer)\n",
    "                for i in range(len(tokens_key)-context_size):\n",
    "                    self.x.append(tokens_key[i:i+context_size])\n",
    "                    self.y.append(tokens_key[i+context_size])\n",
    "\n",
    "            self.x = np.array(self.x)\n",
    "            self.y = np.array(self.y)\n",
    "\n",
    "            np.save(\"x_\" + str(len(texts)) + \".npy\", self.x)\n",
    "            np.save(\"y_\" + str(len(texts)) + \".npy\", self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Escreva seu código aqui\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Escreva seu código aqui\n",
    "        return torch.tensor(self.x[idx]).long(), torch.tensor(self.y[idx]).long()"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando se a implementação do MyDataset está correta"
   ],
   "metadata": {
    "id": "wew-gFbWeBTq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
    "assert len(dummy_dataset) == 5\n",
    "print('passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[3396, 10303, 125],\n",
    "     [1660, 5971, 785],\n",
    "     [5971, 785, 125],\n",
    "     [785, 125, 1847],\n",
    "     [125, 1847, 13779]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor([13239, 125, 1847, 13779, 15616])\n",
    "\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ],
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LfrHHouleJ0"
   },
   "source": [
    "# Carregamento do dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2vFWjsSkmop"
   },
   "source": [
    "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
   ],
   "metadata": {
    "id": "vGlN1WqrXPA6"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sample_brwac.txt’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "valid_examples = 101\n",
    "test_examples = 100\n",
    "texts = open('sample_brwac.txt').readlines()\n",
    "\n",
    "# print('Truncating for debugging purposes.')\n",
    "# texts = texts[:500]\n",
    "\n",
    "training_texts = texts[:-(valid_examples + test_examples)]\n",
    "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
    "test_texts = texts[-test_examples:]\n",
    "\n",
    "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
   ],
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "id": "KCSGJ5m7py4c"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 27675765\n",
      "valid examples: 82250\n",
      "test examples: 166726\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGaAjYDfWdd1"
   },
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            context_size (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # hidden_size for MLP\n",
    "        hidden_size = 10 * embedding_dim\n",
    "\n",
    "        # tokens (words indexes) embedding and positional embedding\n",
    "        self.c_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.p_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "        self.w_k = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "        self.w_v = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "        self.w_0 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # output MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.linear_output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.skip_connection1 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.skip_connection2 = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            logits of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "\n",
    "        input_embeddings = self.c_embedding(inputs)\n",
    "\n",
    "        positional_indexes = torch.arange(self.context_size).view(1, -1).repeat(inputs.shape[0], 1).to(device)\n",
    "        positional_embeddings = self.p_embedding(positional_indexes)\n",
    "\n",
    "        x_embeddings = positional_embeddings + input_embeddings\n",
    "\n",
    "        k = self.w_k(x_embeddings)\n",
    "        v = self.w_v(x_embeddings)\n",
    "        q = self.w_q(x_embeddings)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(1, 2))\n",
    "        probabilities = self.softmax(scores)\n",
    "\n",
    "        e = self.w_0(self.norm1(self.skip_connection1(x_embeddings) + torch.matmul(probabilities, v)))\n",
    "\n",
    "        # We only take the last word to make prediction\n",
    "        logits = self.mlp(e[:, -1, :].flatten(start_dim=1))\n",
    "\n",
    "        return self.linear_output(self.norm2(logits + self.skip_connection2(e[:, -1, :].flatten(start_dim=1))))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teste o modelo com um exemplo"
   ],
   "metadata": {
    "id": "Rm6_PTH2i98e"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RwnxfZlrZoT_"
   },
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=64,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 29794])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I3Vh6B-VkA01"
   },
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of model parameters: {num_params}')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 6171938\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assert da Perplexidade\n"
   ],
   "metadata": {
    "id": "8nhbUVsYnVAp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def perplexity(logits, target):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity\n",
    "    \"\"\"\n",
    "    loss = nn.functional.cross_entropy(logits, target, reduction='mean')\n",
    "    return torch.exp(loss)\n",
    "\n",
    "\n",
    "# n_examples = 1000\n",
    "#\n",
    "# sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "# sample_train_gpu = sample_train.to(device)\n",
    "# target_token_ids = target_token_ids.to(device)\n",
    "# logits = model(sample_train_gpu)\n",
    "#\n",
    "# my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
    "#\n",
    "# print(f'my perplexity:              {int(my_perplexity)}')\n",
    "# print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "#\n",
    "# assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
    "# print('Passou o no assert da perplexidade')"
   ],
   "metadata": {
    "id": "gbMP8VAUncfX"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Laço de Treinamento e Validação"
   ],
   "metadata": {
    "id": "KiJtrsqPnE_l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class DataManager(Thread):\n",
    "    def __init__(self, data_loader, buffer_size=3, device=torch.device(\"cpu\"), data_type=torch.float32):\n",
    "        \"\"\"\n",
    "This manager intends to load a PyTorch dataloader like from disk into memory,\n",
    "reducing the acess time. It does not easily overflow memory, because we set a\n",
    "buffer size limiting how many samples will be loaded at once. Everytime a sample\n",
    "is consumed by the calling thread, another one is replaced in the\n",
    "buffer (unless we reach the end of dataloader).\n",
    "\n",
    "A manger may be called exactly like a dataloader, an it's based in an internal\n",
    "thread that loads samples into memory in parallel. This is specially useful\n",
    "when you are training in GPU and processor is almost idle.\n",
    "\n",
    "        :param data_loader: Base dataloader to load in parallel.\n",
    "        :param buffer_size: How many samples to keep loaded (caution to not overflow RAM). Default: 3.\n",
    "        :param device: Torch device to put samples in, like torch.device(\"cpu\") (default). It saves time by transfering in parallel.\n",
    "        :param data_type: Automatically casts tensor type. Default: torch.float32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.buffer_queue = Queue(maxsize=buffer_size)\n",
    "        self.data_loader = data_loader\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.dataloader_finished = False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "Runs the internal thread that iterates over the dataloader until fulfilling the\n",
    "buffer or the end of samples.\n",
    "        \"\"\"\n",
    "        for i, (x, y) in enumerate(self.data_loader):\n",
    "            # Important to set before put in queue to avoid race condition\n",
    "            # would happen if trying to get() in next() method before setting this flag\n",
    "            if i >= len(self) - 1:\n",
    "                self.dataloader_finished = True\n",
    "\n",
    "            self.buffer_queue.put([x.to(self.data_type).to(self.device),\n",
    "                                   y.to(self.data_type).to(self.device)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "Returns an iterable of itself.\n",
    "\n",
    "        :return: Iterable around this class.\n",
    "        \"\"\"\n",
    "        self.start()\n",
    "        self.dataloader_finished = False\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "Intended to be used as iterator.\n",
    "\n",
    "        :return: Next iteration element.\n",
    "        \"\"\"\n",
    "        if self.dataloader_finished is True and self.buffer_queue.empty():\n",
    "            raise StopIteration()\n",
    "\n",
    "        return self.buffer_queue.get()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIMSaY-UUGUE"
   },
   "source": [
    "max_examples = 800_000_000\n",
    "eval_every_steps = 10000\n",
    "lr = 3e-4\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=128,\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, drop_last=True, num_workers=2)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train_step(input, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    logits = model(input.to(device))\n",
    "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input, target):\n",
    "    model.eval()\n",
    "    logits = model(input)\n",
    "    loss = nn.functional.cross_entropy(logits, target)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "while n_examples < max_examples:\n",
    "    for input, target in DataManager(train_loader, device=device, buffer_size=4, data_type=None):\n",
    "        loss = train_step(input.to(device), target.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(input.to(device), target.to(device))\n",
    "                    for input, target in DataManager(validation_loader, device=device, buffer_size=4, data_type=None)]))\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(input)  # Increment of batch size\n",
    "        step += 1\n",
    "        if n_examples >= max_examples:\n",
    "            break"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train ppl: 34538.93, valid ppl: 25952.82\n",
      "10000 steps; 10240000 examples so far; train ppl: 529.76, valid ppl: 337.01\n",
      "20000 steps; 20480000 examples so far; train ppl: 307.70, valid ppl: 280.29\n",
      "30000 steps; 30720000 examples so far; train ppl: 272.73, valid ppl: 262.06\n",
      "40000 steps; 40960000 examples so far; train ppl: 256.16, valid ppl: 252.97\n",
      "50000 steps; 51200000 examples so far; train ppl: 249.18, valid ppl: 245.87\n",
      "60000 steps; 61440000 examples so far; train ppl: 316.18, valid ppl: 248.65\n",
      "70000 steps; 71680000 examples so far; train ppl: 218.31, valid ppl: 203.66\n",
      "80000 steps; 81920000 examples so far; train ppl: 190.76, valid ppl: 184.61\n",
      "90000 steps; 92160000 examples so far; train ppl: 171.85, valid ppl: 173.23\n",
      "100000 steps; 102400000 examples so far; train ppl: 163.58, valid ppl: 164.38\n",
      "110000 steps; 112640000 examples so far; train ppl: 156.44, valid ppl: 160.74\n",
      "120000 steps; 122880000 examples so far; train ppl: 151.53, valid ppl: 154.67\n",
      "130000 steps; 133120000 examples so far; train ppl: 146.30, valid ppl: 150.58\n",
      "140000 steps; 143360000 examples so far; train ppl: 141.16, valid ppl: 147.77\n",
      "150000 steps; 153600000 examples so far; train ppl: 137.37, valid ppl: 145.29\n",
      "160000 steps; 163840000 examples so far; train ppl: 137.05, valid ppl: 142.67\n",
      "170000 steps; 174080000 examples so far; train ppl: 131.28, valid ppl: 140.31\n",
      "180000 steps; 184320000 examples so far; train ppl: 130.43, valid ppl: 138.37\n",
      "190000 steps; 194560000 examples so far; train ppl: 129.52, valid ppl: 136.83\n",
      "200000 steps; 204800000 examples so far; train ppl: 124.95, valid ppl: 137.03\n",
      "210000 steps; 215040000 examples so far; train ppl: 125.51, valid ppl: 133.40\n",
      "220000 steps; 225280000 examples so far; train ppl: 122.95, valid ppl: 132.37\n",
      "230000 steps; 235520000 examples so far; train ppl: 121.44, valid ppl: 131.42\n",
      "240000 steps; 245760000 examples so far; train ppl: 122.38, valid ppl: 130.11\n",
      "250000 steps; 256000000 examples so far; train ppl: 118.29, valid ppl: 130.15\n",
      "260000 steps; 266240000 examples so far; train ppl: 118.07, valid ppl: 130.01\n",
      "270000 steps; 276480000 examples so far; train ppl: 119.97, valid ppl: 128.35\n",
      "280000 steps; 286720000 examples so far; train ppl: 114.47, valid ppl: 128.46\n",
      "290000 steps; 296960000 examples so far; train ppl: 115.73, valid ppl: 127.03\n",
      "300000 steps; 307200000 examples so far; train ppl: 114.91, valid ppl: 127.35\n",
      "310000 steps; 317440000 examples so far; train ppl: 112.71, valid ppl: 126.12\n",
      "320000 steps; 327680000 examples so far; train ppl: 113.98, valid ppl: 125.07\n",
      "330000 steps; 337920000 examples so far; train ppl: 116.06, valid ppl: 131.81\n",
      "340000 steps; 348160000 examples so far; train ppl: 117.02, valid ppl: 124.72\n",
      "350000 steps; 358400000 examples so far; train ppl: 112.51, valid ppl: 125.02\n",
      "360000 steps; 368640000 examples so far; train ppl: 109.25, valid ppl: 123.38\n",
      "370000 steps; 378880000 examples so far; train ppl: 110.86, valid ppl: 124.70\n",
      "380000 steps; 389120000 examples so far; train ppl: 110.47, valid ppl: 123.22\n",
      "390000 steps; 399360000 examples so far; train ppl: 108.34, valid ppl: 123.64\n",
      "400000 steps; 409600000 examples so far; train ppl: 110.10, valid ppl: 121.81\n",
      "410000 steps; 419840000 examples so far; train ppl: 107.48, valid ppl: 122.12\n",
      "420000 steps; 430080000 examples so far; train ppl: 107.43, valid ppl: 121.67\n",
      "430000 steps; 440320000 examples so far; train ppl: 108.64, valid ppl: 121.40\n",
      "440000 steps; 450560000 examples so far; train ppl: 107.29, valid ppl: 122.40\n",
      "450000 steps; 460800000 examples so far; train ppl: 106.86, valid ppl: 121.57\n",
      "460000 steps; 471040000 examples so far; train ppl: 106.97, valid ppl: 120.22\n",
      "470000 steps; 481280000 examples so far; train ppl: 104.05, valid ppl: 121.30\n",
      "480000 steps; 491520000 examples so far; train ppl: 106.24, valid ppl: 121.90\n",
      "490000 steps; 501760000 examples so far; train ppl: 108.32, valid ppl: 121.96\n",
      "500000 steps; 512000000 examples so far; train ppl: 106.64, valid ppl: 121.14\n",
      "510000 steps; 522240000 examples so far; train ppl: 107.46, valid ppl: 121.06\n",
      "520000 steps; 532480000 examples so far; train ppl: 105.19, valid ppl: 120.42\n",
      "530000 steps; 542720000 examples so far; train ppl: 106.00, valid ppl: 120.44\n",
      "540000 steps; 552960000 examples so far; train ppl: 106.66, valid ppl: 120.12\n",
      "550000 steps; 563200000 examples so far; train ppl: 103.67, valid ppl: 120.02\n",
      "560000 steps; 573440000 examples so far; train ppl: 105.32, valid ppl: 120.86\n",
      "570000 steps; 583680000 examples so far; train ppl: 104.66, valid ppl: 120.75\n",
      "580000 steps; 593920000 examples so far; train ppl: 103.16, valid ppl: 119.72\n",
      "590000 steps; 604160000 examples so far; train ppl: 104.60, valid ppl: 120.15\n",
      "600000 steps; 614400000 examples so far; train ppl: 102.99, valid ppl: 119.29\n",
      "610000 steps; 624640000 examples so far; train ppl: 102.99, valid ppl: 118.62\n",
      "620000 steps; 634880000 examples so far; train ppl: 104.21, valid ppl: 117.58\n",
      "630000 steps; 645120000 examples so far; train ppl: 101.40, valid ppl: 117.88\n",
      "640000 steps; 655360000 examples so far; train ppl: 102.82, valid ppl: 118.29\n",
      "650000 steps; 665600000 examples so far; train ppl: 103.02, valid ppl: 118.26\n",
      "660000 steps; 675840000 examples so far; train ppl: 100.80, valid ppl: 118.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 39>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n_examples \u001B[38;5;241m<\u001B[39m max_examples:\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28minput\u001B[39m, target \u001B[38;5;129;01min\u001B[39;00m DataManager(train_loader, device\u001B[38;5;241m=\u001B[39mdevice, buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, data_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 41\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m         train_losses\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m eval_every_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mtrain_step\u001B[0;34m(input, target)\u001B[0m\n\u001B[1;32m     21\u001B[0m logits \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mcross_entropy(logits, target\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[0;32m---> 23\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliação final no dataset de teste\n",
    "\n",
    "\n",
    "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
   ],
   "metadata": {
    "id": "VgdNymJdNPXP"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxN5YytzZ7Tn"
   },
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in test_loader\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 109.97276644260708\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teste seu modelo com uma sentença\n",
    "\n",
    "Escolha uma sentença gerada pelo modelo que ache interessante."
   ],
   "metadata": {
    "id": "BHvEs8mPszy_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = 'Eu gosto de comer pizza pois me faz'\n",
    "max_output_tokens = 20\n",
    "model.eval()\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
    "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "    print(prompt)"
   ],
   "metadata": {
    "id": "-CFElf4tsytW"
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu gosto de comer pizza pois me faz muito\n",
      "Eu gosto de comer pizza pois me faz muito bem\n",
      "Eu gosto de comer pizza pois me faz muito bem,\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo,\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom tempo\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom tempo,\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom tempo, mas\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom tempo, mas não\n",
      "Eu gosto de comer pizza pois me faz muito bem, mas não é um bom tempo, mas não é um bom tempo, mas não é\n"
     ]
    }
   ]
  }
 ]
}