{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ex08_patrick_ferreira",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex08/patrick_ferreira/ex08_patrick_ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nome = \"Patrick de Carvalho Tavares Rezende Ferreira\"\n",
    "print(f'Meu nome é {nome}')"
   ],
   "metadata": {
    "id": "jOdQB41_4ZxG"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é Patrick de Carvalho Tavares Rezende Ferreira\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IbuChoAPMEn"
   },
   "source": [
    "#  Exercício: Modelo de Linguagem com auto-atenção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DBb0-Klwf2"
   },
   "source": [
    "Este exercício é similar ao da Aula 7, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
    "\n",
    "Na camada de auto-atenção, não se esqueça de implementar:\n",
    "- Embeddings de posição\n",
    "- Projeções lineares (WQ, WK, WV, WO)\n",
    "- Conexões residuais\n",
    "- Camada de feed forward (2-layer MLP)\n",
    "\n",
    "\n",
    "\n",
    "O dataset usado neste exercício (BrWaC) possui um tamanho razoável e você vai precisar rodar seus experimentos com GPU.\n",
    "\n",
    "Alguns conselhos úteis:\n",
    "- **ATENÇÃO:** o dataset é bem grande. Não dê comando de imprimí-lo.\n",
    "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
    "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
    "!pip install transformers"
   ],
   "metadata": {
    "id": "3twP0YJC4jmJ"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (4.19.2)\r\n",
      "Requirement already satisfied: filelock in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: requests in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/patrickctrf/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnyhJZtTRNMx"
   },
   "source": [
    "## Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qlIOVCajPWcU"
   },
   "source": [
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check which GPU we are using\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "w9f3PfifAwpU"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 25 08:21:20 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   44C    P8    13W / 170W |    247MiB / 12288MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1967      G   /usr/lib/xorg/Xorg                138MiB |\r\n",
      "|    0   N/A  N/A      2100      G   ...ome-remote-desktop-daemon        2MiB |\r\n",
      "|    0   N/A  N/A      2136      G   /usr/bin/gnome-shell               65MiB |\r\n",
      "|    0   N/A  N/A     14610      G   ..._14168.log --shared-files       37MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print('Using {}'.format(device))"
   ],
   "metadata": {
    "id": "whTCe2i7AtoV"
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfxgV2DUk58"
   },
   "source": [
    "## Implementação do MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n_xhKm1EZ3bQ"
   },
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize(text: str, tokenizer):\n",
    "    return tokenizer(text, return_tensors=None, add_special_tokens=False).input_ids\n",
    "\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, texts: List[str], tokenizer, context_size: int):\n",
    "        # Escreva seu código aqui\n",
    "        try:\n",
    "            self.x = np.load(\"x_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "            self.y = np.load(\"y_\" + str(len(texts)) + \".npy\", mmap_mode=\"r\", allow_pickle=True)\n",
    "\n",
    "            print(\"Carregando dataset preprocessado\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Excecao: \", e)\n",
    "            print(\"Montando dataset\")\n",
    "\n",
    "            self.x = list()\n",
    "            self.y = list()\n",
    "\n",
    "            for text in tqdm(texts):\n",
    "                tokens_key = tokenize(text, tokenizer)\n",
    "                for i in range(len(tokens_key)-context_size):\n",
    "                    self.x.append(tokens_key[i:i+context_size])\n",
    "                    self.y.append(tokens_key[i+context_size])\n",
    "\n",
    "            self.x = np.array(self.x)\n",
    "            self.y = np.array(self.y)\n",
    "\n",
    "            np.save(\"x_\" + str(len(texts)) + \".npy\", self.x)\n",
    "            np.save(\"y_\" + str(len(texts)) + \".npy\", self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Escreva seu código aqui\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Escreva seu código aqui\n",
    "        return torch.tensor(self.x[idx]).long(), torch.tensor(self.y[idx]).long()"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando se a implementação do MyDataset está correta"
   ],
   "metadata": {
    "id": "wew-gFbWeBTq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
    "\n",
    "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, context_size=3)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
    "assert len(dummy_dataset) == 5\n",
    "print('passou no assert de tamanho do dataset')\n",
    "\n",
    "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
    "\n",
    "correct_first_batch_input = torch.LongTensor(\n",
    "    [[3396, 10303, 125],\n",
    "     [1660, 5971, 785],\n",
    "     [5971, 785, 125],\n",
    "     [785, 125, 1847],\n",
    "     [125, 1847, 13779]])\n",
    "\n",
    "correct_first_batch_target = torch.LongTensor([13239, 125, 1847, 13779, 15616])\n",
    "\n",
    "assert torch.equal(first_batch_input, correct_first_batch_input)\n",
    "print('Passou no assert de input')\n",
    "assert torch.equal(first_batch_target, correct_first_batch_target)\n",
    "print('Passou no assert de target')"
   ],
   "metadata": {
    "id": "8r7jBFFUeApe"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "passou no assert de tamanho do dataset\n",
      "Passou no assert de input\n",
      "Passou no assert de target\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LfrHHouleJ0"
   },
   "source": [
    "# Carregamento do dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2vFWjsSkmop"
   },
   "source": [
    "Iremos usar uma pequena amostra do dataset [BrWaC](https://www.inf.ufrgs.br/pln/wiki/index.php?title=BrWaC) para treinar e avaliar nosso modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -nc https://storage.googleapis.com/unicamp-dl/ia025a_2022s1/aula7/sample_brwac.txt"
   ],
   "metadata": {
    "id": "vGlN1WqrXPA6"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘sample_brwac.txt’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "context_size = 9\n",
    "\n",
    "valid_examples = 101\n",
    "test_examples = 100\n",
    "texts = open('sample_brwac.txt').readlines()\n",
    "\n",
    "# print('Truncating for debugging purposes.')\n",
    "# texts = texts[:500]\n",
    "\n",
    "training_texts = texts[:-(valid_examples + test_examples)]\n",
    "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
    "test_texts = texts[-test_examples:]\n",
    "\n",
    "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, context_size=context_size)\n",
    "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, context_size=context_size)"
   ],
   "metadata": {
    "id": "gxa_4gmiA-wE"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n",
      "Carregando dataset preprocessado\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'training examples: {len(training_dataset)}')\n",
    "print(f'valid examples: {len(valid_dataset)}')\n",
    "print(f'test examples: {len(test_dataset)}')"
   ],
   "metadata": {
    "id": "KCSGJ5m7py4c"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 27675765\n",
      "valid examples: 82250\n",
      "test examples: 166726\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGaAjYDfWdd1"
   },
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Implements the Self-attention, decoder-only.\"\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the input vocabulary.\n",
    "            context_size (int): Size of the sequence to consider as context for prediction.\n",
    "            embedding_dim (int): Dimension of the embedding layer for each word in the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # tokens (words indexes) embedding and positional embedding\n",
    "        self.c_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.p_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_0 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # hidden_size for output MLP\n",
    "        hidden_size = embedding_dim\n",
    "\n",
    "        # output MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "        self.linear_output = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # cast to probabilities\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs is a LongTensor of shape (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            logits of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Escreva seu código aqui.\n",
    "\n",
    "        input_embeddings = self.c_embedding(inputs)\n",
    "\n",
    "        positional_indexes = torch.arange(self.context_size).view(1, -1).repeat(inputs.shape[0], 1).to(device)\n",
    "        positional_embeddings = self.p_embedding(positional_indexes)\n",
    "\n",
    "        x_embeddings = positional_embeddings + input_embeddings\n",
    "\n",
    "        k = self.w_k(x_embeddings)\n",
    "        v = self.w_v(x_embeddings)\n",
    "        q = self.w_q(x_embeddings)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(1, 2))\n",
    "        probabilities = self.softmax(scores)\n",
    "\n",
    "        e = self.w_0(self.norm1(x_embeddings + torch.matmul(probabilities, v)))\n",
    "\n",
    "        # We only take the last word to make prediction\n",
    "        logits = self.mlp(e[:, -1, :].flatten(start_dim=1))\n",
    "\n",
    "        return self.linear_output(self.norm2(logits + e[:, -1, :].flatten(start_dim=1)))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teste o modelo com um exemplo"
   ],
   "metadata": {
    "id": "Rm6_PTH2i98e"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RwnxfZlrZoT_"
   },
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=64,\n",
    ").to(device)\n",
    "\n",
    "sample_train, _ = next(iter(DataLoader(training_dataset)))\n",
    "sample_train_gpu = sample_train.to(device)\n",
    "model(sample_train_gpu).shape"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 29794])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I3Vh6B-VkA01"
   },
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of model parameters: {num_params}')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 5775458\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assert da Perplexidade\n"
   ],
   "metadata": {
    "id": "8nhbUVsYnVAp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "def perplexity(logits, target):\n",
    "    \"\"\"\n",
    "    Computes the perplexity.\n",
    "\n",
    "    Args:\n",
    "        logits: a FloatTensor of shape (batch_size, vocab_size)\n",
    "        target: a LongTensor of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        A float corresponding to the perplexity\n",
    "    \"\"\"\n",
    "    loss = nn.functional.cross_entropy(logits, target, reduction='mean')\n",
    "    return torch.exp(loss)\n",
    "\n",
    "\n",
    "# n_examples = 1000\n",
    "#\n",
    "# sample_train, target_token_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
    "# sample_train_gpu = sample_train.to(device)\n",
    "# target_token_ids = target_token_ids.to(device)\n",
    "# logits = model(sample_train_gpu)\n",
    "#\n",
    "# my_perplexity = perplexity(logits=logits, target=target_token_ids)\n",
    "#\n",
    "# print(f'my perplexity:              {int(my_perplexity)}')\n",
    "# print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
    "#\n",
    "# assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
    "# print('Passou o no assert da perplexidade')"
   ],
   "metadata": {
    "id": "gbMP8VAUncfX"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Laço de Treinamento e Validação"
   ],
   "metadata": {
    "id": "KiJtrsqPnE_l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class DataManager(Thread):\n",
    "    def __init__(self, data_loader, buffer_size=3, device=torch.device(\"cpu\"), data_type=torch.float32):\n",
    "        \"\"\"\n",
    "This manager intends to load a PyTorch dataloader like from disk into memory,\n",
    "reducing the acess time. It does not easily overflow memory, because we set a\n",
    "buffer size limiting how many samples will be loaded at once. Everytime a sample\n",
    "is consumed by the calling thread, another one is replaced in the\n",
    "buffer (unless we reach the end of dataloader).\n",
    "\n",
    "A manger may be called exactly like a dataloader, an it's based in an internal\n",
    "thread that loads samples into memory in parallel. This is specially useful\n",
    "when you are training in GPU and processor is almost idle.\n",
    "\n",
    "        :param data_loader: Base dataloader to load in parallel.\n",
    "        :param buffer_size: How many samples to keep loaded (caution to not overflow RAM). Default: 3.\n",
    "        :param device: Torch device to put samples in, like torch.device(\"cpu\") (default). It saves time by transfering in parallel.\n",
    "        :param data_type: Automatically casts tensor type. Default: torch.float32.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.buffer_queue = Queue(maxsize=buffer_size)\n",
    "        self.data_loader = data_loader\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.data_type = data_type\n",
    "\n",
    "        self.dataloader_finished = False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "Runs the internal thread that iterates over the dataloader until fulfilling the\n",
    "buffer or the end of samples.\n",
    "        \"\"\"\n",
    "        for i, (x, y) in enumerate(self.data_loader):\n",
    "            # Important to set before put in queue to avoid race condition\n",
    "            # would happen if trying to get() in next() method before setting this flag\n",
    "            if i >= len(self) - 1:\n",
    "                self.dataloader_finished = True\n",
    "\n",
    "            self.buffer_queue.put([x.to(self.data_type).to(self.device),\n",
    "                                   y.to(self.data_type).to(self.device)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "Returns an iterable of itself.\n",
    "\n",
    "        :return: Iterable around this class.\n",
    "        \"\"\"\n",
    "        self.start()\n",
    "        self.dataloader_finished = False\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "Intended to be used as iterator.\n",
    "\n",
    "        :return: Next iteration element.\n",
    "        \"\"\"\n",
    "        if self.dataloader_finished is True and self.buffer_queue.empty():\n",
    "            raise StopIteration()\n",
    "\n",
    "        return self.buffer_queue.get()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zIMSaY-UUGUE",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "max_examples = 100_000_000\n",
    "eval_every_steps = 10000\n",
    "lr = 3e-4\n",
    "\n",
    "model = LanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    context_size=context_size,\n",
    "    embedding_dim=128,\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, drop_last=True, num_workers=2)\n",
    "validation_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train_step(input, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    logits = model(input.to(device))\n",
    "    loss = nn.functional.cross_entropy(logits, target.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def validation_step(input, target):\n",
    "    model.eval()\n",
    "    logits = model(input)\n",
    "    loss = nn.functional.cross_entropy(logits, target)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "n_examples = 0\n",
    "step = 0\n",
    "while n_examples < max_examples:\n",
    "    for input, target in DataManager(train_loader, device=device, buffer_size=4, data_type=None):\n",
    "        loss = train_step(input.to(device), target.to(device))\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if step % eval_every_steps == 0:\n",
    "            train_ppl = np.exp(np.average(train_losses))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_ppl = np.exp(np.average([\n",
    "                    validation_step(input.to(device), target.to(device))\n",
    "                    for input, target in DataManager(validation_loader, device=device, buffer_size=4, data_type=None)]))\n",
    "\n",
    "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
    "            train_losses = []\n",
    "\n",
    "        n_examples += len(input)  # Increment of batch size\n",
    "        step += 1\n",
    "        if n_examples >= max_examples:\n",
    "            break"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 steps; 0 examples so far; train ppl: 33106.88, valid ppl: 32088.06\n",
      "10000 steps; 10240000 examples so far; train ppl: 592.85, valid ppl: 356.74\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avaliação final no dataset de teste\n",
    "\n",
    "\n",
    "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
   ],
   "metadata": {
    "id": "VgdNymJdNPXP"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxN5YytzZ7Tn",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_ppl = np.exp(np.average([\n",
    "        validation_step(input.to(device), target.to(device))\n",
    "        for input, target in test_loader\n",
    "    ]))\n",
    "\n",
    "print(f'test perplexity: {test_ppl}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Teste seu modelo com uma sentença\n",
    "\n",
    "Escolha uma sentença gerada pelo modelo que ache interessante."
   ],
   "metadata": {
    "id": "BHvEs8mPszy_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = 'Eu gosto de comer pizza pois me faz'\n",
    "max_output_tokens = 20\n",
    "model.eval()\n",
    "\n",
    "for _ in range(max_output_tokens):\n",
    "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
    "    input_ids_truncated = input_ids[-context_size:]  # Usamos apenas os últimos <context_size> tokens como entrada para o modelo.\n",
    "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
    "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
    "    # Isso se chama decodificação gulosa (greedy decoding).\n",
    "    predicted_id = torch.argmax(logits).item()\n",
    "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
    "    prompt = tokenizer.decode(input_ids)\n",
    "    print(prompt)"
   ],
   "metadata": {
    "id": "-CFElf4tsytW",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}