{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beatrice Silva de Almeida_Exercícios - 20210718",
      "provenance": [],
      "collapsed_sections": [
        "kxS5h1V8nDn6",
        "oJHDaOz_tK38",
        "XRfaKfXwRXn_",
        "JtgSAAKjUfcO",
        "GF_P_GARU62m",
        "cbXIXsDIUmtp",
        "QKnLAyL7zgpa",
        "lqMroZay2ubi",
        "LsOThnt8fDJV",
        "O_Sx1QXZxJ3u",
        "JBXxBmWGK3IU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/Beatrice_Silva_de_Almeida/Beatrice_Silva_de_Almeida_Exerc%C3%ADcios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe"
      },
      "source": [
        "print('Meu nome é: Beatrice Silva de Almeida')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def top_k(L, k):\n",
        "  #Utilizando Counter para encontrar as frequencias mais comuns\n",
        "    f_kmax = dict(Counter(L).most_common(n=k))\n",
        "    return f_kmax\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3f86ec-67e7-4219-92a3-660f3d50c546"
      },
      "source": [
        "from collections import Counter\n",
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L,k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14c0b0b-51c0-4633-b970-a597ba173d93"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 570 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "from re import findall\n",
        "\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "    #Primeiro separar a string consirando pontuações\n",
        "    palavra = findall(r'\\w+|\\S', text)\n",
        "    #identificando quando for unknown\n",
        "    unk = vocabulary['unknown']\n",
        "    #correlacionando  identificador com o dicionário\n",
        "    id = [vocabulary.get(p.lower(),unk) for p in palavra]\n",
        "    return id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e7df58-b3c3-406f-dd4c-a8a71dc8bc54"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9945bc-29cf-4877-d57d-c418591b487d"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.81 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "from random import randint\n",
        "def sample(path: str, k: int):\n",
        "    smpls = [None] * k\n",
        "    for i, l in enumerate(open(path,'r')):\n",
        "      if i < k:\n",
        "        smpls[i] = l[:-1]\n",
        "      else:\n",
        "        j = randint(0,i)\n",
        "        if j < k:\n",
        "          smpls[j] = l[:-1]\n",
        "    return smpls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8295f69d-e77a-46f0-c825-85cd60aac90d"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 0', 'line 37', 'line 2', 'line 38', 'line 64', 'line 98', 'line 68', 'line 65', 'line 94', 'line 34']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: (n-1) *m *p \n",
        "- número de multiplicações:m *n *p "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c5f015-1acd-42a3-d5bb-2aebb9561ffc"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089320da-661e-43e7-aac9-40218ce1ad72"
      },
      "source": [
        "# Calculando a média dos valores de cada linha\n",
        "med = A.mean(axis = 1)\n",
        "print (med)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.5  8.5 14.5 20.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f196b44-501f-4915-e9b2-17c7b6591c23"
      },
      "source": [
        "# Extraindo max e min da matriz A\n",
        "A_max = A.max()\n",
        "A_min = A.min()\n",
        "#Calculando a normalização (Matriz C)\n",
        "C = (A - A_min) /(A_max - A_min)\n",
        "print (C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93520b1b-1a81-4320-a137-1f14975613b3"
      },
      "source": [
        "# Extrair max e min de cada coluna separadamente ao invés da matriz inteira\n",
        "c_max = A.max(axis=0)\n",
        "c_min = A.min(axis=0)\n",
        "#Calculando a normalização das colunas\n",
        "C = (A - c_min) / (c_max - c_min)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67404d7d-f3e9-480e-8c24-effa77f51cc9"
      },
      "source": [
        "# Extrair o max e min de cada linha separadamente\n",
        "l_max = A.max(axis=1).reshape(-1,1)\n",
        "l_min = A.min(axis=1).reshape(-1,1)\n",
        "#Calculando a normalização das linhas\n",
        "C = (A - l_min) / (l_max - l_min)\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Para contornar o problema de estouro da exponencial será subtraído o valor máximo de cada item do array\n",
        "    r_max = np.max(A,axis =1, keepdims=True)\n",
        "    e_A = np.exp(A-r_max)\n",
        "    soma = np.sum(e_A, axis=1, keepdims=True)\n",
        "    soft = e_A/soma\n",
        "    return soft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8414dffa-39e6-4735-90f7-7511c34be348"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfe5906-c1ae-4044-8638-cc03b3273e9f"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386c3085-88b5-4fbf-a623-2f847bc3afec"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 292 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdd1950-c734-4a0e-afca-619687a51229"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "import numpy as np\n",
        "def one_hot(y, n_classes):\n",
        "    \n",
        "    input = np.array([y]).reshape(-1)\n",
        "    one_hot = np.eye(n_classes)[input]\n",
        "    return one_hot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccf7d15-fa0c-4cd0-d50c-10c334c7f24a"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 8 6 2 0 0 4 0 2 1]\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c0d5b3-850b-4e7a-d6ea-48dfda8744ad"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[611 212 429 ... 772 972  46]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555f9135-2c27-4287-a5ca-219de81091e1"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 272 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "import numpy as np\n",
        "class Normalizer():\n",
        "  # Calculando a média e o desvio padrão \n",
        "  def __init__(self, array_b):\n",
        "    array_n = np.array(array_b)\n",
        "    self.mean = array_n.mean()\n",
        "    self.std = array_n.std()\n",
        "\n",
        "  def __call__(self, norm_a):\n",
        "    norm_array = np.array(norm_a)\n",
        "    normalizador = (norm_array - norm_array.mean())/ norm_array.std()\n",
        "    return (normalizador * self.std) + self.mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a330088-fb63-43a4-966e-1e65da8d22c6"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24c87ac4-737a-414b-89d7-e1bda351af40"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.10.0+cu111'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a64da94-fc3f-4f80-f2a7-f37a20236b27"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040adacc-eae2-464d-c08a-bac570dc7bf2"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0acbadc3-10a9-4347-8c68-6b6cdc60fe2d"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c0a3a5-7a5f-42c9-92af-68fd7c8b3517"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ed9834-b4e3-44f3-8f76-364347ea8765"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f38a35e-2a90-4f43-df74-b3d804aefe81"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31460f0d-acaa-4337-8ca3-3b13c5e46bff"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    return (((x*w)-y)**2).sum()\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "def J_fdiff(w,x,y):\n",
        "  dw = 1e-5\n",
        "  #divindo a equação por parcelas\n",
        "  J_soma = J_func((w + dw), x, y)\n",
        "  J_sub = J_func((w - dw), x, y)\n",
        "  return (J_soma - J_sub) / (2*dw)\n",
        "\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "grad = J_fdiff(w, x, y)\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0380)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd8cbaea-b357-4572-9bb2-de5059eaa1fd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "loss = []\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J.item())\n",
        "    print('J=', J)\n",
        "    grad = J_fdiff(w, x, y)\n",
        "    print('grad =',grad)\n",
        "    w -= learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(loss)\n",
        "plt.title(\"Loss vs iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteractions\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0380)\n",
            "w = tensor([1.2804])\n",
            "i = 1\n",
            "J= tensor(7.2499)\n",
            "grad = tensor(-20.1702)\n",
            "w = tensor([1.4821])\n",
            "i = 2\n",
            "J= tensor(3.7553)\n",
            "grad = tensor(-14.5197)\n",
            "w = tensor([1.6273])\n",
            "i = 3\n",
            "J= tensor(1.9449)\n",
            "grad = tensor(-10.4487)\n",
            "w = tensor([1.7318])\n",
            "i = 4\n",
            "J= tensor(1.0073)\n",
            "grad = tensor(-7.5221)\n",
            "w = tensor([1.8070])\n",
            "i = 5\n",
            "J= tensor(0.5216)\n",
            "grad = tensor(-5.4121)\n",
            "w = tensor([1.8611])\n",
            "i = 6\n",
            "J= tensor(0.2701)\n",
            "grad = tensor(-3.8937)\n",
            "w = tensor([1.9000])\n",
            "i = 7\n",
            "J= tensor(0.1399)\n",
            "grad = tensor(-2.8022)\n",
            "w = tensor([1.9281])\n",
            "i = 8\n",
            "J= tensor(0.0724)\n",
            "grad = tensor(-2.0169)\n",
            "w = tensor([1.9482])\n",
            "i = 9\n",
            "J= tensor(0.0375)\n",
            "grad = tensor(-1.4514)\n",
            "w = tensor([1.9627])\n",
            "i = 10\n",
            "J= tensor(0.0194)\n",
            "grad = tensor(-1.0444)\n",
            "w = tensor([1.9732])\n",
            "i = 11\n",
            "J= tensor(0.0101)\n",
            "grad = tensor(-0.7516)\n",
            "w = tensor([1.9807])\n",
            "i = 12\n",
            "J= tensor(0.0052)\n",
            "grad = tensor(-0.5409)\n",
            "w = tensor([1.9861])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3892)\n",
            "w = tensor([1.9900])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2801)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2016)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1451)\n",
            "w = tensor([1.9963])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1044)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0751)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.2033e-05)\n",
            "grad = tensor(-0.0541)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccda6978-1048-4d3a-8886-784d1233bc2c"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "loss =[]\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J.item())\n",
        "    print('J=', J)\n",
        "    if w.grad: w.grad.zero_()\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    with torch.no_grad():\n",
        "      w-= learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "plt.plot(loss)\n",
        "plt.title(\"Loss vs iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteractions\")\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], requires_grad=True)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], requires_grad=True)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], requires_grad=True)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], requires_grad=True)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], requires_grad=True)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], requires_grad=True)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], requires_grad=True)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], requires_grad=True)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], requires_grad=True)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], requires_grad=True)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], requires_grad=True)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], requires_grad=True)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], requires_grad=True)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], requires_grad=True)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], requires_grad=True)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], requires_grad=True)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], requires_grad=True)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], requires_grad=True)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], requires_grad=True)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: Observando o calculo do gradiente por diferenças finitas percebemos que Δw não pode ser zero. Geralmente Δw é um número bem pequeno tendendo a zero, porém não deve ser tão pequeno ao ponto de a máquina representá-lo como zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) Sabendo que o custo é dado por $J=(x_i w - y_i)^2$, tendo em vista que é válido para o parâmetro w, é necessário executar a função N vezes para percorrer todo o vetor de parâmetros. Sabendo que a regra das diferenças finitas é a derivada da função de custo e possui ordem $2O(N) -> O(N)$, se executada N vezes temos $N*N -> O(N^2)$\n",
        "\n",
        "b) Para calcular J o grafo é percorrido uma vez pelo forward propagation e depois é percorrido mais uma vez pelo backpropagation, resultando no custo $O(2*N) -> O(N)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: Assumindo que a probabilidade na primeira iteração seja uniforme, $1/k$, a formula da entropia pode ser descrita como:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log (1/k), $$\n",
        "Levando em consideraçao que $y=1$ para j representando a classe do exemplo e sabendo que as classes possuem probabilidades equivalentes, a Loss pode ser definida como: $$L = log (k) $$\n",
        "Considerando o Batch, temos:\n",
        "$$L = (1/B)\\sum_{i=0}^{B-1} \\log k = \\log k $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}