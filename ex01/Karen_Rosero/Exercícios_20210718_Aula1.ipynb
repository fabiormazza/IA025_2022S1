{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718_Aula1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/Karen_Rosero/Exerc%C3%ADcios_20210718_Aula1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6620f628-ad63-4e85-bb38-80f69f176bda"
      },
      "source": [
        "print('Meu nome é: Karen Rosero')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Karen Rosero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "hd0S6jzBVyGB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "    # Escreva aqui o código\n",
        "    freq = Counter(L)\n",
        "    resultado = dict(freq.most_common(k))\n",
        "    return resultado"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251c614c-2499-469a-cb9d-8508be6759cc"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a82d0c-7b8d-447c-94d7-b6bb23b1feb5"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 499 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "V5E3Bqhsjy5B"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ^ Matches the start of the string\n",
        "\n",
        " \\ signals a special sequence\n",
        "\n",
        " [] used to indicate a set of characters\n",
        "\n",
        "A|B creates a regular expression that will match either A or B\n",
        "\n",
        "\\w Matches Unicode word characters\n",
        "\n",
        "\\s Matches Unicode whitespace characters"
      ],
      "metadata": {
        "id": "ehzsKqLhnnr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseado no notebook do usuario marcus_borela\n",
        "def tokens_to_ids(text, vocabulary):\n",
        "   return [vocabulary[palavra] if palavra in vocabulary else vocabulary['unknown'] for palavra in re.findall(r\"\\w+|[^\\s\\w]\",text.lower()) ]"
      ],
      "metadata": {
        "id": "e0F0Y4colo1W"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd63defa-7f13-4cec-8aef-223851722960"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70d9e37-d3f9-480b-8f0a-914db42b039d"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.27 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "RfwDeEutwbfq"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "    with open(path, buffering=1) as f: #1 means line buffered\n",
        "      item = random.sample(f.readlines(), k)\n",
        "      return item"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad8a950-0891-4514-9cae-898361e33d90"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 82\\n', 'line 85\\n', 'line 50\\n', 'line 10\\n', 'line 38\\n', 'line 68\\n', 'line 86\\n', 'line 39\\n', 'line 93\\n', 'line 49\\n']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0919d0-5db7-4575-fe9d-9ce2bb91e2c5"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 95.8 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: $m \\times p \\times (n-1) $\n",
        "\n",
        "- número de multiplicações: $m \\times p \\times n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ce495b-3500-49c2-8cb3-133b82d68c12"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6419a15-e6fa-44df-f914-0c57c96d80ec"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "A_mean = np.mean(A, axis=1)\n",
        "print('Média de cada linha de A:', A_mean)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média de cada linha de A: [ 2.5  8.5 14.5 20.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a88c995-508b-478c-8c36-243c7b10b775"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "Amin = np.min(A)\n",
        "Amax = np.max(A)\n",
        "print('Matriz normalizada-->\\n',(A-Amin)/(Amax-Amin))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz normalizada-->\n",
            " [[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b3b877-2f03-4df0-9640-8bb54857cca5"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "ACmin = np.min(A, axis=0)\n",
        "ACmax = np.max(A, axis=0)\n",
        "print('Matriz normalizada por colunas-->\\n',(A-ACmin)/(ACmax-ACmin))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz normalizada por colunas-->\n",
            " [[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7907093a-7424-434b-83ec-738b59be653f"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "ALmin = np.min(A, axis=1).reshape((A.shape[0],1))\n",
        "ALmax = np.max(A, axis=1).reshape((A.shape[0],1))\n",
        "print('Matriz normalizada por linhas-->\\n',(A-ALmin)/(ALmax-ALmin))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz normalizada por linhas-->\n",
            " [[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Escreva sua solução aqui.\n",
        "    A_max = np.max(A, axis=1, keepdims=True) \n",
        "    A_exp = np.exp(A - A_max)\n",
        "    sum = np.sum(A_exp, axis=1, keepdims=True) \n",
        "    A_soft = A_exp / sum \n",
        "    return A_soft    "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48326b2-4afb-4b31-98f1-f15b2977af39"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7648a45-c448-4cd5-ab85-6a7351eb3c79"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aad5986-7ee7-4b7f-d104-544ff8ce0d34"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 324 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f60338-0cb9-4772-f16e-b26c61e6f6a7"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "    # Escreva seu código aqui.\n",
        "    matrix_total = np.zeros((len(y), n_classes), dtype = int)    \n",
        "    for linha in range(len(y)):\n",
        "      matrix_total[linha, y[linha]]=1  \n",
        "    return matrix_total"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96cd7c0-6be0-4830-c3d7-c9ef14cc2129"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 7 8 7 8 6 8 8 2 5]\n",
            "[[0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eb1e23-88d0-4192-f97d-e05ad0dc2b3a"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f994020-bdad-4675-fd7a-4e1f925e5644"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 5.82 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 214 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "def Normalizer(array_b):\n",
        "  b_std = np.std(array_b)\n",
        "  b_mean = np.mean(array_b)\n",
        "  def normalize(array_a):\n",
        "    a_std = np.std(array_a)\n",
        "    a_mean = np.mean(array_a)\n",
        "    array_a_norm_b = b_mean + b_std*((array_a-a_mean)/a_std)\n",
        "    return array_a_norm_b\n",
        "  return normalize"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5213d0dc-3278-4028-9d19-2b43554bbbe7"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6abe5c91-0f20-44bf-bb32-21c3d3e2e525"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c96aae2-2f55-4201-feaa-eb4baea3b8b9"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de7fed0-bad9-4210-8346-23a6c44cd9b1"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe8be4b-a2e2-4509-a071-9f274aceee78"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41de8655-ee5d-427c-a5a1-4461cb3b6bb6"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7444c64a-0437-46d4-cbe8-452b663a44f9"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7448d8dd-7fb2-466b-c1ca-39d6e327a14f"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183f0247-125a-4fdb-8038-f5f120244cb4"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    #result =((x*w -y)**2).sum()\n",
        "    #Melhorado baseado no notebook do usuario fabiormazza\n",
        "    result = torch.sum((x*w -y)**2)\n",
        "    return result\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "d_w = 0.05\n",
        "grad = ((J_func(w+d_w, x, y))-(J_func(w-d_w, x, y)))/(2*d_w)\n",
        "print('grad=', grad)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GrFiOHI9-7XY"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62777104-e677-4a46-e328-7d57145aea8b"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "J_values = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    grad = ((J_func(w+d_w, x, y))-(J_func(w-d_w, x, y)))/(2*d_w)\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate * grad\n",
        "    print('w =', w)\n",
        "    J_values.append(J)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "plt.plot(range(iteracoes), J_values)\n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss por iteração')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0000)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2576)\n",
            "grad = tensor(-20.1600)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor(-14.5152)\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9504)\n",
            "grad = tensor(-10.4509)\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0111)\n",
            "grad = tensor(-7.5247)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5241)\n",
            "grad = tensor(-5.4178)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2717)\n",
            "grad = tensor(-3.9008)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1409)\n",
            "grad = tensor(-2.8086)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0222)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0379)\n",
            "grad = tensor(-1.4560)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0483)\n",
            "w = tensor([1.9730])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7548)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5434)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3913)\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2817)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1052)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3059e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss por iteração')"
            ]
          },
          "metadata": {},
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d013858-977d-40dc-f82b-89e3da1ecbfd"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "J_values = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    w.retain_grad()\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    w = w-learning_rate*grad\n",
        "    print('w =', w)\n",
        "    J_values.append(J.detach().numpy())\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "plt.plot(range(iteracoes), J_values)\n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss por iteração')"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss por iteração')"
            ]
          },
          "metadata": {},
          "execution_count": 140
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: Considerando que o gradiente é uma generalização multivariavel da  derivada, devem-se cumprir os requerimentos da ecuação de derivação por diferenças finitas, mesma que calcula o límite com $\\Delta w$ tendendo a zero. Por tal motivo, o valor de $\\Delta w$ deve ser muito próximo de zero. Mas se o valor for excesivamente pequeno, o algoritmo do gradiente pode demorar muito ou não convergir, e o erro pode acrescentar levemente por conta de erros de aproximação. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a)  O custo computacional para calcular cada função de erro $(x_i w - y_i)^2$ é $O(N)$, e a fórmula das diferenças infinitas calcula essa função duas vezes. Então o custo para cada amostra seria $O(2N)$. Mas devemos considerar que esse processo vai ser repetido para os $N$ parâmetros. Finalmente, o custo de uma atualização seria $O(2N^2)$.\n",
        "\n",
        "b)  Segundo https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html, autograd mantem em memória os computos do grafo, para não precisar computá-lo de novo. Então precisariamos calcular uma única vez $(x_i w - y_i)^2$ para todos os parâmetros $N$, o que resulta num custo de $O(N^2)$.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: Supondo que a probabilidade de ocorrência de cada clase é a mesma, podemos definir $p_j = 1/K$. O valor de $y_j$ é 1 porque considera-se a ocorrência da classe. Logo reemplazo os valores na equação:\n",
        "\n",
        "\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j = -(1)\\log(1/K) = \\log(K)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}