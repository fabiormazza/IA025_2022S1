{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA025 - A1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flych3r/IA025_2022S1/blob/main/ex01/matheus_xavier/IA025_A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5a228b-65cc-41bc-981d-5d47b7d64ce0"
      },
      "source": [
        "print('Meu nome é: Matheus Xavier Sampaio (220092)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Matheus Xavier Sampaio (220092)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations"
      ],
      "metadata": {
        "id": "-FMlWoO78ays"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "gJciCmoxcVlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L: list[str], k: int) -> dict[str, int]:\n",
        "    # Escreva aqui o código\n",
        "    count_top = collections.Counter(L)\n",
        "    count_top = dict(sorted(count_top.items(), key=lambda x: x[1], reverse=True)[:k])\n",
        "    return count_top"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46878307-65f6-4005-f093-71d96bca6800"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab06bd8e-ca13-4864-cd93-4236a10c667f"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.09 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text: str, vocabulary: dict[str, int]) -> list[int]:\n",
        "    # escreva o código aqui.\n",
        "    tokens = re.split('(\\W)', text.lower())\n",
        "    tokens = filter(lambda x: x not in (' ', ''), tokens)\n",
        "    tokenized = map(lambda x: vocabulary.get(x, vocabulary['unknown']), tokens)\n",
        "    return list(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a6f475-9a10-4099-a37a-a17ce183f4d8"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7f0e64-0062-4be4-ce02-e2744d4d61f5"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 4.17 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "LLJAuTKyljvy",
        "outputId": "3f396ad2-d5cc-4dca-ad50-7414b3bb984f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.60.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int) -> list[str]:\n",
        "    # Escreva o seu código aqui.\n",
        "    with open(path) as f:\n",
        "        f.seek(0, 2)\n",
        "        file_size = f.tell()\n",
        "        random_line_numbers = sorted(random.sample(range(file_size), k))\n",
        "        random_lines = []\n",
        "        for line in random_line_numbers:\n",
        "            f.seek(line)\n",
        "            f.readline()\n",
        "            random_line = f.readline()\n",
        "            if not random_line:\n",
        "                f.seek(0)\n",
        "                random_line = f.readline()\n",
        "            random_lines.append(random_line)\n",
        "    return random_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1cc0ae-01c0-469e-918f-d0bdf1741bd2"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 1\\n', 'line 4\\n', 'line 5\\n', 'line 44\\n', 'line 49\\n', 'line 60\\n', 'line 62\\n', 'line 72\\n', 'line 75\\n', 'line 90\\n']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000_00\n",
        "n_samples = 100000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab86057-ad1b-45dd-8e17-2dd60e19233d"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.08 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%memit sample(path=filename, k=n_samples)"
      ],
      "metadata": {
        "id": "ZGq9YDvllpwX",
        "outputId": "99923953-f1a9-448c-f0fa-712434df4ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peak memory: 3888.56 MiB, increment: 9.57 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: $m * (n - 1) * p$\n",
        "- número de multiplicações: $m * n * p$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5862b311-38f9-47d1-eb72-cb72e9b7afd4"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70f682d-ea16-4caf-d2c2-0c1c2ea88325"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "A.mean(axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0030d834-ee75-4ecf-d00c-f15952b857de"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C = (A - A.min()) / (A.max() - A.min())\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c26a0b8-2c89-4bea-acf2-bd15697f81c9"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C_cols = (A - A.min(axis=0, keepdims=True)) / (A.max(axis=0, keepdims=True) - A.min(axis=0, keepdims=True))\n",
        "print(C_cols)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d770ea2-cd93-4372-94ac-d224efd6557b"
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C_lines = (A - A.min(axis=1, keepdims=True)) / (A.max(axis=1, keepdims=True) - A.min(axis=1, keepdims=True))\n",
        "print(C_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A: np.array[float]) -> np.array[float]:\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Escreva sua solução aqui.\n",
        "    A = A - A.max(axis=1, keepdims=True)\n",
        "    return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80ca325-2c1b-4749-cf4d-0b5f4c1910af"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c2f66f-ff44-4aa4-aca1-2f15aaff1104"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab20198-6392-4178-afc4-6109a28fa83a"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 656 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1cd630-eee8-45b2-d027-c5876c9fb89c"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y: np.array[int], n_classes: int) -> np.array[np.array[np.ubyte]]:\n",
        "    # Escreva seu código aqui.\n",
        "    return np.eye(n_classes, dtype=np.ubyte)[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb334f3-2205-45d5-853d-376c90672eec"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 6 8 6 5 1 1 4]\n",
            "[[0 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d575a21-9986-4644-9130-4e14d248cd67"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 34.2 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "class Normalizer:\n",
        "    def __init__(self, arr: np.array):\n",
        "        self.mean: float = np.mean(arr)\n",
        "        self.std: float = np.std(arr)\n",
        "    \n",
        "    def __call__(self, arr: np.array) -> np.array:\n",
        "        scaled = (arr - np.mean(arr)) / np.std(arr)\n",
        "        return scaled * self.std + self.mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dc5089-26fe-4be3-d25a-e360494880ab"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "mean_b, std_b = np.mean(array_b), np.std(array_b)\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe621e3f-77d3-45c0-fd36-08511b59c297"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95591b80-a5bf-4f1b-c6d8-b8fd6161470f"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21673ff6-2d98-4703-b5d2-d870a2fd7a84"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919c926a-6483-4423-d125-de1e25085c59"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b41f56-b876-410a-d850-fc8cd1f4f546"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6110ebcb-bf9d-4354-e906-57c98edbecf3"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec594dbb-27dc-4b8a-9ef6-8d9662a3f6fb"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c36035-78d4-4aa0-ccdd-42ca520b04de"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e.pow(2)\n",
        "    return e2.sum()\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "delta_w = 1e-3 * w\n",
        "grad = (J_func(w + delta_w, x, y) - J_func(w - delta_w, x, y)) / (2 * delta_w)\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor([-28.0008])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(w: torch.Tensor, x: torch.Tensor, y: torch.Tensor, delta_w: torch.Tensor) -> torch.Tensor:\n",
        "    return (J_func(w + delta_w, x, y) - J_func(w - delta_w, x, y)) / (2 * delta_w)"
      ],
      "metadata": {
        "id": "dIaDPvE6L4c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FCqJM0qIPMOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cda423e5-cc90-4ec2-89f6-f77ddc21a805"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "losses = np.full(iteracoes, np.inf)\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    grad = gradient(w, x, y, 1e-3 * w)\n",
        "    print('grad =', grad)\n",
        "    w = w - grad * learning_rate\n",
        "    print('w =', w)\n",
        "    losses[i] = J\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "\n",
        "plt.plot(losses, 'o-')\n",
        "plt.xlabel('iteração')\n",
        "plt.ylabel('loss J')\n",
        "plt.xticks(range(20))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor([-28.0008])\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2574)\n",
            "grad = tensor([-20.1593])\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor([-14.5160])\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9503)\n",
            "grad = tensor([-10.4508])\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0110)\n",
            "grad = tensor([-7.5246])\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5241)\n",
            "grad = tensor([-5.4174])\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2717)\n",
            "grad = tensor([-3.9009])\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1408)\n",
            "grad = tensor([-2.8085])\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor([-2.0220])\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0379)\n",
            "grad = tensor([-1.4559])\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor([-1.0482])\n",
            "w = tensor([1.9730])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3059e-05)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d6ffb47-8a4f-454d-c9fe-cd828580fef0"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "losses = np.full(iteracoes, np.inf)\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    J.backward()\n",
        "    print('J=', J)\n",
        "    grad = w.grad\n",
        "    print('grad =', grad)\n",
        "    w = w - grad * learning_rate\n",
        "    w.retain_grad()\n",
        "    print('w =', w)\n",
        "    losses[i] = J\n",
        "\n",
        "\n",
        "# # Plote aqui a loss pela iteração\n",
        "plt.plot(losses, 'o-')\n",
        "plt.xlabel('iteração')\n",
        "plt.ylabel('loss J')\n",
        "plt.xticks(range(20))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: $\\Delta w$ muito pequeno de resultar em erros de truncamento, enquanto $\\Delta w$ muito grande pode levar o $w$ a não convergir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) No método de diferenciais finitas, a função J é executada 3 vezes, e calculamos J para cada $w$, com temos como custo $O(3N*N)$, ou $O(N^2)$\n",
        "\n",
        "b) No método backpropagation, a função J é executada apenas 1 vez, no entanto dependemos do custo da função `backpropagation`. Assumindo o tempo de execução de `backpropagation` como $B$, temos como custo $O(B + N^2)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:  \n",
        "O custo depende de $K$, já que uma predição aleatória acontece como $\\frac{1}{K}$.\n",
        "\n",
        "Como $y$ possui apenas uma classe positiva em cada amostra, temos:\n",
        "$$ \\sum_{j=0}^{K-1} = 1 $$\n",
        "Logo:\n",
        "$$ L = -1 \\log \\frac{1}{K} = \\log K $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}