{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atividade 01 - IA025A-2022S1 - Gustavo da Silva Arantes",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/gustavo_arantes/Atividade_01_IA025A_2022S1_Gustavo_da_Silva_Arantes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6025046f-88a0-4146-fa41-fe186d7835d9"
      },
      "source": [
        "print('Meu nome é: Gustavo da Silva Arantes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Gustavo da Silva Arantes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "def top_k(L, k):\n",
        "    from collections import Counter\n",
        "\n",
        "    # Counter() é capaz de dado uma lista retornar um dicionário com a frequência de cada item\n",
        "    dict_frequencia = dict(Counter(L)) \n",
        "\n",
        "    # ordenando o dicionário\n",
        "    dict_frequencia = {key: value for key, value in sorted(dict_frequencia.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    # selecionando os Top K mais frequentes \n",
        "    dict_frequencia = dict(list(dict_frequencia.items())[:k])\n",
        "\n",
        "    return dict_frequencia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fab947-4fdc-4aab-bf26-7da2385e2d9f"
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1f6d56-9857-46b0-af98-7af99aa6d8e1"
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 547 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "    import re\n",
        "\n",
        "    # garantir que todas as letras sejam minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # expressão regular para dividir as frases e garantindo que a pontuação também será token\n",
        "    text = re.findall(r\"[^,.:;' ]+|[,.:;']\", text)\n",
        "\n",
        "    # atribuindo 'unknown' as palavras não conhecidas\n",
        "    text = [i if i in vocabulary.keys() else 'unknown' for i in text]\n",
        "\n",
        "    # conversão das chaves valores via função map()\n",
        "    text = list(map(vocabulary.get, text))\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658dcbb4-1ced-46c4-b8ae-150a85f56798"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891353a6-926c-4b2b-c19c-83b102cee71d"
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 2.73 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "    import random\n",
        "\n",
        "    # abre o arquivo que está em path e amostra K linhas aleatórias\n",
        "    with open(path) as file:\n",
        "        samples = random.sample(file.readlines(),k)\n",
        "\n",
        "    # remove a string que representa a quebra de linha\n",
        "    samples = [i.replace(\"\\n\", \"\") for i in samples]\n",
        "\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dea4ff7-c04b-4309-9272-b3f2a083c770"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 69', 'line 3', 'line 76', 'line 20', 'line 60', 'line 82', 'line 90', 'line 6', 'line 88', 'line 4']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d0ae91-8fed-4820-c4bb-0f4900a060f6"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 125 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: m x (n-1) x p\n",
        "- número de multiplicações: m x n x p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935bc911-92a8-42f3-f00f-dd0b3ced0baa"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f06870-84b6-4278-f84b-510e5e8106ef"
      },
      "source": [
        "# a lib numpy possui a função mean() que traz a média da matriz de maneira direta e sem utilizar loopings\n",
        "A.mean(axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcf80b0-e5c4-46ba-e30d-e00bc6843819"
      },
      "source": [
        "# as funções min() e max() geram os valores mínimos e máximos da matriz, respectivamente.\n",
        "# a operação, como se escreve, é executada elemento a elemento\n",
        "C = (A - A.min()) / (A.max() - A.min())\n",
        "print(C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ccfec1-e0bc-48f1-9bd3-1a298f014ccb"
      },
      "source": [
        "# definir axis=0 permite encontrar o mínimo e máximo de cada coluna e retornar um novo array\n",
        "# isso permite executar a operação elemento a elemento\n",
        "print((A - A.min(axis=0)) / (A.max(axis=0) - A.min(axis=0)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96664fa2-97c3-4188-9c4a-f09d586db147"
      },
      "source": [
        "# definir axis=1 permite encontrar o mínimo e máximo de cada linha e retornar um novo array que possui uma única linha\n",
        "# [:, np.newaxis] permite preservar os valores e alterar a dimensão da matriz para que ela possa estar em uma forma\n",
        "# que permita a operação elemento a elemento\n",
        "print((A - A.min(axis=1)[:, np.newaxis]) / ((A.max(axis=1) - A.min(axis=1)))[:, np.newaxis])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "\n",
        "    # ao subtituir o valor máximo de cada linha de todos os elementos, é possível \n",
        "    # sanar o problema do estou exponencial pois o maior expoente será zero.\n",
        "    A_adaptada = A - A.max(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # de maneira bem simplista a função softmax pode ser representada pela fórmula abaixo\n",
        "    # softmax = exp()  /   sum( exp() )\n",
        "    A_exponencial = np.exp(A_adaptada)    # numerador\n",
        "    A_sum_exp = A_exponencial.sum(axis=1) # denominador\n",
        "    A_exponencial = A_exponencial / A_sum_exp[:, np.newaxis]  # fração\n",
        "\n",
        "    return A_exponencial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7cd145-0c8c-46de-c820-14a3d707ffbd"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aafce50-d890-40f6-998c-9f4c25553389"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f3638e-ae98-41cd-cddc-7a929538c1ac"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 272 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdb43c6-b390-4bf1-db89-78d715343eb5"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "\n",
        "    # cria N_samples vetores com n_classes zeros\n",
        "    one_hot_enc = np.zeros((y.shape[0], n_classes))\n",
        "\n",
        "    # np.arange() determina a posição das linhas\n",
        "    # y representa a posição da coluna\n",
        "    # a composição dos dois indica quais células devem ser alteradas para 1\n",
        "    # garantindo assim a codificação da matriz\n",
        "    one_hot_enc[np.arange(y.shape[0]), y] = 1\n",
        "\n",
        "    return one_hot_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5a91c0-30fd-42da-bf77-cb5eef1eb7db"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 5 4 3 5 2 6 5 7 8]\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56204121-5f8e-4e1d-a4ae-c8bfba0736a9"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a50db9-9871-4b10-e2e4-04c682ed8cec"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 206 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "class Normalizer:\n",
        "    def __init__(self, matriz_b):\n",
        "        # obtendo a matriz de referência matriz_b\n",
        "        # calculando os parâmetros de ajuste\n",
        "        matriz_b = np.array(matriz_b)\n",
        "        self.media = matriz_b.mean()\n",
        "        self.despad = matriz_b.std()\n",
        "        \n",
        "    def __call__(self, matriz_a):\n",
        "        # padronização da matriz_a, entrada do método __call__\n",
        "        matriz_a = np.array(matriz_a)\n",
        "        matriz_a = (matriz_a - matriz_a.mean()) / matriz_a.std()\n",
        "        matriz_a = matriz_a * self.despad + self.media\n",
        "        return matriz_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4fb860a-9ded-45bf-96d7-a9d49ad2d03a"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752c0502-6edd-4d0e-dcfe-2ba3afafde87"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f8bf5d-9894-466a-8abb-d377de8f575d"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d59a03-9dad-4cd6-d997-1e2afa707f61"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191e7df4-b354-41dc-aeb6-b8d630d601b8"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaff905-7fcb-4dc5-ea5e-79a31017bc87"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2591f9ef-573a-4577-c8de-3f3ad32fb367"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc8416e-e45f-4e36-ea4c-359551a48f4c"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc54235-a5bb-46db-d857-5a7cac18304a"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    J  = ((x * (w)) - y).pow(2).sum()\n",
        "    return J\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "grad = (J_func(w+0.01, x, y) - J_func(w-0.01, x, y)) / (2 * 0.01)\n",
        "print('grad=', grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "480c3614-ed60-4d23-9f5e-ebb35c7a59f1"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "loss = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J)\n",
        "    print('J=', J)\n",
        "    grad = (J_func(w+0.01, x, y) - J_func(w-0.01, x, y)) / (2 * 0.01)\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate*grad\n",
        "    print('w =', w)\n",
        "    \n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Loss J x Número de Iterações')\n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss J')\n",
        "plt.plot(range(iteracoes), loss);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0000)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2576)\n",
            "grad = tensor(-20.1600)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor(-14.5151)\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9504)\n",
            "grad = tensor(-10.4509)\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0111)\n",
            "grad = tensor(-7.5247)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5242)\n",
            "grad = tensor(-5.4178)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2717)\n",
            "grad = tensor(-3.9008)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1409)\n",
            "grad = tensor(-2.8086)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0222)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0379)\n",
            "grad = tensor(-1.4560)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0483)\n",
            "w = tensor([1.9730])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7548)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5434)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3913)\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2817)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1052)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3059e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c3accd-9fe0-4117-905c-3cf2c6f6ad35"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "loss = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss.append(J)\n",
        "    print('J=', J)\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "#     w = w - learning_rate*grad\n",
        "    w = torch.tensor(w - learning_rate*grad, requires_grad=True)\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Loss J x Número de Iterações')\n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss J')\n",
        "plt.plot(range(iteracoes), [i.detach() for i in loss]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], requires_grad=True)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], requires_grad=True)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], requires_grad=True)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], requires_grad=True)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], requires_grad=True)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], requires_grad=True)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], requires_grad=True)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], requires_grad=True)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], requires_grad=True)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], requires_grad=True)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], requires_grad=True)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], requires_grad=True)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], requires_grad=True)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], requires_grad=True)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], requires_grad=True)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], requires_grad=True)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], requires_grad=True)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], requires_grad=True)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], requires_grad=True)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: Dado uma função f(x) da qual queremos saber a derivada em um ponto xi. O resultado da derivada é igual a inclinação da reta tangente aquela função no ponto xi. <br>\n",
        "Se tentarmos encontrar uma reta paralela a essa tangente a fim de determinar a inclinação, podemos na mesma função encontrar uma nova tangente de um outro ponto x(i+1).<br>\n",
        "$\\Delta w$ é dado como a diferença entre x(i) e x(i+1). Quanto maior esse valor menor será a precisão de nossa estimativa. Porém, quanto mais próximos os pontos, mais próximos serão as inclinações e mais precisa será nossa estimativa ($\\Delta w$ muito pequeno) <br>\n",
        "Sendo assim, devemos utilizar valores pequenos para $\\Delta w$ e diferentes de zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) No método de diferenças finitas seria necessário realizar N derivadas parciais que calculariam $(x_i w - y_i)^2$ que possui custo N. Sendo assim, pode-se constatar que o custo seria $O(N²)$\n",
        "\n",
        "b) Através do método de backpropagation, o calculo é realizado uma única vez, garantindo que o custo computacional seja próximo a $O(N)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:\n",
        "* Para uma única classe y = [1,0,0,...,0]\n",
        "* Na inicialização automática todos elementos tem a mesma probabilidade: $ p_j = \\frac{1}{K} $\n",
        "* Com algumas manipulações matemáticas tem-se:\n",
        "$$L = - \\log \\frac{1}{K} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}