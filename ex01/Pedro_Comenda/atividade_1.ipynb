{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercícios - 20210718",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/Pedro_Comenda/atividade_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe"
      },
      "source": [
        "print('Meu nome é: Pedro Rodrigues Comenda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "import collections\n",
        "import re\n",
        "import random\n",
        "\n",
        "def top_k(L, k):\n",
        "    # Escreva aqui o código\n",
        "    return dict(collections.Counter(L).most_common(k))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "outputId": "4d67b798-7dc7-4d21-db63-a4758c25707b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "outputId": "8573dc59-6374-4aa3-f6f7-d9ac01a43f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 513 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "    # escreva o código aqui.\n",
        "    ident = []\n",
        "    word_list = re.findall(r'\\w+|\\.', text.lower())\n",
        "    for word in word_list:\n",
        "      if word in vocabulary.keys():\n",
        "        ident.append(vocabulary[word])\n",
        "      else:\n",
        "        ident.append(vocabulary['unknown'])\n",
        "      \n",
        "    return ident"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "outputId": "c4e6edb4-0ece-415f-813e-89b4a2de308f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "outputId": "acd3f587-3f70-4c43-ebf8-e96d9b33b1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 3.01 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "def sample(path: str, k: int):\n",
        "    # Escreva o seu código aqui.\n",
        "    arquivo = open(path)\n",
        "    amostra = random.sample(arquivo.readlines(), k)\n",
        "\n",
        "    return amostra"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "outputId": "57f8e46d-2412-4104-edcf-2c6951363f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 12\\n', 'line 77\\n', 'line 28\\n', 'line 15\\n', 'line 19\\n', 'line 21\\n', 'line 80\\n', 'line 51\\n', 'line 95\\n', 'line 49\\n']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "outputId": "b8fd2791-6cdd-47b2-dccc-b54b58d27f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 104 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: \n",
        "- número de multiplicações: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "outputId": "151ab9bd-a221-48c1-c46b-b50010c1b838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "outputId": "2ca32392-f08d-4afd-824a-1393f195322d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "print(A.mean(axis=1))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.5  8.5 14.5 20.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "outputId": "b882e368-efd4-4cad-dc86-4970be4040c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C = (A - A.min())/(A.max() - A.min())\n",
        "print(C)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.04347826 0.08695652 0.13043478 0.17391304 0.2173913 ]\n",
            " [0.26086957 0.30434783 0.34782609 0.39130435 0.43478261 0.47826087]\n",
            " [0.52173913 0.56521739 0.60869565 0.65217391 0.69565217 0.73913043]\n",
            " [0.7826087  0.82608696 0.86956522 0.91304348 0.95652174 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "outputId": "92602792-0ced-49c5-efc3-e64652f0d0ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Escreva sua solução aqui.\n",
        "C = (A - A.min(axis = 0))/(A.max(axis = 0) - A.min(axis = 0))\n",
        "print(C)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         0.         0.         0.        ]\n",
            " [0.33333333 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667]\n",
            " [1.         1.         1.         1.         1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "outputId": "b81d38c9-7abb-4168-a4dd-3680d2326c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import newaxis\n",
        "# Escreva sua solução aqui.\n",
        "C = (A - A.min(axis = 1)[:, np.newaxis])/(A.max(axis = 1) - A.min(axis = 1))[:, np.newaxis]\n",
        "print(C)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]\n",
            " [0.  0.2 0.4 0.6 0.8 1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "    \n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # Escreva sua solução aqui.\n",
        "    A_soft = A - A.max(axis = 1)[:, np.newaxis]\n",
        "    A_exp = np.exp(A_soft)\n",
        "    resultado = A_exp/np.sum(A_exp, axis = 1)[:, np.newaxis]\n",
        "\n",
        "    return resultado\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "outputId": "4cb12654-6f8a-40be-d42f-5fd8e01a70cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2,   0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 1.        ],\n",
              "       [0.04861082, 0.35918811, 0.59220107]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta. \n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "outputId": "8f1296cf-2d5a-42e6-8116-c3f755790b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "outputId": "6aee217e-57b1-475d-dd67-952ebb7fd632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 283 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "outputId": "d48d295f-432e-4d81-a94c-96d5851122d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "    # Escreva seu código aqui.\n",
        "    one_hot = np.zeros((y.size, n_classes))\n",
        "    for i in range(len(y)):\n",
        "      one_hot[i][y[i]] = 1\n",
        "\n",
        "    return one_hot"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "outputId": "0a896818-c5d3-43e8-a52f-558940064e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 8 8 8 2 1 5 4 5 1]\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "outputId": "55b45b18-4465-4149-ed0e-07947e369660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "outputId": "e5cabe18-5e17-4e4e-c7c6-8542d33b9cda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 265 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "# Escreva seu código aqui.\n",
        "class Normalizer:\n",
        "  def __init__(self, array_b):\n",
        "      self.media_b = np.mean(array_b)\n",
        "      self.dp_b = np.std(array_b)\n",
        "  \n",
        "  def __call__(self, array_a):\n",
        "      normalized_array = (((array_a - np.mean(array_a))/np.std(array_a)) * self.dp_b) + self.media_b\n",
        "\n",
        "      return normalized_array\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "outputId": "f8d4daf2-cba8-4249-d640-b85f2915ea3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "outputId": "59e4c4b3-fdec-4f55-9844-7b8ec796ef80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "outputId": "a4cf9f16-ffa3-42cd-fdce-2ebeee02a336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "outputId": "b6324563-2a93-4547-fa3e-d4fd196a7a59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "outputId": "aedf1278-8cd3-4075-800d-024a870143cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "outputId": "63d53dfc-4c14-4044-93c4-b097c8569e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w; print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y; print('e =',e)\n",
        "e2 = e.pow(2) ; print('e2 =', e2)\n",
        "J = e2.sum()  ; print('J =', J)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "outputId": "12500d86-bcef-4d88-e50c-d81f8c70ec2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "outputId": "ed5ac33d-39e6-4d7e-b9cc-e37114e58e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "outputId": "2f4fa52d-78f2-4ef7-ee8c-ed993d7e45a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    # programe a função J_func, para facilitar\n",
        "    J = ((x * w) - y).pow(2).sum()\n",
        "    return J\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "grad = (J_func(w+0.001, x, y) - J_func(w-0.001, x, y))/0.002\n",
        "print('grad=', grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "outputId": "1204587e-341a-4b44-924c-74d112ef619a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "loss_J = []\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss_J.append(J)\n",
        "    print('J=', J)\n",
        "    grad = (J_func(w + learning_rate, x, y) - J_func(w - learning_rate, x, y))/(2 * learning_rate)\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i\n",
        "\n",
        "plt.title(\"Gráfico de loss J pela iterção\")\n",
        "plt.xlabel(\"Iterações\")\n",
        "plt.ylabel(\"Loss J\")\n",
        "plt.plot(range(iteracoes), loss_J)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0000)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2576)\n",
            "grad = tensor(-20.1600)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7623)\n",
            "grad = tensor(-14.5151)\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9504)\n",
            "grad = tensor(-10.4509)\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0111)\n",
            "grad = tensor(-7.5247)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5242)\n",
            "grad = tensor(-5.4178)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2717)\n",
            "grad = tensor(-3.9008)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1409)\n",
            "grad = tensor(-2.8086)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0222)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0379)\n",
            "grad = tensor(-1.4560)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0483)\n",
            "w = tensor([1.9730])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7548)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5434)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3913)\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2817)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1052)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3059e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0b7d7b51d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "outputId": "154de08b-6e87-48c7-a740-19be7533afce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "loss_J = []\n",
        "\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    loss_J.append(J.item())\n",
        "    print('J=', J)\n",
        "    w.retain_grad()\n",
        "    if w.grad: w.grad.zero_()\n",
        "    J.backward()\n",
        "    grad = w.grad\n",
        "    print('grad =',grad)\n",
        "    w = w - learning_rate * grad\n",
        "    print('w =', w)\n",
        "\n",
        "# Plote aqui a loss pela iteração\n",
        "\n",
        "plt.title(\"Gráfico de loss J pela iterção\")\n",
        "plt.xlabel(\"Iterações\")\n",
        "plt.ylabel(\"Loss J\")\n",
        "plt.plot(range(iteracoes), loss_J)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0b7d1c2a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta: Delta_w não pode ser zero e deve ser pequeno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a)\n",
        "\n",
        "b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}