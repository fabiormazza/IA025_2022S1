{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex01_patrick_ferreira.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "pycharm-362cd54d",
      "language": "python",
      "display_name": "PyCharm (IA025 - Lotufo)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "117px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unicamp-dl/IA025_2022S1/blob/main/ex01/patrick_ferreira/ex01_patrick_ferreira.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVOQpMfhgLM"
      },
      "source": [
        "Esté um notebook Colab contendo exercícios de programação em python, numpy e pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMoyGt5gXMgK"
      },
      "source": [
        "## Coloque seu nome"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBHbXcibXPRe",
        "outputId": "c7acf5a8-54c2-4a28-ec03-5e094e8eee56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print('Meu nome é: Patrick de Carvalho Tavares Rezende Ferreira')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é: Patrick de Carvalho Tavares Rezende Ferreira\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9S5acRbm1Zr"
      },
      "source": [
        "# Parte 1:\n",
        "\n",
        "##Exercícios de Processamento de Dados\n",
        "\n",
        "Nesta parte pode-se usar as bibliotecas nativas do python como a `collections`, `re` e `random`. Também pode-se usar o NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxS5h1V8nDn6"
      },
      "source": [
        "##Exercício 1.1\n",
        "Crie um dicionário com os `k` itens mais frequentes de uma lista.\n",
        "\n",
        "Por exemplo, dada a lista de itens `L=['a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a']` e `k=2`, o resultado deve ser um dicionário cuja chave é o item e o valor é a sua frequência: {'a': 4, 'e': 3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT08b5Z_nC-j"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def top_k(L, k):\n",
        "    return dict(Counter(L).most_common(k))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLD_e3C9p4xO"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma entrada com poucos itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMW9NiBgnkvA",
        "outputId": "be063ee2-7020-4e76-a696-cb764e49ed4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "L = ['f', 'a', 'a', 'd', 'b', 'd', 'c', 'e', 'a', 'b', 'e', 'e', 'a', 'd']\n",
        "k = 3\n",
        "resultado = top_k(L=L, k=k)\n",
        "print(f'resultado: {resultado}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resultado: {'a': 4, 'd': 3, 'e': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeqZScQqJ0a"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma entrada com 10M de itens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lhcm4ko8bY"
      },
      "source": [
        "import random\n",
        "\n",
        "L = random.choices('abcdefghijklmnopqrstuvwxyz', k=10_000_000)\n",
        "k = 10000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9U-Bgs2o-f_",
        "outputId": "4d870745-ed6e-40fc-bfc9-1847aa3a038a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "resultado = top_k(L=L, k=k)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 571 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJHDaOz_tK38"
      },
      "source": [
        "## Exercício 1.2\n",
        "\n",
        "Em processamento de linguagem natural, é comum convertemos as palavras de um texto para uma lista de identificadores dessas palavras. Dado o dicionário `V` abaixo onde as chaves são palavras e os valores são seus respectivos identificadores, converta o texto `D` para uma lista de identificadores.\n",
        "\n",
        "Palavras que não existem no dicionário deverão ser convertidas para o identificador do token `unknown`.\n",
        "\n",
        "O código deve ser insensível a maiúsculas (case-insensitive).\n",
        "\n",
        "Se atente que pontuações (vírgulas, ponto final, etc) também são consideradas palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVzv89trtTPc"
      },
      "source": [
        "def tokens_to_ids(text, vocabulary):\n",
        "    return [vocabulary.get(word, -1) for word in text.lower().split()]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGZeiqkY-sm"
      },
      "source": [
        "Mostre que sua implementação esta correta com um exemplo pequeno:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApR1h7gY98E",
        "outputId": "1d82b613-de28-4a73-d48e-713ffb7f5a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = 'Eu gosto de comer pizza.'\n",
        "\n",
        "print(tokens_to_ids(D, V))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 4, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWtTMxlXZN25"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxT_g-ZxZUsX"
      },
      "source": [
        "V = {'eu': 1, 'de': 2, 'gosto': 3, 'comer': 4, '.': 5, 'unknown': -1}\n",
        "D = ' '.join(1_000_000 * ['Eu gosto de comer pizza.'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp1nataGZU-V",
        "outputId": "c91584e3-7154-4996-efa2-7bee134e9fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%timeit\n",
        "resultado = tokens_to_ids(D, V)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 1.21 s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRfaKfXwRXn_"
      },
      "source": [
        "## Exercício 1.3\n",
        "\n",
        "Em aprendizado profundo é comum termos que lidar com arquivos muito grandes.\n",
        "\n",
        "Dado um arquivo de texto onde cada item é separado por `\\n`, escreva um programa que amostre `k` itens desse arquivo aleatoriamente.\n",
        "\n",
        "Nota 1: Assuma amostragem de uma distribuição uniforme, ou seja, todos os itens tem a mesma probablidade de amostragem.\n",
        "\n",
        "Nota 2: Assuma que o arquivo não cabe em memória.\n",
        "\n",
        "Nota 3: Utilize apenas bibliotecas nativas do python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PsadE9SRG_9"
      },
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def sample(path: str, k: int):\n",
        "    with open(path, 'r') as f:\n",
        "\n",
        "        # Get file LENGTH without loading into memory\n",
        "        # iterate until file is over\n",
        "        length = 0\n",
        "        while f.readline():\n",
        "            length = length + 1\n",
        "\n",
        "        # random sampling\n",
        "        random_indexes = random.sample(range(length), k=k)\n",
        "\n",
        "    retorno = []\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i in random_indexes:\n",
        "                retorno.append(line[:-1])\n",
        "\n",
        "    return retorno\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycEnlFWxSt0i"
      },
      "source": [
        "Mostre que sua implementação está correta com um exemplo pequeno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyLJ1e2ZSzC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33bcfcc-f209-4f71-c5fb-3616c2afab23"
      },
      "source": [
        "filename = 'small.txt'\n",
        "total_size = 100\n",
        "n_samples = 10\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))\n",
        "\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "print(samples)\n",
        "print(len(samples) == n_samples)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['line 3', 'line 11', 'line 14', 'line 38', 'line 42', 'line 51', 'line 60', 'line 64', 'line 72', 'line 84']\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r4FMiMj12Xg"
      },
      "source": [
        "Mostre que sua implementação é eficiente com um exemplo grande:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUwnNMGg18Ty"
      },
      "source": [
        "filename = 'large.txt'\n",
        "total_size = 1_000_000\n",
        "n_samples = 10000\n",
        "\n",
        "with open(filename, 'w') as fout:\n",
        "    fout.write('\\n'.join(f'line {i}' for i in range(total_size)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA9sAZmo0UDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49fc273-2197-410e-c8de-e7a7195d9b4e"
      },
      "source": [
        "%%timeit\n",
        "samples = sample(path=filename, k=n_samples)\n",
        "assert len(samples) == n_samples"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 3min 10s per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS0Ns4etoJs"
      },
      "source": [
        "# Parte 2:\n",
        "\n",
        "##Exercícios de Numpy\n",
        "\n",
        "Nesta parte deve-se usar apenas a biblioteca NumPy. Aqui não se pode usar o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcMz3Vzjt144"
      },
      "source": [
        "##Exercício 2.1\n",
        "\n",
        "Quantos operações de ponto flutuante (flops) de soma e de multiplicação tem a multiplicação matricial $AB$, sendo que a matriz $A$ tem tamanho $m \\times n$ e a matriz $B$ tem tamanho $n \\times p$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gNXj45RJqUm"
      },
      "source": [
        "Resposta:\n",
        "- número de somas: m * (n - 1) * p\n",
        "- número de multiplicações: m * n * p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iI7udBFeDlP"
      },
      "source": [
        "## Exercício 2.2\n",
        "\n",
        "Em programação matricial, não se faz o loop em cada elemento da matriz,\n",
        "mas sim, utiliza-se operações matriciais.\n",
        "\n",
        "Dada a matriz `A` abaixo, calcule a média dos valores de cada linha sem utilizar laços explícitos.\n",
        "\n",
        "Utilize apenas a biblioteca numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrXf18N5KrK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fqxgNBW27Z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bcd4e7-ea72-4f76-def3-5685d1ddee1d"
      },
      "source": [
        "A = np.arange(24).reshape(4, 6)\n",
        "print(A)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3  4  5]\n",
            " [ 6  7  8  9 10 11]\n",
            " [12 13 14 15 16 17]\n",
            " [18 19 20 21 22 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1EmKFrT5g7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8d6ad0-acd1-4828-8db3-c49d8648ca06"
      },
      "source": [
        "np.mean(A, axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.5,  8.5, 14.5, 20.5])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtgSAAKjUfcO"
      },
      "source": [
        "## Exercício 2.3\n",
        "\n",
        "Seja a matriz $C$ que é a normalização da matriz $A$:\n",
        "$$ C(i,j) = \\frac{A(i,j) - A_{min}}{A_{max} - A_{min}} $$\n",
        "\n",
        "Normalizar a matriz `A` do exercício acima de forma que seus valores fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:00:34.072719Z",
          "start_time": "2019-12-11T00:00:34.036017Z"
        },
        "id": "_pDhb2-0eDlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb42435-516a-4f74-e4bd-1903016561a1"
      },
      "source": [
        "C = (A - A.min()) / (A.max() - A.min())\n",
        "C"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.04347826, 0.08695652, 0.13043478, 0.17391304,\n",
              "        0.2173913 ],\n",
              "       [0.26086957, 0.30434783, 0.34782609, 0.39130435, 0.43478261,\n",
              "        0.47826087],\n",
              "       [0.52173913, 0.56521739, 0.60869565, 0.65217391, 0.69565217,\n",
              "        0.73913043],\n",
              "       [0.7826087 , 0.82608696, 0.86956522, 0.91304348, 0.95652174,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_P_GARU62m"
      },
      "source": [
        "## Exercício 2.4\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *coluna* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras colunas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVzFOYeDla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0710451-b113-4c0b-f248-405b8e99c744"
      },
      "source": [
        "C = (A - A.min(axis=0)) / (A.max(axis=0) - A.min(axis=0))\n",
        "C"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n",
              "        0.33333333],\n",
              "       [0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,\n",
              "        0.66666667],\n",
              "       [1.        , 1.        , 1.        , 1.        , 1.        ,\n",
              "        1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbXIXsDIUmtp"
      },
      "source": [
        "## Exercício 2.5\n",
        "\n",
        "Modificar o exercício anterior de forma que os valores de cada *linha* da matriz `A` sejam normalizados entre 0 e 1 independentemente dos valores das outras linhas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-10T17:56:40.413601Z",
          "start_time": "2019-12-10T17:56:40.405056Z"
        },
        "id": "i-5Hv8-heDlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606276c6-0f2d-48cc-99ce-ff359ea19aa5"
      },
      "source": [
        "C = (A - A.min(axis=1).reshape(-1, 1)) / (A.max(axis=1) - A.min(axis=1)).reshape(-1, 1)\n",
        "C"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ],\n",
              "       [0. , 0.2, 0.4, 0.6, 0.8, 1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnLAyL7zgpa"
      },
      "source": [
        "## Exercício 2.6\n",
        "\n",
        "A [função softmax](https://en.wikipedia.org/wiki/Softmax_function) é bastante usada em apredizado de máquina para converter uma lista de números para uma distribuição de probabilidade, isto é, os números ficarão normalizados entre zero e um e sua soma será igual à um.\n",
        "\n",
        "Implemente a função softmax com suporte para batches, ou seja, o softmax deve ser aplicado a cada linha da matriz. Deve-se usar apenas a biblioteca numpy. Se atente que a exponenciação gera estouro de representação quando os números da entrada são muito grandes. Tente corrigir isto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5W9vxNEmOj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def softmax(A):\n",
        "    '''\n",
        "    Aplica a função de softmax à matriz `A`.\n",
        "\n",
        "    Entrada:\n",
        "      `A` é uma matriz M x N, onde M é o número de exemplos a serem processados\n",
        "      independentemente e N é o tamanho de cada exemplo.\n",
        "\n",
        "    Saída:\n",
        "      Uma matriz M x N, onde a soma de cada linha é igual a um.\n",
        "    '''\n",
        "    # evita estouro da exponenciação\n",
        "    A = (A - A.min(axis=1).reshape(-1, 1)) / (A.max(axis=1) - A.min(axis=1)).reshape(-1, 1)\n",
        "    # Expoente multiplicado por 10 para dispercar melhor os valores\n",
        "    A = np.exp(10 * A)\n",
        "    return A / A.sum(axis=1).reshape(-1, 1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpxlbh4ND54q"
      },
      "source": [
        "Mostre que sua implementação está correta usando uma matriz pequena como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6EZ5ZD7HFao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c7bc3f-8d6f-4f2e-bfb5-da2772ccd1de"
      },
      "source": [
        "A = np.array([[0.5, -1, 1000],\n",
        "              [-2, 0, 0.5]])\n",
        "softmax(A)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.60811555e-05, 4.53957767e-05, 9.99908523e-01],\n",
              "       [3.99865265e-05, 1.19198156e-01, 8.80761858e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j2uXmKH8HF4"
      },
      "source": [
        "O código a seguir verifica se sua implementação do softmax está correta.\n",
        "- A soma de cada linha de A deve ser 1;\n",
        "- Os valores devem estar entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sN4STk7qyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e617dc95-dc8e-4a32-d751-3a739a96db64"
      },
      "source": [
        "np.allclose(softmax(A).sum(axis=1), 1) and softmax(A).min() >= 0 and softmax(A).max() <= 1"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_ZRWRfCZtI"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhUeyrGaJ3J2"
      },
      "source": [
        "A = np.random.uniform(low=-10, high=10, size=(128, 100_000))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaa-C8XkKJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b684d9c9-765d-43a1-c2d2-981387dcd867"
      },
      "source": [
        "%%timeit\n",
        "softmax(A)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 320 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XE6LaWi81zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ab1be8-b29f-4dcf-df7b-e2772889a697"
      },
      "source": [
        "SM = softmax(A)\n",
        "np.allclose(SM.sum(axis=1), 1) and SM.min() >= 0 and SM.max() <= 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flr1lI5o-HpG"
      },
      "source": [
        "## Exercício 2.7\n",
        "\n",
        "A codificação one-hot é usada para codificar entradas categóricas. É uma codificação onde apenas um bit é 1 e os demais são zero, conforme a tabela a seguir.\n",
        "\n",
        "| Decimal | Binary | One-hot\n",
        "| ------- | ------ | -------\n",
        "| 0 | 000    | 1 0 0 0 0 0 0 0\n",
        "| 1 | 001    | 0 1 0 0 0 0 0 0\n",
        "| 2 | 010    | 0 0 1 0 0 0 0 0\n",
        "| 3 | 011    | 0 0 0 1 0 0 0 0\n",
        "| 4 | 100    | 0 0 0 0 1 0 0 0\n",
        "| 5 | 101    | 0 0 0 0 0 1 0 0\n",
        "| 6 | 110    | 0 0 0 0 0 0 1 0\n",
        "| 7 | 111    | 0 0 0 0 0 0 0 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CqXP_5ABbfo"
      },
      "source": [
        "Implemente a função one_hot(y, n_classes) que codifique o vetor de inteiros y que possuem valores entre 0 e n_classes-1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-02w7qCH7L"
      },
      "source": [
        "def one_hot(y, n_classes):\n",
        "    # converte para valores binarios\n",
        "    # Em binario, cada potencia ocupa 1 unico bit\n",
        "    y = 2 ** y\n",
        "    # retorna uma matriz em que cada elemento eh a representacao\n",
        "    # em binario do numero de entrada. Como sao potencias de 2,\n",
        "    # fica tudo em one-hot.\n",
        "    # O shift ali embaixo esta apenas aplicando mascara binaria\n",
        "    return ((y.reshape(-1, 1) & (1 << np.arange(n_classes))) > 0) // 1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5zyZO5Aiz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b976765-d675-46e0-edc8-6cc564f88c19"
      },
      "source": [
        "N_CLASSES = 9\n",
        "N_SAMPLES = 10\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)\n",
        "print(y)\n",
        "print(one_hot(y, N_CLASSES))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4 6 8 4 0 3 3 1 8 4]\n",
            "[[0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwuKnQUCzve"
      },
      "source": [
        "Mostre que sua implementação é eficiente usando uma matriz grande como entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwuFy5rWC2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365697d4-33eb-4352-f17b-ce721ef08741"
      },
      "source": [
        "N_SAMPLES = 100_000\n",
        "N_CLASSES = 1_000\n",
        "y = (np.random.rand((N_SAMPLES)) * N_CLASSES).astype(np.int)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azMtF7wDJ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e72ea1-fa14-42ed-da61-4b40493e1c96"
      },
      "source": [
        "%%timeit\n",
        "one_hot(y, N_CLASSES)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 420 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqMroZay2ubi"
      },
      "source": [
        "## Exercício 2.8\n",
        "\n",
        "Implemente uma classe que normalize um array de pontos flutuantes `array_a` para a mesma média e desvio padrão de um outro array `array_b`, conforme exemplo abaixo:\n",
        "```\n",
        "array_a = np.array([-1, 1.5, 0])\n",
        "array_b = np.array([1.4, 0.8, 0.3, 2.5])\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)  # Deve imprimir [0.3187798  2.31425165 1.11696854]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaedJ5Cf5Oy2"
      },
      "source": [
        "class Normalizer:\n",
        "    def __init__(self, b):\n",
        "        b = np.array(b)\n",
        "        self.b = b\n",
        "        self.std = b.std()\n",
        "        self.mean = b.mean()\n",
        "\n",
        "    def __call__(self, a):\n",
        "        a = np.array(a)\n",
        "        a_norm = (a - a.mean()) / a.std()\n",
        "        return a_norm * self.std + self.mean\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlkNNU6h5RbR"
      },
      "source": [
        "Mostre que seu código está correto com o exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gad6zsbh5a0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a94517-b185-46eb-e0dd-03d9c7e1a33b"
      },
      "source": [
        "array_a = [-1, 1.5, 0]\n",
        "array_b = [1.4, 0.8, 0.3, 2.5]\n",
        "normalize = Normalizer(array_b)\n",
        "normalized_array = normalize(array_a)\n",
        "print(normalized_array)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3187798  2.31425165 1.11696854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrGVQFUYI_LP"
      },
      "source": [
        "# Parte 3:\n",
        "\n",
        "##Exercícios Pytorch: Grafo Computacional e Gradientes\n",
        "\n",
        "Nesta parte pode-se usar quaisquer bibliotecas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIlQdKAuCZtR"
      },
      "source": [
        "Um dos principais fundamentos para que o PyTorch seja adequado para deep learning é a sua habilidade de calcular o gradiente automaticamente a partir da expressões definidas. Essa facilidade é implementada através do cálculo automático do gradiente e construção dinâmica do grafo computacional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF_-dJ2nCZtT"
      },
      "source": [
        "## Grafo computacional\n",
        "\n",
        "Seja um exemplo simples de uma função de perda J dada pela Soma dos Erros ao Quadrado (SEQ - Sum of Squared Errors): \n",
        "$$ J = \\sum_i (x_i w - y_i)^2 $$\n",
        "que pode ser reescrita como:\n",
        "$$ \\hat{y_i} = x_i w $$\n",
        "$$ e_i = \\hat{y_i} - y_i $$\n",
        "$$ e2_i = e_i^2 $$\n",
        "$$ J = \\sum_i e2_i $$\n",
        "\n",
        "As redes neurais são treinadas através da minimização de uma função de perda usando o método do gradiente descendente. Para ajustar o parâmetro $w$ precisamos calcular o gradiente $  \\frac{ \\partial J}{\\partial w} $. Usando a\n",
        "regra da cadeia podemos escrever:\n",
        "$$ \\frac{ \\partial J}{\\partial w} = \\frac{ \\partial J}{\\partial e2_i} \\frac{ \\partial e2_i}{\\partial e_i} \\frac{ \\partial e_i}{\\partial \\hat{y_i} } \\frac{ \\partial \\hat{y_i}}{\\partial w}$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboejVQMCZtU"
      },
      "source": [
        "```\n",
        "    y_pred = x * w\n",
        "    e = y_pred - y\n",
        "    e2 = e**2\n",
        "    J = e2.sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7JmU6qhc2Y2"
      },
      "source": [
        "As quatro expressões acima, para o cálculo do J podem ser representadas pelo grafo computacional visualizado a seguir: os círculos são as variáveis (tensores), os quadrados são as operações, os números em preto são os cálculos durante a execução das quatro expressões para calcular o J (forward, predict). O cálculo do gradiente, mostrado em vermelho, é calculado pela regra da cadeia, de trás para frente (backward)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeEBKl4CZtV"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/robertoalotufo/files/master/figures/GrafoComputacional.png\" width=\"600pt\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZun7wrCZtX"
      },
      "source": [
        "Para entender melhor o funcionamento do grafo computacional com os tensores, recomenda-se leitura em:\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.431853Z",
          "start_time": "2019-12-11T00:23:00.414813Z"
        },
        "id": "HlT2d-4fCZtZ"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-12-11T00:23:00.863228Z",
          "start_time": "2019-12-11T00:23:00.844457Z"
        },
        "id": "xX0QwUduCZtf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "090e4e1d-ab91-4da1-e23f-3db0dfe5c27f"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.10.0+cu111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsqzALS4CZtl"
      },
      "source": [
        "**Tensor com atributo .requires_grad=True**\n",
        "\n",
        "Quando um tensor possui o atributo `requires_grad` como verdadeiro, qualquer expressão que utilizar esse tensor irá construir um grafo computacional para permitir posteriormente, após calcular a função a ser derivada, poder usar a regra da cadeia e calcular o gradiente da função em termos dos tensores que possuem o atributo `requires_grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:22.117010Z",
          "start_time": "2019-09-29T03:07:22.041861Z"
        },
        "id": "foaAb94aCZtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4d1d1a-8169-487b-cd91-be84fe39c2c5"
      },
      "source": [
        "y = torch.arange(0, 8, 2).float()\n",
        "y"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:28.610934Z",
          "start_time": "2019-09-29T03:07:28.598223Z"
        },
        "id": "no6SdSyICZtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6bd69e-a88a-4626-823f-beef72036ce7"
      },
      "source": [
        "x = torch.arange(0, 4).float()\n",
        "x"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:31.523762Z",
          "start_time": "2019-09-29T03:07:31.497683Z"
        },
        "id": "eL_i1mwGCZtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d4d41e-5088-4a9c-87ec-2a558ff43f91"
      },
      "source": [
        "w = torch.ones(1, requires_grad=True)\n",
        "w"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEl-0l7CZt0"
      },
      "source": [
        "## Cálculo automático do gradiente da função perda J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUh-SCnCZt1"
      },
      "source": [
        "Seja a expressão: $$ J = \\sum_i ((x_i  w) - y_i)^2 $$\n",
        "\n",
        "Queremos calcular a derivada de $J$ em relação a $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMwwVtJ1CZt2"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Durante a execução da expressão, o grafo computacional é criado. Compare os valores de cada parcela calculada com os valores em preto da figura ilustrativa do grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:36.290122Z",
          "start_time": "2019-09-29T03:07:36.273229Z"
        },
        "id": "zp2aK4YhCZt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d40fe7-1557-4282-d12f-b503af1a7eb1"
      },
      "source": [
        "# predict (forward)\n",
        "y_pred = x * w;\n",
        "print('y_pred =', y_pred)\n",
        "\n",
        "# cálculo da perda J: loss\n",
        "e = y_pred - y;\n",
        "print('e =', e)\n",
        "e2 = e.pow(2);\n",
        "print('e2 =', e2)\n",
        "J = e2.sum();\n",
        "print('J =', J)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_pred = tensor([0., 1., 2., 3.], grad_fn=<MulBackward0>)\n",
            "e = tensor([ 0., -1., -2., -3.], grad_fn=<SubBackward0>)\n",
            "e2 = tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "J = tensor(14., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96wB7PCZt8"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-10-04T15:55:45.308858",
          "start_time": "2017-10-04T15:55:45.304654"
        },
        "id": "kKbf4D0CCZt-"
      },
      "source": [
        "O `backward()` varre o grafo computacional a partir da variável a ele associada (raiz) e calcula o gradiente para todos os tensores que possuem o atributo `requires_grad` como verdadeiro.\n",
        "Observe que os tensores que tiverem o atributo `requires_grad` serão sempre folhas no grafo computacional.\n",
        "O `backward()` destroi o grafo após sua execução. Esse comportamento é padrão no PyTorch. \n",
        "\n",
        "A título ilustrativo, se quisermos depurar os gradientes dos nós que não são folhas no grafo computacional, precisamos primeiro invocar `retain_grad()` em cada um desses nós, como a seguir. Entretanto nos exemplos reais não há necessidade de verificar o gradiente desses nós."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CjLPu6clVo"
      },
      "source": [
        "e2.retain_grad()\n",
        "e.retain_grad()\n",
        "y_pred.retain_grad()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsZS2Bicof-"
      },
      "source": [
        "E agora calculamos os gradientes com o `backward()`.\n",
        "\n",
        "w.grad é o gradiente de J em relação a w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-29T03:07:40.267334Z",
          "start_time": "2019-09-29T03:07:40.247422Z"
        },
        "id": "Z1lnkb0GCZt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e9d201-2006-4a35-9eb7-7351838ec9a2"
      },
      "source": [
        "if w.grad: w.grad.zero_()\n",
        "J.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-28.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1xYDPR_uOcZ"
      },
      "source": [
        "Mostramos agora os gradientes que estão grafados em vermelho no grafo computacional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enuk2tf0sDyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b6cbf8-ac92-48a6-cd59-e78c324e4f8c"
      },
      "source": [
        "print(e2.grad)\n",
        "print(e.grad)\n",
        "print(y_pred.grad)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([ 0., -2., -4., -6.])\n",
            "tensor([ 0., -2., -4., -6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsOThnt8fDJV"
      },
      "source": [
        "##Exercício 3.1\n",
        "Calcule o mesmo gradiente ilustrado no exemplo anterior usando a regra das diferenças finitas, de acordo com a equação a seguir, utilizando um valor de $\\Delta w$ bem pequeno.\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{J(w + \\Delta w) - J(w - \\Delta w)}{2 \\Delta w} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hidden": true,
        "id": "62nZAfUoCZu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b470fabb-d87d-46fc-a85f-fe77e1b25918"
      },
      "source": [
        "def J_func(w, x, y):\n",
        "    return ((y - w * x) ** 2).sum()\n",
        "\n",
        "\n",
        "# Calcule o gradiente usando a regra diferenças finitas\n",
        "# Confira com o valor já calculado anteriormente\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "grad = (J_func(w + (10 ** -3), x, y) - J_func(w - (10 ** -3), x, y)) / (2 * (10 ** -3))\n",
        "print('grad=', grad)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= tensor(-28.0008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Sx1QXZxJ3u"
      },
      "source": [
        "##Exercício 3.2\n",
        "\n",
        "Minimizando $J$ pelo gradiente descendente\n",
        "\n",
        "$$ w_{k+1} = w_k - \\lambda \\frac {\\partial J}{\\partial w} $$\n",
        "\n",
        "Supondo que valor inicial ($k=0$) $w_0 = 1$, use learning rate $\\lambda = 0.01$ para calcular o valor do novo $w_{20}$, ou seja, fazendo 20 atualizações de gradientes. Deve-se usar a função `J_func` criada no exercício anterior.\n",
        "\n",
        "Confira se o valor do primeiro gradiente está de acordo com os valores já calculado acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNszCOED1Wtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01e27c1e-e43e-4afd-ea0d-498953decc76"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1)\n",
        "\n",
        "losses_list = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    print('J=', J)\n",
        "    grad = (J_func(w + (10 ** -3), x, y) - J_func(w - (10 ** -3), x, y)) / (2 * (10 ** -3))\n",
        "    print('grad =', grad)\n",
        "    w = w - grad * learning_rate\n",
        "    print('w =', w)\n",
        "    losses_list.append(J.detach().numpy())\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.close()\n",
        "plt.plot(losses_list)\n",
        "plt.xticks(range(iteracoes))\n",
        "plt.legend(['Loss', ], loc='upper right')\n",
        "plt.title(\"Gráfico de Loss por iteração\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteração\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plote o gráfico da loss J pela iteração i"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14.)\n",
            "grad = tensor(-28.0008)\n",
            "w = tensor([1.2800])\n",
            "i = 1\n",
            "J= tensor(7.2574)\n",
            "grad = tensor(-20.1611)\n",
            "w = tensor([1.4816])\n",
            "i = 2\n",
            "J= tensor(3.7621)\n",
            "grad = tensor(-14.5156)\n",
            "w = tensor([1.6268])\n",
            "i = 3\n",
            "J= tensor(1.9501)\n",
            "grad = tensor(-10.4510)\n",
            "w = tensor([1.7313])\n",
            "i = 4\n",
            "J= tensor(1.0109)\n",
            "grad = tensor(-7.5245)\n",
            "w = tensor([1.8065])\n",
            "i = 5\n",
            "J= tensor(0.5240)\n",
            "grad = tensor(-5.4175)\n",
            "w = tensor([1.8607])\n",
            "i = 6\n",
            "J= tensor(0.2716)\n",
            "grad = tensor(-3.9003)\n",
            "w = tensor([1.8997])\n",
            "i = 7\n",
            "J= tensor(0.1408)\n",
            "grad = tensor(-2.8083)\n",
            "w = tensor([1.9278])\n",
            "i = 8\n",
            "J= tensor(0.0730)\n",
            "grad = tensor(-2.0218)\n",
            "w = tensor([1.9480])\n",
            "i = 9\n",
            "J= tensor(0.0378)\n",
            "grad = tensor(-1.4557)\n",
            "w = tensor([1.9626])\n",
            "i = 10\n",
            "J= tensor(0.0196)\n",
            "grad = tensor(-1.0482)\n",
            "w = tensor([1.9731])\n",
            "i = 11\n",
            "J= tensor(0.0102)\n",
            "grad = tensor(-0.7546)\n",
            "w = tensor([1.9806])\n",
            "i = 12\n",
            "J= tensor(0.0053)\n",
            "grad = tensor(-0.5433)\n",
            "w = tensor([1.9860])\n",
            "i = 13\n",
            "J= tensor(0.0027)\n",
            "grad = tensor(-0.3912)\n",
            "w = tensor([1.9899])\n",
            "i = 14\n",
            "J= tensor(0.0014)\n",
            "grad = tensor(-0.2816)\n",
            "w = tensor([1.9928])\n",
            "i = 15\n",
            "J= tensor(0.0007)\n",
            "grad = tensor(-0.2028)\n",
            "w = tensor([1.9948])\n",
            "i = 16\n",
            "J= tensor(0.0004)\n",
            "grad = tensor(-0.1460)\n",
            "w = tensor([1.9962])\n",
            "i = 17\n",
            "J= tensor(0.0002)\n",
            "grad = tensor(-0.1051)\n",
            "w = tensor([1.9973])\n",
            "i = 18\n",
            "J= tensor(0.0001)\n",
            "grad = tensor(-0.0757)\n",
            "w = tensor([1.9981])\n",
            "i = 19\n",
            "J= tensor(5.3023e-05)\n",
            "grad = tensor(-0.0545)\n",
            "w = tensor([1.9986])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXxBmWGK3IU"
      },
      "source": [
        "##Exercício 3.3\n",
        "\n",
        "Repita o exercício 2 mas usando agora o calculando o gradiente usando o método backward() do pytorch. Confira se o primeiro valor do gradiente está de acordo com os valores anteriores. Execute essa próxima célula duas vezes. Os valores devem ser iguais.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMP4d5vtHtqy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8681c01-695d-4c70-e732-d53bd84b57c6"
      },
      "source": [
        "learning_rate = 0.01\n",
        "iteracoes = 20\n",
        "\n",
        "x = torch.arange(0, 4).float()\n",
        "y = torch.arange(0, 8, 2).float()\n",
        "w = torch.ones(1, requires_grad=True)\n",
        "\n",
        "losses_list = []\n",
        "for i in range(iteracoes):\n",
        "    print('i =', i)\n",
        "    J = J_func(w, x, y)\n",
        "    w.retain_grad()\n",
        "    J.backward()\n",
        "    print('J=', J)\n",
        "    grad = w.grad\n",
        "    print('grad =', grad)\n",
        "    w = w - grad * learning_rate\n",
        "    print('w =', w)\n",
        "    losses_list.append(J.detach().numpy())\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.close()\n",
        "plt.plot(losses_list)\n",
        "plt.xticks(range(iteracoes))\n",
        "plt.legend(['Loss', ], loc='upper right')\n",
        "plt.title(\"Gráfico de Loss por iteração\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Iteração\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plote aqui a loss pela iteração"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = 0\n",
            "J= tensor(14., grad_fn=<SumBackward0>)\n",
            "grad = tensor([-28.])\n",
            "w = tensor([1.2800], grad_fn=<SubBackward0>)\n",
            "i = 1\n",
            "J= tensor(7.2576, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-20.1600])\n",
            "w = tensor([1.4816], grad_fn=<SubBackward0>)\n",
            "i = 2\n",
            "J= tensor(3.7623, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-14.5152])\n",
            "w = tensor([1.6268], grad_fn=<SubBackward0>)\n",
            "i = 3\n",
            "J= tensor(1.9504, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-10.4509])\n",
            "w = tensor([1.7313], grad_fn=<SubBackward0>)\n",
            "i = 4\n",
            "J= tensor(1.0111, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-7.5247])\n",
            "w = tensor([1.8065], grad_fn=<SubBackward0>)\n",
            "i = 5\n",
            "J= tensor(0.5241, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-5.4178])\n",
            "w = tensor([1.8607], grad_fn=<SubBackward0>)\n",
            "i = 6\n",
            "J= tensor(0.2717, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-3.9008])\n",
            "w = tensor([1.8997], grad_fn=<SubBackward0>)\n",
            "i = 7\n",
            "J= tensor(0.1409, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.8086])\n",
            "w = tensor([1.9278], grad_fn=<SubBackward0>)\n",
            "i = 8\n",
            "J= tensor(0.0730, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-2.0222])\n",
            "w = tensor([1.9480], grad_fn=<SubBackward0>)\n",
            "i = 9\n",
            "J= tensor(0.0379, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.4560])\n",
            "w = tensor([1.9626], grad_fn=<SubBackward0>)\n",
            "i = 10\n",
            "J= tensor(0.0196, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-1.0483])\n",
            "w = tensor([1.9730], grad_fn=<SubBackward0>)\n",
            "i = 11\n",
            "J= tensor(0.0102, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.7548])\n",
            "w = tensor([1.9806], grad_fn=<SubBackward0>)\n",
            "i = 12\n",
            "J= tensor(0.0053, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.5434])\n",
            "w = tensor([1.9860], grad_fn=<SubBackward0>)\n",
            "i = 13\n",
            "J= tensor(0.0027, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.3913])\n",
            "w = tensor([1.9899], grad_fn=<SubBackward0>)\n",
            "i = 14\n",
            "J= tensor(0.0014, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2817])\n",
            "w = tensor([1.9928], grad_fn=<SubBackward0>)\n",
            "i = 15\n",
            "J= tensor(0.0007, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.2028])\n",
            "w = tensor([1.9948], grad_fn=<SubBackward0>)\n",
            "i = 16\n",
            "J= tensor(0.0004, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1460])\n",
            "w = tensor([1.9962], grad_fn=<SubBackward0>)\n",
            "i = 17\n",
            "J= tensor(0.0002, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.1052])\n",
            "w = tensor([1.9973], grad_fn=<SubBackward0>)\n",
            "i = 18\n",
            "J= tensor(0.0001, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0757])\n",
            "w = tensor([1.9981], grad_fn=<SubBackward0>)\n",
            "i = 19\n",
            "J= tensor(5.3059e-05, grad_fn=<SumBackward0>)\n",
            "grad = tensor([-0.0545])\n",
            "w = tensor([1.9986], grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GulfYtzBMx2e"
      },
      "source": [
        "##Exercício 3.4\n",
        "\n",
        "Quais são as restrições na escolha dos valores de $\\Delta w$ no cálculo do gradiente por diferenças finitas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXQGEyvtiTAR"
      },
      "source": [
        "Resposta:   Valores muito grandes de learning rate (LR) poderiam fazer o gradiente oscilar em torno do valor ideal e nunca se aproximar da solução.\n",
        "            Valores muito pequenos de LR poderiam resultar em erros no cálculo devido a instabilidades numéricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrSF8GEiXk4"
      },
      "source": [
        "##Exercício 3.5\n",
        "\n",
        "Até agora trabalhamos com $w$ contendo apenas um parâmetro. Suponha agora que $w$ seja uma matriz com $N$ parâmetros e que o custo para executar $(x_i w - y_i)^2$ seja $O(N)$.\n",
        "> a) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método das diferencas finitas?\n",
        ">\n",
        "> b) Qual é o custo computacional para fazer uma única atualização (um passo de gradiente) dos parâmetros de $w$ usando o método do backpropagation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pna3bcicHj"
      },
      "source": [
        "Resposta (justifique):\n",
        "\n",
        "a) Para obter o valor do erro, precisamos calcular a função de erro, com O(N), e repetí-la para cada um dos N parâmetros que terão seus gradientes avaliados, gerando custo final de O(N^2).\n",
        "\n",
        "b) Para calcular o erro (loss), temos O(N). Para atualizar cada um dos N parâmetros, também temos O(N).\n",
        "Logo, o custo final também é O(N^2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35I5w8EZdjIo"
      },
      "source": [
        "##Exercício 3.6\n",
        "\n",
        "Qual o custo (entropia cruzada) esperado para um exemplo (uma amostra) no começo do treinamento de um classificador inicializado aleatoriamente?\n",
        "\n",
        "A equação da entropia cruzada é:\n",
        "$$L = - \\sum_{j=0}^{K-1} y_j \\log p_j, $$\n",
        "Onde:\n",
        "\n",
        "- K é o número de classes;\n",
        "\n",
        "- $y_j=1$ se $j$ é a classe do exemplo (ground-truth), 0 caso contrário. Ou seja, $y$ é um vetor one-hot;\n",
        "\n",
        "- $p_j$ é a probabilidade predita pelo modelo para a classe $j$.\n",
        "\n",
        "A resposta tem que ser em função de uma ou mais das seguintes variáveis:\n",
        "\n",
        "- K = número de classes\n",
        "\n",
        "- B = batch size\n",
        "\n",
        "- D = dimensão de qualquer vetor do modelo\n",
        "\n",
        "- LR = learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTOphiVs6eN"
      },
      "source": [
        "Resposta: A probabilidade de cada classe ser predita em um modelo inicializado aleatoriamente é de 1/K, ou seja, p_i = 1/K.\n",
        "y_j vale 1 para apenas uma classe em cada iteração, logo: L = -log(1/K) = log(K)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNdHqgSB6S9"
      },
      "source": [
        "Fim do notebook."
      ]
    }
  ]
}